{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d2f8ee58-c1eb-4123-8865-7f01a811c938",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.12/site-packages (1.26.4)\n",
      "Requirement already satisfied: scipy in /opt/conda/lib/python3.12/site-packages (1.13.1)\n",
      "Requirement already satisfied: numpy<2.3,>=1.22.4 in /opt/conda/lib/python3.12/site-packages (from scipy) (1.26.4)\n",
      "Requirement already satisfied: gensim in /opt/conda/lib/python3.12/site-packages (4.3.3)\n",
      "Requirement already satisfied: numpy<2.0,>=1.18.5 in /opt/conda/lib/python3.12/site-packages (from gensim) (1.26.4)\n",
      "Requirement already satisfied: scipy<1.14.0,>=1.7.0 in /opt/conda/lib/python3.12/site-packages (from gensim) (1.13.1)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in /opt/conda/lib/python3.12/site-packages (from gensim) (7.3.1)\n",
      "Requirement already satisfied: wrapt in /opt/conda/lib/python3.12/site-packages (from smart-open>=1.8.1->gensim) (1.17.3)\n",
      "Requirement already satisfied: nltk in /opt/conda/lib/python3.12/site-packages (3.9.1)\n",
      "Requirement already satisfied: click in /opt/conda/lib/python3.12/site-packages (from nltk) (8.2.1)\n",
      "Requirement already satisfied: joblib in /opt/conda/lib/python3.12/site-packages (from nltk) (1.5.1)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /opt/conda/lib/python3.12/site-packages (from nltk) (2025.9.1)\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.12/site-packages (from nltk) (4.67.1)\n",
      "Requirement already satisfied: sentencepiece in /opt/conda/lib/python3.12/site-packages (0.2.1)\n"
     ]
    }
   ],
   "source": [
    "# 순서대로 재설치\n",
    "!pip install numpy\n",
    "!pip install scipy\n",
    "!pip install gensim\n",
    "!pip install nltk\n",
    "!pip install sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bd5b7108-a80d-420d-a51a-e3e60e8c9a75",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "import pandas\n",
    "import torch\n",
    "import nltk\n",
    "import gensim\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "import requests\n",
    "import re\n",
    "import sentencepiece as spm\n",
    "import random\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from collections import Counter\n",
    "from gensim.models import KeyedVectors\n",
    "from io import StringIO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "973e0f77-66c7-4e24-952d-5e86f93dc5c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from io import StringIO\n",
    "\n",
    "# GitHub raw 링크로 직접 다운로드\n",
    "url = \"https://raw.githubusercontent.com/songys/Chatbot_data/master/ChatbotData.csv\"\n",
    "response = requests.get(url)\n",
    "df = pd.read_csv(StringIO(response.text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "528a3cc1-197c-4c59-8642-8f999ca3dd96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "데이터 형태: (11823, 3)\n",
      "\n",
      "컬럼명:\n",
      "['Q', 'A', 'label']\n",
      "\n",
      "첫 5행:\n",
      "                 Q            A  label\n",
      "0           12시 땡!   하루가 또 가네요.      0\n",
      "1      1지망 학교 떨어졌어    위로해 드립니다.      0\n",
      "2     3박4일 놀러가고 싶다  여행은 언제나 좋죠.      0\n",
      "3  3박4일 정도 놀러가고 싶다  여행은 언제나 좋죠.      0\n",
      "4          PPL 심하네   눈살이 찌푸려지죠.      0\n",
      "\n",
      "결측값:\n",
      "Q        0\n",
      "A        0\n",
      "label    0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# 데이터 구조 확인\n",
    "print(\"데이터 형태:\", df.shape)\n",
    "print(\"\\n컬럼명:\")\n",
    "print(df.columns.tolist())\n",
    "print(\"\\n첫 5행:\")\n",
    "print(df.head())\n",
    "\n",
    "# 결측값 확인\n",
    "print(\"\\n결측값:\")\n",
    "print(df.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ea0d86b8-a7e7-4247-8ede-995472058fa3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "질문 개수: 11823\n",
      "답변 개수: 11823\n",
      "\n",
      "질문 1: 12시 땡!\n",
      "답변 1: 하루가 또 가네요.\n",
      "\n",
      "질문 2: 1지망 학교 떨어졌어\n",
      "답변 2: 위로해 드립니다.\n",
      "\n",
      "질문 3: 3박4일 놀러가고 싶다\n",
      "답변 3: 여행은 언제나 좋죠.\n"
     ]
    }
   ],
   "source": [
    "# 질문과 답변을 각각 변수에 저장\n",
    "questions = df['Q'].tolist()  # 또는 df['Q'].values\n",
    "answers = df['A'].tolist()    # 또는 df['A'].values\n",
    "\n",
    "# 확인\n",
    "print(f\"질문 개수: {len(questions)}\")\n",
    "print(f\"답변 개수: {len(answers)}\")\n",
    "\n",
    "# 첫 몇 개 예시 출력\n",
    "for i in range(3):\n",
    "    print(f\"\\n질문 {i+1}: {questions[i]}\")\n",
    "    print(f\"답변 {i+1}: {answers[i]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b1c25d44-919b-4798-a384-162970d83d3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== 전처리 테스트 ===\n",
      "원본 1: 안녕하세요! 오늘 날씨가 어떤가요?\n",
      "처리 1: 안녕하세요! 오늘 날씨가 어떤가요?\n",
      "\n",
      "원본 2: Hello World! How are you today?\n",
      "처리 2: hello world! how are you today?\n",
      "\n",
      "원본 3: 이것은 123개의 숫자와 ABC 영문자가 포함된 문장입니다.\n",
      "처리 3: 이것은 123개의 숫자와 abc 영문자가 포함된 문장입니다.\n",
      "\n",
      "원본 4: 특수문자@#$%^&*를 포함한 문장입니다!!!\n",
      "처리 4: 특수문자를 포함한 문장입니다!!!\n",
      "\n",
      "원본 5: 여러    공백이    있는    문장입니다.\n",
      "처리 5: 여러 공백이 있는 문장입니다.\n",
      "\n",
      "원본 6: 안녕하세요~ ^^ 좋은 하루 되세요!!! ♥♥♥\n",
      "처리 6: 안녕하세요 좋은 하루 되세요!!!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def preprocess_sentence(sentence):\n",
    "    # 1. 영문자를 모두 소문자로 변환\n",
    "    sentence = sentence.lower()\n",
    "    \n",
    "    # 2. 영문자, 한글, 숫자, 주요 특수문자만 남기고 나머지 제거\n",
    "    # 한글: \\uAC00-\\uD7A3 (가-힣)\n",
    "    # 영문자: a-z (이미 소문자로 변환됨)\n",
    "    # 숫자: 0-9\n",
    "    # 주요 특수문자: . , ? ! ; : - ( ) \" ' 등\n",
    "    sentence = re.sub(r'[^a-z가-힣0-9.,?!;:\\-()\"\\'\\s]', '', sentence)\n",
    "    \n",
    "    # 3. 연속된 공백을 하나로 정리\n",
    "    sentence = re.sub(r'\\s+', ' ', sentence)\n",
    "    \n",
    "    # 4. 앞뒤 공백 제거\n",
    "    sentence = sentence.strip()\n",
    "    \n",
    "    return sentence\n",
    "\n",
    "# 테스트 함수\n",
    "def test_preprocess():\n",
    "    \"\"\"전처리 함수 테스트\"\"\"\n",
    "    test_sentences = [\n",
    "        \"안녕하세요! 오늘 날씨가 어떤가요?\",\n",
    "        \"Hello World! How are you today?\",\n",
    "        \"이것은 123개의 숫자와 ABC 영문자가 포함된 문장입니다.\",\n",
    "        \"특수문자@#$%^&*를 포함한 문장입니다!!!\",\n",
    "        \"여러    공백이    있는    문장입니다.\",\n",
    "        \"안녕하세요~ ^^ 좋은 하루 되세요!!! ♥♥♥\"\n",
    "    ]\n",
    "    \n",
    "    print(\"=== 전처리 테스트 ===\")\n",
    "    for i, sentence in enumerate(test_sentences, 1):\n",
    "        processed = preprocess_sentence(sentence)\n",
    "        print(f\"원본 {i}: {sentence}\")\n",
    "        print(f\"처리 {i}: {processed}\")\n",
    "        print()\n",
    "\n",
    "# 테스트 실행\n",
    "test_preprocess()\n",
    "\n",
    "# ChatBot 데이터에 적용하는 예시\n",
    "def apply_preprocessing_to_data(questions, answers):\n",
    "    \"\"\"\n",
    "    질문과 답변 데이터에 전처리 적용\n",
    "    \n",
    "    Args:\n",
    "        questions (list): 질문 리스트\n",
    "        answers (list): 답변 리스트\n",
    "    \n",
    "    Returns:\n",
    "        tuple: 전처리된 (질문 리스트, 답변 리스트)\n",
    "    \"\"\"\n",
    "    processed_questions = []\n",
    "    processed_answers = []\n",
    "    \n",
    "    for q, a in zip(questions, answers):\n",
    "        # 전처리 적용\n",
    "        processed_q = preprocess_sentence(q)\n",
    "        processed_a = preprocess_sentence(a)\n",
    "        \n",
    "        # 빈 문자열이 아닌 경우만 추가\n",
    "        if processed_q.strip() and processed_a.strip():\n",
    "            processed_questions.append(processed_q)\n",
    "            processed_answers.append(processed_a)\n",
    "    \n",
    "    print(f\"전처리 전: {len(questions)}개\")\n",
    "    print(f\"전처리 후: {len(processed_questions)}개\")\n",
    "    \n",
    "    return processed_questions, processed_answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "103c02b2-6777-486c-ae47-9b226f936fb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "데이터 발견! 토큰화 시작...\n",
      "SentencePiece 토크나이저 훈련 시작\n",
      "토크나이저 훈련 완료\n",
      "어휘 사전 크기: 8000\n",
      "코퍼스 생성 시작 - 입력 데이터: 11823개 쌍\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sentencepiece_trainer.cc(78) LOG(INFO) Starts training with : \n",
      "trainer_spec {\n",
      "  input: chatbot_corpus.txt\n",
      "  input_format: \n",
      "  model_prefix: chatbot_spm\n",
      "  model_type: BPE\n",
      "  vocab_size: 8000\n",
      "  self_test_sample_size: 0\n",
      "  character_coverage: 0.9995\n",
      "  input_sentence_size: 0\n",
      "  shuffle_input_sentence: 1\n",
      "  seed_sentencepiece_size: 1000000\n",
      "  shrinking_factor: 0.75\n",
      "  max_sentence_length: 4192\n",
      "  num_threads: 16\n",
      "  num_sub_iterations: 2\n",
      "  max_sentencepiece_length: 16\n",
      "  split_by_unicode_script: 1\n",
      "  split_by_number: 1\n",
      "  split_by_whitespace: 1\n",
      "  split_digits: 0\n",
      "  pretokenization_delimiter: \n",
      "  treat_whitespace_as_suffix: 0\n",
      "  allow_whitespace_only_pieces: 0\n",
      "  user_defined_symbols: <pad>\n",
      "  user_defined_symbols: <sos>\n",
      "  user_defined_symbols: <eos>\n",
      "  required_chars: \n",
      "  byte_fallback: 0\n",
      "  vocabulary_output_piece_score: 1\n",
      "  train_extremely_large_corpus: 0\n",
      "  seed_sentencepieces_file: \n",
      "  hard_vocab_limit: 1\n",
      "  use_all_vocab: 0\n",
      "  unk_id: 0\n",
      "  bos_id: 1\n",
      "  eos_id: 2\n",
      "  pad_id: -1\n",
      "  unk_piece: <unk>\n",
      "  bos_piece: <s>\n",
      "  eos_piece: </s>\n",
      "  pad_piece: <pad>\n",
      "  unk_surface:  ⁇ \n",
      "  enable_differential_privacy: 0\n",
      "  differential_privacy_noise_level: 0\n",
      "  differential_privacy_clipping_threshold: 0\n",
      "}\n",
      "normalizer_spec {\n",
      "  name: nmt_nfkc\n",
      "  add_dummy_prefix: 1\n",
      "  remove_extra_whitespaces: 1\n",
      "  escape_whitespaces: 1\n",
      "  normalization_rule_tsv: \n",
      "}\n",
      "denormalizer_spec {}\n",
      "trainer_interface.cc(355) LOG(INFO) SentenceIterator is not specified. Using MultiFileSentenceIterator.\n",
      "trainer_interface.cc(186) LOG(INFO) Loading corpus: chatbot_corpus.txt\n",
      "trainer_interface.cc(411) LOG(INFO) Loaded all 23646 sentences\n",
      "trainer_interface.cc(427) LOG(INFO) Adding meta_piece: <unk>\n",
      "trainer_interface.cc(427) LOG(INFO) Adding meta_piece: <s>\n",
      "trainer_interface.cc(427) LOG(INFO) Adding meta_piece: </s>\n",
      "trainer_interface.cc(427) LOG(INFO) Adding meta_piece: <pad>\n",
      "trainer_interface.cc(427) LOG(INFO) Adding meta_piece: <sos>\n",
      "trainer_interface.cc(427) LOG(INFO) Adding meta_piece: <eos>\n",
      "trainer_interface.cc(432) LOG(INFO) Normalizing sentences...\n",
      "trainer_interface.cc(541) LOG(INFO) all chars count=353449\n",
      "trainer_interface.cc(552) LOG(INFO) Done: 99.9502% characters are covered.\n",
      "trainer_interface.cc(562) LOG(INFO) Alphabet size=1085\n",
      "trainer_interface.cc(563) LOG(INFO) Final character coverage=0.999502\n",
      "trainer_interface.cc(594) LOG(INFO) Done! preprocessed 23646 sentences.\n",
      "trainer_interface.cc(600) LOG(INFO) Tokenizing input sentences with whitespace: 23646\n",
      "trainer_interface.cc(611) LOG(INFO) Done! 21781\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=3353 min_freq=42\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1031 size=20 all=16243 active=1813 piece=▁생\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=759 size=40 all=16956 active=2526 piece=▁너\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=572 size=60 all=17594 active=3164 piece=해보세요\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=422 size=80 all=18261 active=3831 piece=나봐요\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=352 size=100 all=18985 active=4555 piece=▁먹\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=352 min_freq=37\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=296 size=120 all=19482 active=1470 piece=▁될\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=261 size=140 all=20011 active=1999 piece=▁진\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=225 size=160 all=20411 active=2399 piece=▁필요\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=201 size=180 all=20741 active=2729 piece=▁짝남\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=186 size=200 all=21115 active=3103 piece=사랑\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=185 min_freq=31\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=169 size=220 all=21471 active=1386 piece=▁싫\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=154 size=240 all=21802 active=1717 piece=▁것도\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=143 size=260 all=22127 active=2042 piece=지만\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=135 size=280 all=22508 active=2423 piece=까지\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=129 size=300 all=22837 active=2752 piece=해서\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=129 min_freq=26\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=119 size=320 all=23202 active=1451 piece=▁귀\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=115 size=340 all=23536 active=1785 piece=해주세요\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=108 size=360 all=23793 active=2042 piece=보는\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=103 size=380 all=24076 active=2325 piece=▁쓰\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=99 size=400 all=24497 active=2746 piece=▁바랍니다\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=98 min_freq=22\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=91 size=420 all=24773 active=1501 piece=▁말해보세요\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=87 size=440 all=25016 active=1744 piece=▁있었\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=84 size=460 all=25270 active=1998 piece=▁사람은\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=81 size=480 all=25523 active=2251 piece=▁몰라요\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=77 size=500 all=25761 active=2489 piece=▁운동\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=77 min_freq=20\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=74 size=520 all=25965 active=1475 piece=▁처\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=71 size=540 all=26195 active=1705 piece=▁나한테\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=67 size=560 all=26458 active=1968 piece=▁질\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=66 size=580 all=26656 active=2166 piece=▁생각을\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=64 size=600 all=26813 active=2323 piece=나봅니다\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=64 min_freq=18\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=61 size=620 all=27009 active=1513 piece=기를\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=59 size=640 all=27280 active=1784 piece=▁난\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=57 size=660 all=27504 active=2008 piece=▁목\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=55 size=680 all=27686 active=2190 piece=▁나는\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=53 size=700 all=27820 active=2324 piece=▁대화를\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=53 min_freq=16\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=51 size=720 all=27968 active=1539 piece=동안\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=50 size=740 all=28120 active=1691 piece=▁말해\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=49 size=760 all=28250 active=1821 piece=▁사랑의\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=47 size=780 all=28429 active=2000 piece=▁몇\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=46 size=800 all=28531 active=2102 piece=▁빨\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=46 min_freq=14\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=45 size=820 all=28637 active=1527 piece=신이\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=44 size=840 all=28795 active=1685 piece=▁거라\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=43 size=860 all=28925 active=1815 piece=▁한번\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=41 size=880 all=28985 active=1875 piece=▁강\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=40 size=900 all=29143 active=2033 piece=▁욕\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=40 min_freq=13\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=39 size=920 all=29254 active=1555 piece=▁면\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=38 size=940 all=29428 active=1729 piece=다니\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=37 size=960 all=29587 active=1888 piece=▁4\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=37 size=980 all=29734 active=2035 piece=▁지금도\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=36 size=1000 all=29869 active=2170 piece=▁솔직\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=36 min_freq=13\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=35 size=1020 all=30029 active=1643 piece=▁것이\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=34 size=1040 all=30173 active=1787 piece=▁내려\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=33 size=1060 all=30281 active=1895 piece=▁책\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=32 size=1080 all=30430 active=2044 piece=▁능\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=32 size=1100 all=30540 active=2154 piece=▁친구들\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=32 min_freq=12\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=31 size=1120 all=30612 active=1594 piece=▁않죠\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=30 size=1140 all=30726 active=1708 piece=▁아는\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=29 size=1160 all=30828 active=1810 piece=하길\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=29 size=1180 all=30926 active=1908 piece=▁원하는\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=28 size=1200 all=31034 active=2016 piece=▁되지\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=28 min_freq=11\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=28 size=1220 all=31093 active=1611 piece=▁사랑했던\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=27 size=1240 all=31195 active=1713 piece=▁죽을\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=26 size=1260 all=31219 active=1737 piece=▁숨\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=26 size=1280 all=31316 active=1834 piece=▁월급\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=26 size=1300 all=31377 active=1895 piece=▁이별의\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=26 min_freq=10\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=25 size=1320 all=31504 active=1695 piece=이가\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=25 size=1340 all=31613 active=1804 piece=▁약속\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=25 size=1360 all=31667 active=1858 piece=▁남자애가\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=24 size=1380 all=31759 active=1950 piece=▁미치\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=24 size=1400 all=31833 active=2024 piece=▁알려줘\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=24 min_freq=10\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=23 size=1420 all=31882 active=1639 piece=▁먹을\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=23 size=1440 all=32002 active=1759 piece=▁못하는\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=22 size=1460 all=32067 active=1824 piece=▁곳이\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=22 size=1480 all=32115 active=1872 piece=▁해서\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=22 size=1500 all=32174 active=1931 piece=▁않는다면\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=22 min_freq=9\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=21 size=1520 all=32310 active=1744 piece=▁기본\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=21 size=1540 all=32391 active=1825 piece=드려요\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=21 size=1560 all=32420 active=1854 piece=▁자연스러운\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=20 size=1580 all=32524 active=1958 piece=야할\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=20 size=1600 all=32634 active=2068 piece=▁집중\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=20 min_freq=9\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=20 size=1620 all=32670 active=1661 piece=▁준비가\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=19 size=1640 all=32702 active=1693 piece=치가\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=19 size=1660 all=32809 active=1800 piece=▁오빠\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=19 size=1680 all=32895 active=1886 piece=▁받았어\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=18 size=1700 all=32904 active=1895 piece=▁걷\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=18 min_freq=9\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=18 size=1720 all=33032 active=1765 piece=이요\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=18 size=1740 all=33119 active=1852 piece=▁애랑\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=18 size=1760 all=33175 active=1908 piece=▁아름다\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=17 size=1780 all=33192 active=1925 piece=고생\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=17 size=1800 all=33328 active=2061 piece=▁나타\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=17 min_freq=8\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=17 size=1820 all=33384 active=1719 piece=▁있고\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=17 size=1840 all=33436 active=1771 piece=▁한동안\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=16 size=1860 all=33477 active=1812 piece=냐고\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=16 size=1880 all=33583 active=1918 piece=▁다니\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=16 size=1900 all=33636 active=1971 piece=▁없애\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=16 min_freq=8\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=16 size=1920 all=33714 active=1754 piece=▁SNS\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=16 size=1940 all=33731 active=1771 piece=▁사랑하는데\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=15 size=1960 all=33787 active=1827 piece=어서\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=15 size=1980 all=33876 active=1916 piece=▁되면\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=15 size=2000 all=33944 active=1984 piece=▁주지\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=15 min_freq=8\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=15 size=2020 all=33983 active=1737 piece=▁안해도\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=15 size=2040 all=33992 active=1746 piece=▁좋아할까\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=14 size=2060 all=34032 active=1786 piece=간이\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=14 size=2080 all=34183 active=1937 piece=▁나의\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=14 size=2100 all=34229 active=1983 piece=▁왔다\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=14 min_freq=7\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=14 size=2120 all=34256 active=1737 piece=▁무서운\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=14 size=2140 all=34251 active=1732 piece=▁사랑해서\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=14 size=2160 all=34239 active=1720 piece=▁이해해주세���\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=13 size=2180 all=34312 active=1793 piece=방법\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=13 size=2200 all=34434 active=1915 piece=▁남을\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=13 min_freq=7\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=13 size=2220 all=34471 active=1757 piece=▁생활\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=13 size=2240 all=34543 active=1829 piece=▁줘도\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=13 size=2260 all=34601 active=1887 piece=▁기념일\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=13 size=2280 all=34609 active=1895 piece=▁연애를\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=13 size=2300 all=34617 active=1903 piece=▁당당하게\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=13 min_freq=7\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=13 size=2320 all=34614 active=1728 piece=▁헤어졌는데\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=12 size=2340 all=34677 active=1791 piece=렸어\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=12 size=2360 all=34751 active=1865 piece=▁다들\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=12 size=2380 all=34779 active=1893 piece=▁신고\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=12 size=2400 all=34824 active=1938 piece=▁편한\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=12 min_freq=7\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=12 size=2420 all=34855 active=1773 piece=▁만난지\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=12 size=2440 all=34850 active=1768 piece=▁운동을\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=12 size=2460 all=34840 active=1758 piece=▁아니라면\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=12 size=2480 all=34825 active=1743 piece=▁적극적으로\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=11 size=2500 all=34859 active=1777 piece=▁헬\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=11 min_freq=6\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=11 size=2520 all=34960 active=1843 piece=전환\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=11 size=2540 all=34998 active=1881 piece=▁나쁘\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=11 size=2560 all=35032 active=1915 piece=▁사용\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=11 size=2580 all=35074 active=1957 piece=▁움직\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=11 size=2600 all=35130 active=2013 piece=싶은데\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=11 min_freq=6\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=11 size=2620 all=35160 active=1779 piece=▁모습이\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=11 size=2640 all=35159 active=1778 piece=▁정리할\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=11 size=2660 all=35167 active=1786 piece=▁썸남이랑\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=11 size=2680 all=35155 active=1774 piece=▁고백해보세요\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=10 size=2700 all=35194 active=1813 piece=갔어\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=10 min_freq=6\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=10 size=2720 all=35278 active=1840 piece=인이\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=10 size=2740 all=35328 active=1890 piece=▁다되\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=10 size=2760 all=35371 active=1933 piece=▁보기\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=10 size=2780 all=35410 active=1972 piece=▁이불\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=10 size=2800 all=35457 active=2019 piece=이라니\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=10 min_freq=6\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=10 size=2820 all=35493 active=1802 piece=▁들어가\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=10 size=2840 all=35489 active=1798 piece=��앞머리\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=10 size=2860 all=35479 active=1788 piece=▁피곤해\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=10 size=2880 all=35487 active=1796 piece=▁스트레칭\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=10 size=2900 all=35477 active=1786 piece=▁믿어보세요\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=10 min_freq=6\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=9 size=2920 all=35509 active=1806 piece=▁띄\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=9 size=2940 all=35585 active=1882 piece=렌타\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=9 size=2960 all=35702 active=1999 piece=함을\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=9 size=2980 all=35735 active=2032 piece=▁데는\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=9 size=3000 all=35759 active=2056 piece=▁삶을\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=9 min_freq=5\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=9 size=3020 all=35778 active=1807 piece=▁외모\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=9 size=3040 all=35812 active=1841 piece=▁회원\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=9 size=3060 all=35887 active=1916 piece=▁감정의\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=9 size=3080 all=35886 active=1915 piece=▁되는지\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=9 size=3100 all=35875 active=1904 piece=▁알아차\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=9 min_freq=5\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=9 size=3120 all=35870 active=1786 piece=▁재미가\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=9 size=3140 all=35863 active=1779 piece=▁기다리면\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=9 size=3160 all=35859 active=1775 piece=▁있었으면\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=9 size=3180 all=35852 active=1768 piece=▁감사합니다\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=8 size=3200 all=35832 active=1748 piece=▁A\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=8 min_freq=5\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=8 size=3220 all=35870 active=1828 piece=겹살\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=8 size=3240 all=35938 active=1896 piece=이스\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=8 size=3260 all=35988 active=1946 piece=▁그립\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=8 size=3280 all=36016 active=1974 piece=▁로또\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=8 size=3300 all=36050 active=2008 piece=▁사면\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=8 min_freq=5\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=8 size=3320 all=36087 active=1840 piece=▁어이\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=8 size=3340 all=36113 active=1866 piece=▁적금\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=8 size=3360 all=36147 active=1900 piece=���한숨\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=8 size=3380 all=36214 active=1967 piece=을텐데\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=8 size=3400 all=36240 active=1993 piece=▁끝났어\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=8 min_freq=5\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=8 size=3420 all=36235 active=1807 piece=▁마시면\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=8 size=3440 all=36225 active=1797 piece=▁성공할\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=8 size=3460 all=36217 active=1789 piece=▁우연히\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=8 size=3480 all=36212 active=1784 piece=▁회사와\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=8 size=3500 all=36223 active=1795 piece=▁드릴게요\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=8 min_freq=5\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=8 size=3520 all=36216 active=1805 piece=▁좋아하나\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=8 size=3540 all=36205 active=1794 piece=▁사랑이네요\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=7 size=3560 all=36191 active=1780 piece=▁끓\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=7 size=3580 all=36232 active=1821 piece=▁흥\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=7 size=3600 all=36287 active=1876 piece=렸을\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=7 min_freq=5\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=7 size=3620 all=36363 active=1886 piece=장에\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=7 size=3640 all=36388 active=1911 piece=▁건조\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=7 size=3660 all=36402 active=1925 piece=▁노는\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=7 size=3680 all=36425 active=1948 piece=▁발목\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=7 size=3700 all=36445 active=1968 piece=▁쓰지\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=7 min_freq=4\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=7 size=3720 all=36457 active=1835 piece=▁있다\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=7 size=3740 all=36474 active=1852 piece=▁충격\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=7 size=3760 all=36519 active=1897 piece=에서도\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=7 size=3780 all=36561 active=1939 piece=▁경험이\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=7 size=3800 all=36545 active=1923 piece=▁될지도\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=7 min_freq=4\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=7 size=3820 all=36543 active=1826 piece=▁번호를\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=7 size=3840 all=36533 active=1816 piece=▁아니고\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=7 size=3860 all=36527 active=1810 piece=▁운명이\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=7 size=3880 all=36528 active=1811 piece=▁처음에\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=7 size=3900 all=36523 active=1806 piece=하니까요\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=7 min_freq=4\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=7 size=3920 all=36515 active=1814 piece=▁사람인데\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=7 size=3940 all=36501 active=1800 piece=▁잡으세요\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=7 size=3960 all=36496 active=1795 piece=▁가능합니다\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=7 size=3980 all=36479 active=1778 piece=▁이야기해도\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=6 size=4000 all=36472 active=1771 piece=▁광\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=6 min_freq=4\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=6 size=4020 all=36496 active=1845 piece=거같\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=6 size=4040 all=36558 active=1907 piece=라서\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=6 size=4060 all=36619 active=1968 piece=작정\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=6 size=4080 all=36673 active=2022 piece=해를\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=6 size=4100 all=36692 active=2041 piece=▁고장\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=6 min_freq=4\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=6 size=4120 all=36713 active=1854 piece=▁단순\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=6 size=4140 all=36735 active=1876 piece=▁맞춤\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=6 size=4160 all=36750 active=1891 piece=▁삶의\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=6 size=4180 all=36764 active=1905 piece=▁쌩얼\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=6 size=4200 all=36780 active=1921 piece=▁온다\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=6 min_freq=4\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=6 size=4220 all=36793 active=1852 piece=▁입이\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=6 size=4240 all=36806 active=1865 piece=▁진지\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=6 size=4260 all=36839 active=1898 piece=▁티가\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=6 size=4280 all=36852 active=1911 piece=것같아\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=6 size=4300 all=36900 active=1959 piece=으려면\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=6 min_freq=4\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=6 size=4320 all=36942 active=1883 piece=▁건가요\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=6 size=4340 all=36930 active=1871 piece=▁기대가\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=6 size=4360 all=36916 active=1857 piece=▁되겠지\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=6 size=4380 all=36906 active=1847 piece=▁무거워\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=6 size=4400 all=36897 active=1838 piece=▁사람만\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=6 min_freq=4\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=6 size=4420 all=36881 active=1829 piece=▁아프고\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=6 size=4440 all=36868 active=1816 piece=▁의미가\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=6 size=4460 all=36863 active=1811 piece=▁중요할\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=6 size=4480 all=36859 active=1807 piece=▁터놓고\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=6 size=4500 all=36848 active=1796 piece=었겠네요\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=6 min_freq=4\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=6 size=4520 all=36846 active=1838 piece=▁다가가는\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=6 size=4540 all=36836 active=1828 piece=▁생각나고\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=6 size=4560 all=36830 active=1822 piece=▁오토바이\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=6 size=4580 all=36814 active=1806 piece=▁처음에는\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=6 size=4600 all=36807 active=1799 piece=▁마음이라도\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=6 min_freq=4\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=6 size=4620 all=36790 active=1824 piece=▁안타깝네요\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=6 size=4640 all=36773 active=1807 piece=▁시간이겠네요\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=5 size=4660 all=36768 active=1802 piece=▁떡\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=5 size=4680 all=36798 active=1832 piece=▁탓\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=5 size=4700 all=36843 active=1877 piece=되기\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=5 min_freq=4\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=5 size=4720 all=36886 active=1883 piece=생각\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=5 size=4740 all=36954 active=1951 piece=은거\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=5 size=4760 all=36999 active=1996 piece=한번\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=5 size=4780 all=37012 active=2009 piece=▁괜한\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=5 size=4800 all=37031 active=2028 piece=▁낭만\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=5 min_freq=4\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=5 size=4820 all=37030 active=1849 piece=▁등산\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=5 size=4840 all=37037 active=1856 piece=▁무한\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=5 size=4860 all=37045 active=1864 piece=▁봄은\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=5 size=4880 all=37057 active=1876 piece=▁수업\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=5 size=4900 all=37070 active=1889 piece=▁어깨\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=5 min_freq=4\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=5 size=4920 all=37089 active=1873 piece=▁작아\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=5 size=4940 all=37107 active=1891 piece=▁짧게\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=5 size=4960 all=37126 active=1910 piece=▁표정\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=5 size=4980 all=37160 active=1944 piece=릅니다\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=5 size=5000 all=37206 active=1990 piece=해졌어\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=5 min_freq=3\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=5 size=5020 all=37211 active=1862 piece=▁기다림\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=5 size=5040 all=37205 active=1856 piece=▁뒤숭숭\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=5 size=5060 all=37196 active=1847 piece=▁망했다\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=5 size=5080 all=37190 active=1841 piece=▁바뀌게\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=5 size=5100 all=37178 active=1829 piece=▁사라질\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=5 min_freq=3\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=5 size=5120 all=37167 active=1847 piece=▁아무말\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=5 size=5140 all=37162 active=1842 piece=▁오빠가\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=5 size=5160 all=37151 active=1831 piece=▁주면서\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=5 size=5180 all=37144 active=1824 piece=▁친해질\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=5 size=5200 all=37131 active=1811 piece=▁행운을\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=5 min_freq=3\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=5 size=5220 all=37147 active=1873 piece=▁것입니다\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=5 size=5240 all=37134 active=1860 piece=▁만나는데\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=5 size=5260 all=37117 active=1843 piece=▁생각해서\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=5 size=5280 all=37106 active=1832 piece=▁없겠지요\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=5 size=5300 all=37092 active=1818 piece=▁있을게요\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=5 min_freq=3\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=5 size=5320 all=37076 active=1839 piece=▁짝사랑을\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=5 size=5340 all=37070 active=1833 piece=▁걱정된다고\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=5 size=5360 all=37050 active=1813 piece=▁상황이네요\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=5 size=5380 all=37032 active=1795 piece=▁프라이버시\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=4 size=5400 all=37020 active=1783 piece=▁갠\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=4 min_freq=3\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=4 size=5420 all=37043 active=1874 piece=▁뻑\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=4 size=5440 all=37056 active=1887 piece=가가\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=4 size=5460 all=37097 active=1928 piece=꺼야\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=4 size=5480 all=37134 active=1965 piece=리죠\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=4 size=5500 all=37162 active=1993 piece=뻑해\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=4 min_freq=3\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=4 size=5520 all=37196 active=1892 piece=영상\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=4 size=5540 all=37239 active=1935 piece=질뻔\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=4 size=5560 all=37275 active=1971 piece=했나\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=4 size=5580 all=37287 active=1983 piece=▁걷고\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=4 size=5600 all=37288 active=1984 piece=▁끈���\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=4 min_freq=3\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=4 size=5620 all=37285 active=1861 piece=▁누락\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=4 size=5640 all=37286 active=1862 piece=▁드릴\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=4 size=5660 all=37281 active=1857 piece=▁먹은\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=4 size=5680 all=37290 active=1866 piece=▁바꿨\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=4 size=5700 all=37303 active=1879 piece=▁부질\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=4 min_freq=3\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=4 size=5720 all=37324 active=1884 piece=▁소름\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=4 size=5740 all=37339 active=1899 piece=▁안부\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=4 size=5760 all=37343 active=1903 piece=▁오려\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=4 size=5780 all=37344 active=1904 piece=▁음료\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=4 size=5800 all=37360 active=1920 piece=▁적당\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=4 min_freq=3\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=4 size=5820 all=37372 active=1878 piece=▁지속\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=4 size=5840 all=37387 active=1893 piece=▁축제\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=4 size=5860 all=37389 active=1895 piece=▁패션\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=4 size=5880 all=37397 active=1903 piece=▁환기\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=4 size=5900 all=37415 active=1921 piece=보려고\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=4 min_freq=3\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=4 size=5920 all=37452 active=1904 piece=하려면\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=4 size=5940 all=37473 active=1925 piece=▁각질제\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=4 size=5960 all=37458 active=1910 piece=▁곳까지\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=4 size=5980 all=37442 active=1894 piece=▁기울여\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=4 size=6000 all=37433 active=1885 piece=▁노래는\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=4 min_freq=3\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=4 size=6020 all=37419 active=1858 piece=▁되어요\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=4 size=6040 all=37410 active=1849 piece=▁막히는\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=4 size=6060 all=37400 active=1839 piece=▁먹는데\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=4 size=6080 all=37386 active=1825 piece=▁발전이\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=4 size=6100 all=37373 active=1812 piece=▁불편한\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=4 min_freq=3\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=4 size=6120 all=37359 active=1855 piece=▁선택이\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=4 size=6140 all=37353 active=1849 piece=▁식습관\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=4 size=6160 all=37339 active=1835 piece=▁아픔도\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=4 size=6180 all=37330 active=1826 piece=▁어리석\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=4 size=6200 all=37317 active=1813 piece=▁오니까\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=4 min_freq=3\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=4 size=6220 all=37306 active=1855 piece=▁이러는\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=4 size=6240 all=37298 active=1847 piece=▁있군요\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=4 size=6260 all=37294 active=1843 piece=▁정리해\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=4 size=6280 all=37278 active=1827 piece=▁지금의\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=4 size=6300 all=37263 active=1812 piece=▁콩깍지\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=4 min_freq=3\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=4 size=6320 all=37245 active=1846 piece=▁해줄까\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=4 size=6340 all=37238 active=1839 piece=받으세요\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=4 size=6360 all=37259 active=1860 piece=▁간사하죠\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=4 size=6380 all=37248 active=1849 piece=▁괜찮을거\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=4 size=6400 all=37232 active=1833 piece=▁너무너무\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=4 min_freq=3\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=4 size=6420 all=37215 active=1844 piece=▁들리려고\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=4 size=6440 all=37196 active=1825 piece=▁물어봐도\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=4 size=6460 all=37184 active=1813 piece=▁사람이야\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=4 size=6480 all=37167 active=1796 piece=▁스테이크\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=4 size=6500 all=37152 active=1781 piece=▁않겠네요\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=4 min_freq=3\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=4 size=6520 all=37135 active=1841 piece=▁오래오래\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=4 size=6540 all=37119 active=1825 piece=▁이해하고\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=4 size=6560 all=37106 active=1812 piece=▁존중하고\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=4 size=6580 all=37090 active=1796 piece=▁컴퓨터가\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=4 size=6600 all=37072 active=1778 piece=▁핸드폰이\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=4 min_freq=3\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=4 size=6620 all=37063 active=1845 piece=▁갑작스러운\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=4 size=6640 all=37047 active=1829 piece=▁다행이네요\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=4 size=6660 all=37027 active=1809 piece=▁병원가세요\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=4 size=6680 all=37009 active=1791 piece=▁소개팅하고\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=4 size=6700 all=36989 active=1771 piece=▁연락해보는\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=4 min_freq=3\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=4 size=6720 all=36970 active=1831 piece=▁정정기간을\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=4 size=6740 all=36953 active=1814 piece=▁하우스웨딩\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=4 size=6760 all=36935 active=1796 piece=▁부딪혔나봐요\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=4 size=6780 all=36915 active=1776 piece=▁힘드시���어요\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=3 size=6800 all=36912 active=1773 piece=▁넋\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=3 min_freq=3\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=3 size=6820 all=36928 active=1861 piece=▁앱\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=3 size=6840 all=36939 active=1872 piece=▁팩\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=3 size=6860 all=36959 active=1892 piece=계절\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=3 size=6880 all=36986 active=1919 piece=낭비\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=3 size=6900 all=37008 active=1941 piece=동차\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=3 min_freq=2\n",
      "trainer_interface.cc(689) LOG(INFO) Saving model: chatbot_spm.model\n",
      "trainer_interface.cc(701) LOG(INFO) Saving vocabs: chatbot_spm.vocab\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "길이 제한 후: 11823개 쌍\n",
      "중복 제거 후: 7737개 쌍\n",
      "최종 토큰화 완료: 7737개 문장 쌍\n",
      "\n",
      "토큰화 결과 샘플:\n",
      "질문 토큰: [5568, 6957, 3209, 7063]...\n",
      "답변 토큰: [4491, 213, 5938, 6916]...\n",
      "복원 질문: 12시 땡!\n",
      "복원 답변: 하루가 또 가네요.\n",
      "--------------------------------------------------\n",
      "질문 토큰: [346, 6924, 7230, 1008, 2460]...\n",
      "답변 토큰: [1621, 6424, 6916]...\n",
      "복원 질문: 1지망 학교 떨어졌어\n",
      "복원 답변: 위로해 드립니다.\n",
      "--------------------------------------------------\n",
      "질문 토큰: [472, 7425, 7392, 6965, 3502, 200]...\n",
      "답변 토큰: [5138, 1359, 381, 6916]...\n",
      "복원 질문: 3박4일 놀러가고 싶다\n",
      "복원 답변: 여행은 언제나 좋죠.\n",
      "--------------------------------------------------\n",
      "\n",
      "특수 토큰 ID:\n",
      "PAD: 3, SOS: 4, EOS: 5\n"
     ]
    }
   ],
   "source": [
    "def preprocess_sentence(sentence):\n",
    "    \"\"\"문장 전처리\"\"\"\n",
    "    sentence = sentence.lower()\n",
    "    sentence = re.sub(r'[^a-z가-힣0-9.,?!;:\\-()\"\\'\\s]', '', sentence)\n",
    "    sentence = re.sub(r'\\s+', ' ', sentence)\n",
    "    return sentence.strip()\n",
    "\n",
    "def build_corpus(source_sentences, target_sentences, tokenize_fn, max_length=40):\n",
    "    \"\"\"\n",
    "    소스 문장과 타겟 문장을 정제하고 토큰화하여 코퍼스를 생성하는 함수\n",
    "    \n",
    "    Args:\n",
    "        source_sentences: 소스 문장 리스트 (예: 질문들)\n",
    "        target_sentences: 타겟 문장 리스트 (예: 답변들)\n",
    "        tokenize_fn: 토큰화 함수 (tokenizer.encode_as_ids)\n",
    "        max_length: 최대 토큰 길이 (기본값: 40)\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (source_corpus, target_corpus) - 토큰화된 코퍼스\n",
    "    \"\"\"\n",
    "    print(f\"코퍼스 생성 시작 - 입력 데이터: {len(source_sentences)}개 쌍\")\n",
    "    \n",
    "    # 1. 전처리 및 토큰화\n",
    "    processed_sources = []\n",
    "    processed_targets = []\n",
    "    source_tokens_list = []\n",
    "    target_tokens_list = []\n",
    "    \n",
    "    for src, tgt in zip(source_sentences, target_sentences):\n",
    "        # 전처리\n",
    "        clean_src = preprocess_sentence(src)\n",
    "        clean_tgt = preprocess_sentence(tgt)\n",
    "        \n",
    "        # 토큰화\n",
    "        src_tokens = tokenize_fn(clean_src)\n",
    "        tgt_tokens = tokenize_fn(clean_tgt)\n",
    "        \n",
    "        # 길이 제한 확인\n",
    "        if len(src_tokens) <= max_length and len(tgt_tokens) <= max_length:\n",
    "            processed_sources.append(clean_src)\n",
    "            processed_targets.append(clean_tgt)\n",
    "            source_tokens_list.append(src_tokens)\n",
    "            target_tokens_list.append(tgt_tokens)\n",
    "    \n",
    "    print(f\"길이 제한 후: {len(processed_sources)}개 쌍\")\n",
    "    \n",
    "    # 2. 중복 제거 (소스와 타겟 각각 독립적으로, 하지만 쌍은 유지)\n",
    "    seen_sources = set()\n",
    "    seen_targets = set()\n",
    "    final_source_corpus = []\n",
    "    final_target_corpus = []\n",
    "    \n",
    "    for i, (src_text, tgt_text) in enumerate(zip(processed_sources, processed_targets)):\n",
    "        # 소스나 타겟 중 하나라도 중복이면 해당 쌍 전체 제외\n",
    "        if src_text not in seen_sources and tgt_text not in seen_targets:\n",
    "            seen_sources.add(src_text)\n",
    "            seen_targets.add(tgt_text)\n",
    "            final_source_corpus.append(source_tokens_list[i])\n",
    "            final_target_corpus.append(target_tokens_list[i])\n",
    "    \n",
    "    print(f\"중복 제거 후: {len(final_source_corpus)}개 쌍\")\n",
    "    \n",
    "    return final_source_corpus, final_target_corpus\n",
    "\n",
    "def process_chatbot_data(questions, answers):\n",
    "    \"\"\"ChatBot 데이터 토큰화 (build_corpus 함수 활용)\"\"\"\n",
    "    print(\"SentencePiece 토크나이저 훈련 시작\")\n",
    "    \n",
    "    # 1. 훈련용 텍스트 파일 생성\n",
    "    with open('chatbot_corpus.txt', 'w', encoding='utf-8') as f:\n",
    "        for q, a in zip(questions, answers):\n",
    "            f.write(f\"{q}\\n\")\n",
    "            f.write(f\"{a}\\n\")\n",
    "    \n",
    "    # 2. SentencePiece 모델 훈련\n",
    "    spm.SentencePieceTrainer.train(\n",
    "        input='chatbot_corpus.txt',\n",
    "        model_prefix='chatbot_spm',\n",
    "        vocab_size=8000,\n",
    "        model_type='bpe',\n",
    "        user_defined_symbols='<pad>,<sos>,<eos>'\n",
    "    )\n",
    "    \n",
    "    # 3. 토크나이저 로드\n",
    "    tokenizer = spm.SentencePieceProcessor()\n",
    "    tokenizer.load('chatbot_spm.model')\n",
    "    \n",
    "    print(\"토크나이저 훈련 완료\")\n",
    "    print(f\"어휘 사전 크기: {tokenizer.get_piece_size()}\")\n",
    "    \n",
    "    # 4. build_corpus 함수를 사용하여 데이터 토큰화\n",
    "    que_corpus, ans_corpus = build_corpus(\n",
    "        source_sentences=questions,\n",
    "        target_sentences=answers,\n",
    "        tokenize_fn=tokenizer.encode_as_ids,\n",
    "        max_length=40\n",
    "    )\n",
    "    \n",
    "    print(f\"최종 토큰화 완료: {len(que_corpus)}개 문장 쌍\")\n",
    "    \n",
    "    # 5. 샘플 확인\n",
    "    print(\"\\n토큰화 결과 샘플:\")\n",
    "    for i in range(min(3, len(que_corpus))):\n",
    "        print(f\"질문 토큰: {que_corpus[i][:10]}...\")\n",
    "        print(f\"답변 토큰: {ans_corpus[i][:10]}...\")\n",
    "        print(f\"복원 질문: {tokenizer.decode_ids(que_corpus[i])}\")\n",
    "        print(f\"복원 답변: {tokenizer.decode_ids(ans_corpus[i])}\")\n",
    "        print(\"-\" * 50)\n",
    "    \n",
    "    return que_corpus, ans_corpus, tokenizer\n",
    "\n",
    "# ==============================================\n",
    "# 실행 부분\n",
    "# ==============================================\n",
    "\n",
    "# 먼저 데이터가 있는지 확인\n",
    "if 'questions' in globals() and 'answers' in globals():\n",
    "    print(\"데이터 발견! 토큰화 시작...\")\n",
    "    que_corpus, ans_corpus, tokenizer = process_chatbot_data(questions, answers)\n",
    "    \n",
    "    # 특수 토큰 ID 확인\n",
    "    PAD_IDX = tokenizer.piece_to_id('<pad>')\n",
    "    SOS_IDX = tokenizer.piece_to_id('<sos>')\n",
    "    EOS_IDX = tokenizer.piece_to_id('<eos>')\n",
    "    \n",
    "    print(f\"\\n특수 토큰 ID:\")\n",
    "    print(f\"PAD: {PAD_IDX}, SOS: {SOS_IDX}, EOS: {EOS_IDX}\")\n",
    "    \n",
    "else:\n",
    "    print(\"❌ questions, answers 변수가 없습니다!\")\n",
    "    print(\"💡 먼저 Step 1(데이터 로드)를 실행해주세요:\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4fe080a0-c48e-4dfd-ae7b-3b1687740892",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 6189개, Test: 1548개\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def split_data(que_corpus, ans_corpus, test_size=0.2, random_state=42):\n",
    "    \"\"\"토큰화된 데이터를 train/test로 분할\"\"\"\n",
    "    \n",
    "    train_que, test_que, train_ans, test_ans = train_test_split(\n",
    "        que_corpus, ans_corpus, \n",
    "        test_size=test_size, \n",
    "        random_state=random_state\n",
    "    )\n",
    "    \n",
    "    print(f\"Train: {len(train_que)}개, Test: {len(test_que)}개\")\n",
    "    \n",
    "    return train_que, test_que, train_ans, test_ans\n",
    "\n",
    "# 실행\n",
    "if 'que_corpus' in globals() and 'ans_corpus' in globals():\n",
    "    train_que, test_que, train_ans, test_ans = split_data(que_corpus, ans_corpus)\n",
    "else:\n",
    "    print(\"que_corpus, ans_corpus 변수가 없습니다!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bd79fbd3-0990-492a-a667-fcf41fd10395",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "슝=3\n"
     ]
    }
   ],
   "source": [
    "def generate_tokenizer(corpus,\n",
    "                       vocab_size,\n",
    "                       lang=\"chatbot\",\n",
    "                       pad_id=0,\n",
    "                       bos_id=1,\n",
    "                       eos_id=2,\n",
    "                       unk_id=3):\n",
    "    file = \"./%s_corpus.txt\" % lang\n",
    "    model = \"%s_spm\" % lang\n",
    "    with open(file, 'w') as f:\n",
    "        for row in corpus: f.write(str(row) + '\\n')\n",
    "    import sentencepiece as spm\n",
    "    spm.SentencePieceTrainer.Train(\n",
    "        '--input=%s --model_prefix=%s --vocab_size=%d --pad_id=%d --bos_id=%d --eos_id=%d --unk_id=%d'\n",
    "        % (file, model, vocab_size, pad_id, bos_id, eos_id, unk_id)\n",
    "    )\n",
    "    tokenizer = spm.SentencePieceProcessor()\n",
    "    tokenizer.Load('%s.model' % model)\n",
    "    return tokenizer\n",
    "\n",
    "print(\"슝=3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bc0af201-f011-47b6-a358-8481a4da1f4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sentencepiece_trainer.cc(178) LOG(INFO) Running command: --input=./chatbot_corpus.txt --model_prefix=chatbot_spm --vocab_size=8000 --pad_id=0 --bos_id=1 --eos_id=2 --unk_id=3\n",
      "sentencepiece_trainer.cc(78) LOG(INFO) Starts training with : \n",
      "trainer_spec {\n",
      "  input: ./chatbot_corpus.txt\n",
      "  input_format: \n",
      "  model_prefix: chatbot_spm\n",
      "  model_type: UNIGRAM\n",
      "  vocab_size: 8000\n",
      "  self_test_sample_size: 0\n",
      "  character_coverage: 0.9995\n",
      "  input_sentence_size: 0\n",
      "  shuffle_input_sentence: 1\n",
      "  seed_sentencepiece_size: 1000000\n",
      "  shrinking_factor: 0.75\n",
      "  max_sentence_length: 4192\n",
      "  num_threads: 16\n",
      "  num_sub_iterations: 2\n",
      "  max_sentencepiece_length: 16\n",
      "  split_by_unicode_script: 1\n",
      "  split_by_number: 1\n",
      "  split_by_whitespace: 1\n",
      "  split_digits: 0\n",
      "  pretokenization_delimiter: \n",
      "  treat_whitespace_as_suffix: 0\n",
      "  allow_whitespace_only_pieces: 0\n",
      "  required_chars: \n",
      "  byte_fallback: 0\n",
      "  vocabulary_output_piece_score: 1\n",
      "  train_extremely_large_corpus: 0\n",
      "  seed_sentencepieces_file: \n",
      "  hard_vocab_limit: 1\n",
      "  use_all_vocab: 0\n",
      "  unk_id: 3\n",
      "  bos_id: 1\n",
      "  eos_id: 2\n",
      "  pad_id: 0\n",
      "  unk_piece: <unk>\n",
      "  bos_piece: <s>\n",
      "  eos_piece: </s>\n",
      "  pad_piece: <pad>\n",
      "  unk_surface:  ⁇ \n",
      "  enable_differential_privacy: 0\n",
      "  differential_privacy_noise_level: 0\n",
      "  differential_privacy_clipping_threshold: 0\n",
      "}\n",
      "normalizer_spec {\n",
      "  name: nmt_nfkc\n",
      "  add_dummy_prefix: 1\n",
      "  remove_extra_whitespaces: 1\n",
      "  escape_whitespaces: 1\n",
      "  normalization_rule_tsv: \n",
      "}\n",
      "denormalizer_spec {}\n",
      "trainer_interface.cc(355) LOG(INFO) SentenceIterator is not specified. Using MultiFileSentenceIterator.\n",
      "trainer_interface.cc(186) LOG(INFO) Loading corpus: ./chatbot_corpus.txt\n",
      "trainer_interface.cc(411) LOG(INFO) Loaded all 23646 sentences\n",
      "trainer_interface.cc(427) LOG(INFO) Adding meta_piece: <pad>\n",
      "trainer_interface.cc(427) LOG(INFO) Adding meta_piece: <s>\n",
      "trainer_interface.cc(427) LOG(INFO) Adding meta_piece: </s>\n",
      "trainer_interface.cc(427) LOG(INFO) Adding meta_piece: <unk>\n",
      "trainer_interface.cc(432) LOG(INFO) Normalizing sentences...\n",
      "trainer_interface.cc(541) LOG(INFO) all chars count=353449\n",
      "trainer_interface.cc(552) LOG(INFO) Done: 99.9502% characters are covered.\n",
      "trainer_interface.cc(562) LOG(INFO) Alphabet size=1085\n",
      "trainer_interface.cc(563) LOG(INFO) Final character coverage=0.999502\n",
      "trainer_interface.cc(594) LOG(INFO) Done! preprocessed 23646 sentences.\n",
      "unigram_model_trainer.cc(265) LOG(INFO) Making suffix array...\n",
      "unigram_model_trainer.cc(269) LOG(INFO) Extracting frequent sub strings... node_num=162596\n",
      "unigram_model_trainer.cc(312) LOG(INFO) Initialized 18866 seed sentencepieces\n",
      "trainer_interface.cc(600) LOG(INFO) Tokenizing input sentences with whitespace: 23646\n",
      "trainer_interface.cc(611) LOG(INFO) Done! 21781\n",
      "unigram_model_trainer.cc(602) LOG(INFO) Using 21781 sentences for EM training\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=10912 obj=13.23 num_tokens=46285 num_tokens/piece=4.24166\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=9598 obj=12.125 num_tokens=46421 num_tokens/piece=4.83653\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=8798 obj=12.2749 num_tokens=47272 num_tokens/piece=5.37304\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=8785 obj=12.2502 num_tokens=47306 num_tokens/piece=5.38486\n",
      "trainer_interface.cc(689) LOG(INFO) Saving model: chatbot_spm.model\n",
      "trainer_interface.cc(701) LOG(INFO) Saving vocabs: chatbot_spm.vocab\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "VOCAB_SIZE = 8000\n",
    "corpus = questions + answers  # 리스트 합치기\n",
    "tokenizer = generate_tokenizer(corpus, VOCAB_SIZE, 'chatbot')\n",
    "tokenizer.set_encode_extra_options(\"bos:eos\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "379a3d61-dd5a-4085-be88-1adae823a860",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "슝=3\n"
     ]
    }
   ],
   "source": [
    "def make_corpus(sentences, tokenizer):\n",
    "    corpus = []\n",
    "    for sentence in tqdm(sentences):\n",
    "        tokens = tokenizer.encode_as_ids(sentence)\n",
    "        corpus.append(tokens)\n",
    "    return corpus\n",
    "\n",
    "print('슝=3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1c8c3c6c-7e4b-45ee-8111-72c82152891c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4a4d655a-5bf8-4102-9d1e-a3a2a14edbc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 11823/11823 [00:00<00:00, 152551.35it/s]\n",
      "100%|██████████| 11823/11823 [00:00<00:00, 158634.86it/s]\n"
     ]
    }
   ],
   "source": [
    "questions_corpus = make_corpus(questions, tokenizer)\n",
    "answers_corpus = make_corpus(answers, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d9c72dc6-b98c-4429-adb5-f7ce0fea0ce9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 9458개, Test: 2365개\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_que, test_que, train_ans, test_ans = train_test_split(\n",
    "    questions_corpus, answers_corpus, \n",
    "    test_size=0.2, \n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "print(f\"Train: {len(train_que)}개, Test: {len(test_que)}개\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a39f266b-b090-4947-be21-abfe1e45da4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([9458, 50])\n",
      "torch.Size([9458, 50])\n",
      "슝=3\n"
     ]
    }
   ],
   "source": [
    "MAX_LEN = 50\n",
    "def pad_sequences_custom(sequences, max_len=50, pad_value=0):\n",
    "   \"\"\"\n",
    "   sequences: list of list (각 문장별 토큰 ID 리스트)\n",
    "   max_len: 고정할 최대 시퀀스 길이\n",
    "   pad_value: 패딩에 사용할 값 (일반적으로 0)\n",
    "   \"\"\"\n",
    "   padded_sequences = []\n",
    "   for seq in sequences:\n",
    "       # 초과 길이는 자르고\n",
    "       if len(seq) > max_len:\n",
    "           seq = seq[:max_len]\n",
    "       # 부족한 길이는 pad_value로 채우기\n",
    "       else:\n",
    "           seq = seq + [pad_value] * (max_len - len(seq))\n",
    "       padded_sequences.append(seq)\n",
    "   # 최종적으로 torch.Tensor로 변환 (shape: [batch_size, max_len])\n",
    "   return torch.tensor(padded_sequences, dtype=torch.long)\n",
    "enc_ndarray = pad_sequences_custom(train_que, max_len=MAX_LEN, pad_value=0)\n",
    "dec_ndarray = pad_sequences_custom(train_ans, max_len=MAX_LEN, pad_value=0)\n",
    "print(enc_ndarray.shape)  # 예) [batch_size, 50]\n",
    "print(dec_ndarray.shape)  # 예) [batch_size, 50]\n",
    "print(\"슝=3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c0c9d3c1-e3b5-4c29-ba53-5f5a7afe03f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "슝=3\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "BATCH_SIZE = 64\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "train_dataset = TensorDataset(enc_ndarray, dec_ndarray)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, pin_memory=True)\n",
    "\n",
    "print(\"슝=3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "bf129650-a34b-4f86-b48a-7ca3f8748ce4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "슝=3\n"
     ]
    }
   ],
   "source": [
    "# Positional Encoding 구현\n",
    "def positional_encoding(pos, d_model):\n",
    "    def cal_angle(position, i):\n",
    "        return position / np.power(10000, (2*(i//2)) / np.float32(d_model))\n",
    "\n",
    "    def get_posi_angle_vec(position):\n",
    "        return [cal_angle(position, i) for i in range(d_model)]\n",
    "\n",
    "    sinusoid_table = np.array([get_posi_angle_vec(pos_i) for pos_i in range(pos)])\n",
    "\n",
    "    sinusoid_table[:, 0::2] = np.sin(sinusoid_table[:, 0::2])\n",
    "    sinusoid_table[:, 1::2] = np.cos(sinusoid_table[:, 1::2])\n",
    "\n",
    "    return sinusoid_table\n",
    "print(\"슝=3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "edeadc37-a718-43ca-b9e0-0b61375bb7ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "슝=3\n"
     ]
    }
   ],
   "source": [
    "def generate_padding_mask(seq: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    seq: shape [batch_size, seq_len]의 입력 (토큰 ID 텐서)\n",
    "    반환: shape [batch_size, 1, 1, seq_len]의 패딩 마스크\n",
    "         (seq == 0)인 위치가 1, 나머지는 0\n",
    "    \"\"\"\n",
    "    # (seq == 0)은 불리언 텐서를 반환 -> float()로 형변환 -> (1.0 or 0.0)\n",
    "    # 차원 확장: [batch_size, seq_len] → [batch_size, 1, 1, seq_len]\n",
    "    return (seq == 0).unsqueeze(1).unsqueeze(2).float()\n",
    "\n",
    "\n",
    "def generate_lookahead_mask(size: int) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    size: 문장(시퀀스) 길이\n",
    "    반환: shape [size, size],\n",
    "         i < j (대각선 위)에 해당하는 위치가 1, 아닌 곳은 0\n",
    "         (미래 토큰을 가리기 위한 마스크)\n",
    "    \"\"\"\n",
    "    # triu(diagonal=1)은 주대각선 위가 1, 아래가 0인 텐서를 만들어 줌\n",
    "    return torch.triu(torch.ones(size, size), diagonal=1)\n",
    "\n",
    "\n",
    "def generate_masks(src: torch.Tensor, tgt: torch.Tensor):\n",
    "    \"\"\"\n",
    "    src, tgt: shape [batch_size, seq_len]\n",
    "    3가지 마스크를 반환:\n",
    "      - enc_mask: 인코더 입력용 패딩 마스크\n",
    "      - dec_enc_mask: 디코더-인코더 어텐션용 패딩 마스크\n",
    "      - dec_mask: 디코더 자기어텐션용 마스크(룩어헤드 + 패딩)\n",
    "\n",
    "    각각의 shape:\n",
    "      - enc_mask, dec_enc_mask: [batch_size, 1, 1, src_seq_len]\n",
    "      - dec_mask: [batch_size, 1, tgt_seq_len, tgt_seq_len]\n",
    "    \"\"\"\n",
    "    # 1) 인코더 입력용 패딩 마스크\n",
    "    enc_mask = generate_padding_mask(src)\n",
    "    # 2) 디코더에서 인코더 값을 볼 때 사용하는 마스크 (src 마스크 재사용)\n",
    "    dec_enc_mask = generate_padding_mask(src)\n",
    "\n",
    "    # 3) 디코더 자기어텐션 마스크 (미래 토큰 방지 룩어헤드 + tgt 자체 패딩 마스크)\n",
    "    dec_lookahead_mask = generate_lookahead_mask(tgt.shape[1])  # [tgt_seq_len, tgt_seq_len]\n",
    "    dec_tgt_padding_mask = generate_padding_mask(tgt)           # [batch_size, 1, 1, tgt_seq_len]\n",
    "\n",
    "    # 룩어헤드 마스크를 (batch 차원과 head 차원을 가상으로) 확장\n",
    "    dec_lookahead_mask = dec_lookahead_mask.unsqueeze(0).unsqueeze(1)  # [1, 1, seq_len, seq_len]\n",
    "\n",
    "    # 패딩 + 룩어헤드 마스크 병합\n",
    "    # 브로드캐스팅에 의해 shape [batch_size, 1, tgt_seq_len, tgt_seq_len]이 됨\n",
    "\n",
    "    dec_tgt_padding_mask = dec_tgt_padding_mask.to(device)\n",
    "    dec_lookahead_mask = dec_lookahead_mask.to(device)\n",
    "\n",
    "    dec_mask = torch.max(dec_tgt_padding_mask, dec_lookahead_mask)\n",
    "\n",
    "    return enc_mask, dec_enc_mask, dec_mask\n",
    "\n",
    "print(\"슝=3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "56adb089-5e95-4bc8-8dd8-e20fd592759d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "슝=3\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.d_model = d_model\n",
    "\n",
    "        # d_model을 num_heads로 나눈 만큼이 각 head가 담당할 차원 수\n",
    "        self.depth = d_model // num_heads\n",
    "\n",
    "        # Query, Key, Value를 구하는 선형 레이어\n",
    "        self.W_q = nn.Linear(d_model, d_model)\n",
    "        self.W_k = nn.Linear(d_model, d_model)\n",
    "        self.W_v = nn.Linear(d_model, d_model)\n",
    "\n",
    "        # 최종적으로 head들의 출력을 결합해주는 선형 레이어\n",
    "        self.linear = nn.Linear(d_model, d_model)\n",
    "\n",
    "    def scaled_dot_product_attention(self, Q, K, V, mask=None):\n",
    "        \"\"\"\n",
    "        Q, K, V:  [batch_size, num_heads, seq_len, depth]\n",
    "        mask:     [batch_size, 1, seq_len, seq_len] 혹은\n",
    "                  [batch_size, num_heads, seq_len, seq_len]\n",
    "                  (어텐션에서 제외할 위치=1, 사용할 위치=0)\n",
    "        \"\"\"\n",
    "        # d_k = depth\n",
    "        d_k = Q.size(-1)  # K.shape[-1]도 동일\n",
    "        # Q와 K의 전치 곱: (batch_size, num_heads, seq_len, seq_len)\n",
    "        QK = torch.matmul(Q, K.transpose(-1, -2))\n",
    "\n",
    "        # 스케일링\n",
    "        scaled_qk = QK / torch.sqrt(torch.tensor(d_k, dtype=torch.float32))\n",
    "\n",
    "        # 마스크가 있는 경우 -1e9(매우 작은 수)를 더하여 softmax 후 확률이 0에 가깝도록 처리\n",
    "        if mask is not None:\n",
    "            scaled_qk = scaled_qk + (mask * -1e9)\n",
    "\n",
    "        attentions = F.softmax(scaled_qk, dim=-1)  # (batch_size, num_heads, seq_len, seq_len)\n",
    "        out = torch.matmul(attentions, V)         # (batch_size, num_heads, seq_len, depth)\n",
    "\n",
    "        return out, attentions\n",
    "\n",
    "    def split_heads(self, x):\n",
    "        \"\"\"\n",
    "        x: [batch_size, seq_len, d_model]\n",
    "        반환: [batch_size, num_heads, seq_len, depth]\n",
    "        \"\"\"\n",
    "        bsz, seq_len, _ = x.size()\n",
    "        # d_model -> (num_heads * depth)이므로 view로 재배치\n",
    "        x = x.view(bsz, seq_len, self.num_heads, self.depth)\n",
    "        # (batch_size, seq_len, num_heads, depth) -> (batch_size, num_heads, seq_len, depth)\n",
    "        x = x.permute(0, 2, 1, 3)\n",
    "        return x\n",
    "\n",
    "    def combine_heads(self, x):\n",
    "        \"\"\"\n",
    "        x: [batch_size, num_heads, seq_len, depth]\n",
    "        반환: [batch_size, seq_len, d_model]\n",
    "        \"\"\"\n",
    "        bsz, num_heads, seq_len, depth = x.size()\n",
    "        # (batch_size, num_heads, seq_len, depth) -> (batch_size, seq_len, num_heads, depth)\n",
    "        x = x.permute(0, 2, 1, 3).contiguous()\n",
    "        x = x.view(bsz, seq_len, self.d_model)\n",
    "        return x\n",
    "\n",
    "    def forward(self, Q, K, V, mask=None):\n",
    "        \"\"\"\n",
    "        Q, K, V: [batch_size, seq_len, d_model]\n",
    "        mask:    [batch_size, 1, seq_len, seq_len] 혹은\n",
    "                 [batch_size, num_heads, seq_len, seq_len]\n",
    "        \"\"\"\n",
    "        # W_q, W_k, W_v는 각각 (d_model -> d_model) 선형 변환\n",
    "        WQ = self.W_q(Q)  # [batch_size, seq_len, d_model]\n",
    "        WK = self.W_k(K)  # [batch_size, seq_len, d_model]\n",
    "        WV = self.W_v(V)  # [batch_size, seq_len, d_model]\n",
    "\n",
    "        # 멀티헤드 분할\n",
    "        WQ_splits = self.split_heads(WQ)  # [batch_size, num_heads, seq_len, depth]\n",
    "        WK_splits = self.split_heads(WK)\n",
    "        WV_splits = self.split_heads(WV)\n",
    "\n",
    "        # Scaled dot-product attention\n",
    "        out, attention_weights = self.scaled_dot_product_attention(\n",
    "            WQ_splits, WK_splits, WV_splits, mask\n",
    "        )\n",
    "\n",
    "        # head 결과 결합 후 최종 선형\n",
    "        out = self.combine_heads(out)  # [batch_size, seq_len, d_model]\n",
    "        out = self.linear(out)         # [batch_size, seq_len, d_model]\n",
    "\n",
    "        return out, attention_weights\n",
    "\n",
    "print(\"슝=3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3cb966c7-1675-46be-a395-fe5c1403fc53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "슝=3\n"
     ]
    }
   ],
   "source": [
    "class PoswiseFeedForwardNet(nn.Module):\n",
    "    def __init__(self, d_model, d_ff):\n",
    "        super(PoswiseFeedForwardNet, self).__init__()\n",
    "        self.d_model = d_model\n",
    "        self.d_ff = d_ff\n",
    "\n",
    "        self.fc1 = nn.Linear(d_model, d_ff)\n",
    "        self.fc2 = nn.Linear(d_ff, d_model)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.relu(self.fc1(x))  # 첫 번째 Dense + ReLU\n",
    "        out = self.fc2(out)          # 두 번째 Dense\n",
    "        return out\n",
    "\n",
    "print(\"슝=3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e2cdac0f-f814-4d98-83b3-2a1f42184cb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "슝=3\n"
     ]
    }
   ],
   "source": [
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, n_heads, d_ff, dropout):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        self.enc_self_attn = MultiHeadAttention(d_model, n_heads)\n",
    "        self.ffn = PoswiseFeedForwardNet(d_model, d_ff)\n",
    "\n",
    "        # nn.LayerNorm은 마지막 차원(d_model)을 기준으로 정규화\n",
    "        self.norm_1 = nn.LayerNorm(d_model, eps=1e-6)\n",
    "        self.norm_2 = nn.LayerNorm(d_model, eps=1e-6)\n",
    "\n",
    "        self.do = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        # Multi-Head Attention 단계\n",
    "        residual = x\n",
    "        out = self.norm_1(x)\n",
    "        out, enc_attn = self.enc_self_attn(out, out, out, mask)\n",
    "        out = self.do(out)\n",
    "        out = out + residual  # residual connection\n",
    "\n",
    "        # Position-Wise Feed Forward 단계\n",
    "        residual = out\n",
    "        out = self.norm_2(out)\n",
    "        out = self.ffn(out)\n",
    "        out = self.do(out)\n",
    "        out = out + residual  # residual connection\n",
    "\n",
    "        return out, enc_attn\n",
    "\n",
    "print(\"슝=3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4f11ffe1-47ea-4c25-8614-359c080d8e66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "슝=3\n"
     ]
    }
   ],
   "source": [
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, d_ff, dropout):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "        self.dec_self_attn = MultiHeadAttention(d_model, num_heads)\n",
    "        self.enc_dec_attn = MultiHeadAttention(d_model, num_heads)\n",
    "        self.ffn = PoswiseFeedForwardNet(d_model, d_ff)\n",
    "\n",
    "        self.norm_1 = nn.LayerNorm(d_model, eps=1e-6)\n",
    "        self.norm_2 = nn.LayerNorm(d_model, eps=1e-6)\n",
    "        self.norm_3 = nn.LayerNorm(d_model, eps=1e-6)\n",
    "\n",
    "        self.do = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, enc_out, dec_enc_mask, padding_mask):\n",
    "        # Masked Multi-Head Attention\n",
    "        residual = x\n",
    "        out = self.norm_1(x)\n",
    "        out, dec_attn = self.dec_self_attn(out, out, out, mask=padding_mask)\n",
    "        out = self.do(out)\n",
    "        out = out + residual\n",
    "\n",
    "        # Encoder-Decoder Multi-Head Attention (주의: Q, K, V 순서)\n",
    "        residual = out\n",
    "        out = self.norm_2(out)\n",
    "        out, dec_enc_attn = self.enc_dec_attn(out, enc_out, enc_out, mask=dec_enc_mask)\n",
    "        out = self.do(out)\n",
    "        out = out + residual\n",
    "\n",
    "        # Position-Wise Feed Forward Network\n",
    "        residual = out\n",
    "        out = self.norm_3(out)\n",
    "        out = self.ffn(out)\n",
    "        out = self.do(out)\n",
    "        out = out + residual\n",
    "\n",
    "        return out, dec_attn, dec_enc_attn\n",
    "\n",
    "print(\"슝=3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7b030523-4ed5-49b7-a134-7f229b54b795",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "슝=3\n"
     ]
    }
   ],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, n_layers, d_model, n_heads, d_ff, dropout):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.n_layers = n_layers\n",
    "        self.enc_layers = nn.ModuleList(\n",
    "            [EncoderLayer(d_model, n_heads, d_ff, dropout) for _ in range(n_layers)]\n",
    "        )\n",
    "        self.do = nn.Dropout(dropout)  # 필요 시 입력에 dropout 적용 가능\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        out = x\n",
    "        enc_attns = []\n",
    "        for i in range(self.n_layers):\n",
    "            out, enc_attn = self.enc_layers[i](out, mask)\n",
    "            enc_attns.append(enc_attn)\n",
    "        return out, enc_attns\n",
    "\n",
    "# 사용 예시: Encoder 인스턴스 생성 후 forward 호출\n",
    "# encoder = Encoder(n_layers, d_model, n_heads, d_ff, dropout)\n",
    "# out, enc_attns = encoder(x, mask)\n",
    "print(\"슝=3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3f273573-a346-4e22-9983-500e770e54e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "슝=3\n"
     ]
    }
   ],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, n_layers, d_model, n_heads, d_ff, dropout):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.n_layers = n_layers\n",
    "        self.dec_layers = nn.ModuleList(\n",
    "            [DecoderLayer(d_model, n_heads, d_ff, dropout) for _ in range(n_layers)]\n",
    "        )\n",
    "\n",
    "    def forward(self, x, enc_out, dec_enc_mask, padding_mask):\n",
    "        out = x\n",
    "        dec_attns = []\n",
    "        dec_enc_attns = []\n",
    "        for i in range(self.n_layers):\n",
    "            out, dec_attn, dec_enc_attn = self.dec_layers[i](out, enc_out, dec_enc_mask, padding_mask)\n",
    "            dec_attns.append(dec_attn)\n",
    "            dec_enc_attns.append(dec_enc_attn)\n",
    "        return out, dec_attns, dec_enc_attns\n",
    "\n",
    "print(\"슝=3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "bf1260c0-cf4e-4b25-90f5-d6b3087c9dd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "슝=3\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self, n_layers, d_model, n_heads, d_ff,\n",
    "                 src_vocab_size, tgt_vocab_size, pos_len,\n",
    "                 dropout=0.2, shared_fc=True, shared_emb=False):\n",
    "        super(Transformer, self).__init__()\n",
    "        # d_model은 스케일링에 사용되므로 float으로 저장\n",
    "        self.d_model = float(d_model)\n",
    "\n",
    "        # Embedding 레이어: shared_emb True면 동일한 임베딩을 사용합니다.\n",
    "        if shared_emb:\n",
    "            self.enc_emb = self.dec_emb = nn.Embedding(src_vocab_size, d_model)\n",
    "        else:\n",
    "            self.enc_emb = nn.Embedding(src_vocab_size, d_model)\n",
    "            self.dec_emb = nn.Embedding(tgt_vocab_size, d_model)\n",
    "\n",
    "        # Positional encoding (넘파이 버전 결과를 torch.Tensor로 변환)\n",
    "        pos_encoding_np = positional_encoding(pos_len, d_model)\n",
    "        # 파라미터로 등록하지 않고 고정값이므로 buffer로 등록합니다.\n",
    "        self.register_buffer(\"pos_encoding\", torch.tensor(pos_encoding_np, dtype=torch.float32))\n",
    "\n",
    "        self.do = nn.Dropout(dropout)\n",
    "\n",
    "        self.encoder = Encoder(n_layers, d_model, n_heads, d_ff, dropout)\n",
    "        self.decoder = Decoder(n_layers, d_model, n_heads, d_ff, dropout)\n",
    "\n",
    "        self.fc = nn.Linear(d_model, tgt_vocab_size)\n",
    "\n",
    "        self.shared_fc = shared_fc\n",
    "        if shared_fc:\n",
    "            # fc 레이어와 디코더 임베딩의 weight를 공유합니다.\n",
    "            self.fc.weight = self.dec_emb.weight\n",
    "\n",
    "    def embedding(self, emb, x):\n",
    "        \"\"\"\n",
    "        emb: 임베딩 레이어\n",
    "        x: [batch_size, seq_len] (토큰 인덱스)\n",
    "        \"\"\"\n",
    "        seq_len = x.size(1)\n",
    "        out = emb(x)  # [batch_size, seq_len, d_model]\n",
    "        if self.shared_fc:\n",
    "            out = out * math.sqrt(self.d_model)\n",
    "        # pos_encoding: [pos_len, d_model] → [1, pos_len, d_model] 후 슬라이싱\n",
    "        out = out + self.pos_encoding[:seq_len, :].unsqueeze(0)\n",
    "        out = self.do(out)\n",
    "        return out\n",
    "\n",
    "    def forward(self, enc_in, dec_in, enc_mask, dec_enc_mask, dec_mask):\n",
    "        \"\"\"\n",
    "        enc_in: [batch_size, src_seq_len]\n",
    "        dec_in: [batch_size, tgt_seq_len]\n",
    "        enc_mask, dec_enc_mask, dec_mask: 마스킹 텐서들\n",
    "        \"\"\"\n",
    "        # Embedding 및 positional encoding 적용\n",
    "        enc_in_emb = self.embedding(self.enc_emb, enc_in)\n",
    "        dec_in_emb = self.embedding(self.dec_emb, dec_in)\n",
    "\n",
    "        # Encoder와 Decoder 통과\n",
    "        enc_out, enc_attns = self.encoder(enc_in_emb, enc_mask)\n",
    "        dec_out, dec_attns, dec_enc_attns = self.decoder(dec_in_emb, enc_out, dec_enc_mask, dec_mask)\n",
    "\n",
    "        logits = self.fc(dec_out)\n",
    "        return logits, enc_attns, dec_attns, dec_enc_attns\n",
    "\n",
    "print(\"슝=3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "82d4dae9-c0ba-4acc-a46b-3cf5a8691112",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "1807a585-da2d-47ea-a398-8313f9cb4571",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "슝=3\n"
     ]
    }
   ],
   "source": [
    "# 주어진 하이퍼파라미터로 Transformer 인스턴스 생성\n",
    "transformer = Transformer(\n",
    "    n_layers=2,\n",
    "    d_model=512,\n",
    "    n_heads=8,\n",
    "    d_ff=2048,\n",
    "    src_vocab_size=VOCAB_SIZE,\n",
    "    tgt_vocab_size=VOCAB_SIZE,\n",
    "    pos_len=200,\n",
    "    dropout=0.3,\n",
    "    shared_fc=True,\n",
    "    shared_emb=True)\n",
    "\n",
    "transformer = transformer.to(device)\n",
    "\n",
    "d_model = 512\n",
    "\n",
    "print(\"슝=3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "2738b047-8438-45e6-98b6-a34ab408b7bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "슝=3\n"
     ]
    }
   ],
   "source": [
    "class LearningRateScheduler:\n",
    "    def __init__(self, d_model, warmup_steps=60): # 4000\n",
    "        self.d_model = d_model\n",
    "        self.warmup_steps = warmup_steps\n",
    "\n",
    "    def __call__(self, step):\n",
    "        # step을 float으로 변환하여 지수 연산이 제대로 수행되도록 함\n",
    "        step = float(step)\n",
    "        arg1 = step ** -0.5\n",
    "        arg2 = step * (self.warmup_steps ** -1.5)\n",
    "        return (self.d_model ** -0.5) * min(arg1, arg2)\n",
    "\n",
    "print(\"슝=3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e41bddc9-bbe8-44c6-9933-8deaa1afbd6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "슝=3\n"
     ]
    }
   ],
   "source": [
    "# Learning Rate 인스턴스 선언\n",
    "learning_rate = LearningRateScheduler(d_model)\n",
    "\n",
    "# 초기 lr은 스텝 1에 해당하는 값으로 설정합니다.\n",
    "optimizer = torch.optim.Adam(transformer.parameters(),\n",
    "                             lr=learning_rate(1),\n",
    "                             betas=(0.9, 0.98),\n",
    "                             eps=1e-9)\n",
    "\n",
    "print(\"슝=3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "3a54a285-dea2-48f0-bcc3-e89d729f1bc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "슝=3\n"
     ]
    }
   ],
   "source": [
    "def loss_function(real, pred):\n",
    "    \"\"\"\n",
    "    real: [batch_size, seq_len] (정답 토큰 인덱스)\n",
    "    pred: [batch_size, seq_len, num_classes] (모델의 raw logits)\n",
    "    \"\"\"\n",
    "\n",
    "    real = real.to(device)\n",
    "    pred = pred.to(device)\n",
    "\n",
    "    # 예측 값을 (N, C) 형태로 flatten하고, 정답도 flatten하여 개별 손실 값을 구함\n",
    "    loss_ = F.cross_entropy(pred.contiguous().view(-1, pred.size(-1)), real.contiguous().view(-1), reduction='none')\n",
    "    # 다시 (batch_size, seq_len)로 reshape\n",
    "    loss_ = loss_.view(real.size())\n",
    "\n",
    "    # real이 0이 아닌 위치에 대한 마스크 생성 (0이면 패딩 토큰)\n",
    "    mask = (real != 0).float()\n",
    "    loss_ = loss_ * mask\n",
    "\n",
    "    # 전체 손실 합을 마스크 합으로 나누어 평균 손실 계산\n",
    "    return loss_.sum() / mask.sum()\n",
    "\n",
    "print(\"슝=3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "9759ecce-fbeb-4f05-8c96-df20c3253970",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "슝=3\n"
     ]
    }
   ],
   "source": [
    "def train_step(src, tgt, model, optimizer):\n",
    "    model.train()  # 모델을 training 모드로 전환\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # tgt의 오른쪽 시프트: decoder input과 gold target 분리\n",
    "    tgt_in = tgt[:, :-1]  # Decoder의 입력\n",
    "    gold = tgt[:, 1:]     # Decoder의 정답(target)\n",
    "\n",
    "    # 마스크 생성 (generate_masks는 PyTorch용으로 변환된 함수여야 합니다)\n",
    "    enc_mask, dec_enc_mask, dec_mask = generate_masks(src, tgt_in)\n",
    "\n",
    "    src = src.to(device)\n",
    "    tgt_in = tgt_in.to(device)\n",
    "    enc_mask = enc_mask.to(device)\n",
    "    dec_enc_mask = dec_enc_mask.to(device)\n",
    "    dec_mask = dec_mask.to(device)\n",
    "\n",
    "    # 모델 forward pass\n",
    "    predictions, enc_attns, dec_attns, dec_enc_attns = model(src, tgt_in, enc_mask, dec_enc_mask, dec_mask)\n",
    "\n",
    "    # loss 계산\n",
    "    loss = loss_function(gold, predictions)\n",
    "\n",
    "    # 역전파 수행 및 파라미터 업데이트\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    return loss, enc_attns, dec_attns, dec_enc_attns\n",
    "\n",
    "print(\"슝=3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "cfbfbd8a-ce37-4f12-92ab-c973cb5b9b97",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 148/148 [00:19<00:00,  7.47it/s, Batch Loss=6258.9404]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 7705.0352\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 148/148 [00:17<00:00,  8.66it/s, Batch Loss=5058.3076]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2, Loss: 5343.2152\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 148/148 [00:16<00:00,  9.02it/s, Batch Loss=4518.4399]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3, Loss: 4749.6545\n",
      "CPU times: user 59.5 s, sys: 408 ms, total: 60 s\n",
      "Wall time: 53.3 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "EPOCHS = 3\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    total_loss = 0.0\n",
    "    dataset_count = len(train_dataloader)  # train_loader는 PyTorch DataLoader입니다.\n",
    "    tqdm_bar = tqdm(total=dataset_count)\n",
    "\n",
    "    for batch, (src, tgt) in enumerate(train_dataloader):\n",
    "        # train_step 함수는 (loss, enc_attns, dec_attns, dec_enc_attns)를 반환합니다.\n",
    "        loss, enc_attns, dec_attns, dec_enc_attns = train_step(src, tgt, transformer, optimizer)\n",
    "\n",
    "        total_loss += loss.item()  # PyTorch에서는 loss.numpy() 대신 loss.item() 사용\n",
    "        tqdm_bar.set_postfix({\"Batch Loss\": f\"{loss.item():.4f}\"})\n",
    "        tqdm_bar.update(1)\n",
    "\n",
    "    tqdm_bar.close()\n",
    "    print(f\"Epoch {epoch+1}, Loss: {total_loss / dataset_count:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "8c0908db-2fa3-49ab-89c8-ec7015b23634",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "워드벡터 로드 성공: 30,186개 단어\n",
      "원본 데이터: 11823개\n",
      "증강 완료: 35469개\n",
      "증강 전: 11823개\n",
      "증강 후: 35469개\n",
      "증강 비율: 3.0배\n",
      "슝=3\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "from gensim.models import KeyedVectors\n",
    "import re\n",
    "\n",
    "def preprocess_sentence(sentence):\n",
    "    sentence = sentence.lower()\n",
    "    sentence = re.sub(r'[^a-z가-힣0-9.,?!;:\\-()\"\\'\\s]', '', sentence)\n",
    "    sentence = re.sub(r'\\s+', ' ', sentence)\n",
    "    return sentence.strip()\n",
    "\n",
    "def enhanced_lexical_sub(sentence, wv, num_substitutions=1):\n",
    "    tokens = sentence.split()\n",
    "    valid_tokens = [tok for tok in tokens if tok in wv]\n",
    "    \n",
    "    if not valid_tokens:\n",
    "        return sentence\n",
    "    \n",
    "    max_subs = min(num_substitutions, len(valid_tokens), max(1, len(tokens) // 3))\n",
    "    selected_tokens = random.sample(valid_tokens, max_subs)\n",
    "    \n",
    "    new_tokens = tokens.copy()\n",
    "    for selected_tok in selected_tokens:\n",
    "        try:\n",
    "            similar_words = wv.most_similar(selected_tok, topk=3)\n",
    "            similar_word = random.choice(similar_words)[0]\n",
    "            new_tokens = [similar_word if tok == selected_tok else tok for tok in new_tokens]\n",
    "        except:\n",
    "            continue\n",
    "    \n",
    "    return \" \".join(new_tokens)\n",
    "\n",
    "def augmented_corpus(questions, answers, tokenizer, wv_model, max_length=40):\n",
    "    print(f\"원본 데이터: {len(questions)}개\")\n",
    "    \n",
    "    final_questions = []\n",
    "    final_answers = []\n",
    "    \n",
    "    for q, a in zip(questions, answers):\n",
    "        clean_q = preprocess_sentence(q)\n",
    "        clean_a = preprocess_sentence(a)\n",
    "        \n",
    "        try:\n",
    "            # 원본\n",
    "            final_questions.append(clean_q)\n",
    "            final_answers.append(clean_a)\n",
    "            \n",
    "            # 증강 질문 + 원본 답변\n",
    "            aug_question = enhanced_lexical_sub(clean_q, wv_model)\n",
    "            final_questions.append(aug_question)\n",
    "            final_answers.append(clean_a)\n",
    "            \n",
    "            # 원본 질문 + 증강 답변\n",
    "            aug_answer = enhanced_lexical_sub(clean_a, wv_model)\n",
    "            final_questions.append(clean_q)\n",
    "            final_answers.append(aug_answer)\n",
    "                \n",
    "        except:\n",
    "            final_questions.extend([clean_q] * 3)\n",
    "            final_answers.extend([clean_a] * 3)\n",
    "    \n",
    "    # 토큰화\n",
    "    que_corpus = []\n",
    "    ans_corpus = []\n",
    "    \n",
    "    for q, a in zip(final_questions, final_answers):\n",
    "        q_tokens = tokenizer.encode_as_ids(q)\n",
    "        a_tokens = tokenizer.encode_as_ids(a)\n",
    "        \n",
    "        if len(q_tokens) <= max_length and len(a_tokens) <= max_length:\n",
    "            que_corpus.append(q_tokens)\n",
    "            ans_corpus.append(a_tokens)\n",
    "    \n",
    "    print(f\"증강 완료: {len(que_corpus)}개\")\n",
    "    return que_corpus, ans_corpus\n",
    "\n",
    "def load_korean_word_vectors(model_path='ko.vec'):\n",
    "    try:\n",
    "        wv = KeyedVectors.load_word2vec_format(model_path, binary=False, limit=100000)\n",
    "        print(f\"워드벡터 로드 성공: {len(wv.key_to_index):,}개 단어\")\n",
    "        return wv\n",
    "    except:\n",
    "        print(\"워드벡터 로드 실패\")\n",
    "        return None\n",
    "\n",
    "# 실행\n",
    "wv_model = load_korean_word_vectors('ko.vec')\n",
    "if wv_model:\n",
    "    que_corpus, ans_corpus = augmented_corpus(questions, answers, tokenizer, wv_model)\n",
    "    \n",
    "    # 결과 확인\n",
    "    print(f\"증강 전: {len(questions)}개\")\n",
    "    print(f\"증강 후: {len(que_corpus)}개\")\n",
    "    print(f\"증강 비율: {len(que_corpus)/len(questions):.1f}배\")\n",
    "    print(\"슝=3\")\n",
    "else:\n",
    "    print(\"워드벡터 모델이 없어 증강을 건너뜁니다\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "19fdebef-5fb5-4246-9b8b-44294e90215d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "START 토큰 ID: 1\n",
      "END 토큰 ID: 2\n",
      "토큰 추가 완료: 35469개 시퀀스\n",
      "슝=3\n"
     ]
    }
   ],
   "source": [
    "# 특수 토큰 ID 확인\n",
    "START_IDX = tokenizer.piece_to_id('<s>')  # 또는 '<sos>'\n",
    "END_IDX = tokenizer.piece_to_id('</s>')   # 또는 '<eos>'\n",
    "\n",
    "print(f\"START 토큰 ID: {START_IDX}\")\n",
    "print(f\"END 토큰 ID: {END_IDX}\")\n",
    "\n",
    "# ans_corpus에 시작/끝 토큰 추가\n",
    "ans_corpus_with_tokens = []\n",
    "for ans_seq in ans_corpus:\n",
    "    # [START] + 원본 시퀀스 + [END]\n",
    "    new_seq = [START_IDX] + ans_seq + [END_IDX]\n",
    "    ans_corpus_with_tokens.append(new_seq)\n",
    "\n",
    "ans_corpus = ans_corpus_with_tokens\n",
    "print(f\"토큰 추가 완료: {len(ans_corpus)}개 시퀀스\")\n",
    "print(\"슝=3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "da12483c-30a8-4bc7-971f-7d9b272b5e44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "전체 어휘 사전 크기: 7857\n",
      "enc_train: 35469개 시퀀스\n",
      "dec_train: 35469개 시퀀스\n",
      "슝=3\n"
     ]
    }
   ],
   "source": [
    "# 전체 데이터에서 고유 토큰 추출하여 단어 사전 구축\n",
    "all_tokens = set()\n",
    "\n",
    "# que_corpus와 ans_corpus의 모든 토큰 수집\n",
    "for seq in que_corpus:\n",
    "   all_tokens.update(seq)\n",
    "for seq in ans_corpus:\n",
    "   all_tokens.update(seq)\n",
    "\n",
    "# 단어 사전 구축 (토큰 ID -> 인덱스 매핑)\n",
    "vocab_size = len(all_tokens)\n",
    "token_to_idx = {token: idx for idx, token in enumerate(sorted(all_tokens))}\n",
    "idx_to_token = {idx: token for token, idx in token_to_idx.items()}\n",
    "\n",
    "print(f\"전체 어휘 사전 크기: {vocab_size}\")\n",
    "\n",
    "# 토큰 ID를 인덱스로 변환하여 벡터화\n",
    "def vectorize_sequences(sequences, token_to_idx):\n",
    "   vectorized = []\n",
    "   for seq in sequences:\n",
    "       vec_seq = [token_to_idx[token] for token in seq if token in token_to_idx]\n",
    "       vectorized.append(vec_seq)\n",
    "   return vectorized\n",
    "\n",
    "# 벡터화 실행\n",
    "enc_train = vectorize_sequences(que_corpus, token_to_idx)\n",
    "dec_train = vectorize_sequences(ans_corpus, token_to_idx)\n",
    "\n",
    "print(f\"enc_train: {len(enc_train)}개 시퀀스\")\n",
    "print(f\"dec_train: {len(dec_train)}개 시퀀스\")\n",
    "print(\"슝=3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "81234e27-270e-481f-8fc4-6c6cdaedef72",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 148/148 [00:17<00:00,  8.44it/s, Batch Loss=4096.9268]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 3976.8963\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 148/148 [00:17<00:00,  8.65it/s, Batch Loss=3866.5576]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2, Loss: 3853.3530\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 148/148 [00:16<00:00,  8.98it/s, Batch Loss=3674.2617]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3, Loss: 3736.9872\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 148/148 [00:16<00:00,  9.06it/s, Batch Loss=3672.5645]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4, Loss: 3635.8926\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 148/148 [00:16<00:00,  8.96it/s, Batch Loss=3471.8479]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5, Loss: 3535.6263\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 148/148 [00:16<00:00,  8.83it/s, Batch Loss=3488.7761]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6, Loss: 3452.6307\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 148/148 [00:16<00:00,  8.81it/s, Batch Loss=3346.7676]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7, Loss: 3368.9851\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 148/148 [00:16<00:00,  8.86it/s, Batch Loss=2842.2012]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8, Loss: 3288.9311\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 148/148 [00:16<00:00,  8.91it/s, Batch Loss=2937.2292]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9, Loss: 3219.3553\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 148/148 [00:16<00:00,  8.93it/s, Batch Loss=3208.9800]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10, Loss: 3149.8557\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 148/148 [00:16<00:00,  8.92it/s, Batch Loss=3181.5254]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11, Loss: 3087.9570\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 26%|██▋       | 39/148 [00:04<00:12,  8.83it/s, Batch Loss=3049.3149]"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[43]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mget_ipython\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_cell_magic\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mtime\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[33;43mEPOCHS = 15\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[33;43mfor epoch in range(EPOCHS):\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[33;43m    total_loss = 0.0\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[33;43m    dataset_count = len(train_dataloader)  # train_loader는 PyTorch DataLoader입니다.\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[33;43m    tqdm_bar = tqdm(total=dataset_count)\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[33;43m    for batch, (src, tgt) in enumerate(train_dataloader):\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[33;43m        # train_step 함수는 (loss, enc_attns, dec_attns, dec_enc_attns)를 반환합니다.\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[33;43m        loss, enc_attns, dec_attns, dec_enc_attns = train_step(src, tgt, transformer, optimizer)\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[33;43m        total_loss += loss.item()  # PyTorch에서는 loss.numpy() 대신 loss.item() 사용\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[33;43m        tqdm_bar.set_postfix(\u001b[39;49m\u001b[33;43m{\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mBatch Loss\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m: f\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m{\u001b[39;49m\u001b[33;43mloss.item():.4f}\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m})\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[33;43m        tqdm_bar.update(1)\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[33;43m    tqdm_bar.close()\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[33;43m    print(f\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mEpoch \u001b[39;49m\u001b[33;43m{\u001b[39;49m\u001b[33;43mepoch+1}, Loss: \u001b[39;49m\u001b[33;43m{\u001b[39;49m\u001b[33;43mtotal_loss / dataset_count:.4f}\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m)\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/lib/python3.12/site-packages/IPython/core/interactiveshell.py:2549\u001b[39m, in \u001b[36mInteractiveShell.run_cell_magic\u001b[39m\u001b[34m(self, magic_name, line, cell)\u001b[39m\n\u001b[32m   2547\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m.builtin_trap:\n\u001b[32m   2548\u001b[39m     args = (magic_arg_s, cell)\n\u001b[32m-> \u001b[39m\u001b[32m2549\u001b[39m     result = \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2551\u001b[39m \u001b[38;5;66;03m# The code below prevents the output from being displayed\u001b[39;00m\n\u001b[32m   2552\u001b[39m \u001b[38;5;66;03m# when using magics with decorator @output_can_be_silenced\u001b[39;00m\n\u001b[32m   2553\u001b[39m \u001b[38;5;66;03m# when the last Python token in the expression is a ';'.\u001b[39;00m\n\u001b[32m   2554\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(fn, magic.MAGIC_OUTPUT_CAN_BE_SILENCED, \u001b[38;5;28;01mFalse\u001b[39;00m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/lib/python3.12/site-packages/IPython/core/magics/execution.py:1390\u001b[39m, in \u001b[36mExecutionMagics.time\u001b[39m\u001b[34m(self, line, cell, local_ns)\u001b[39m\n\u001b[32m   1388\u001b[39m st = clock2()\n\u001b[32m   1389\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1390\u001b[39m     \u001b[43mexec\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mglob\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlocal_ns\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1391\u001b[39m     out = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1392\u001b[39m     \u001b[38;5;66;03m# multi-line %%time case\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<timed exec>:10\u001b[39m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[36]\u001b[39m\u001b[32m, line 25\u001b[39m, in \u001b[36mtrain_step\u001b[39m\u001b[34m(src, tgt, model, optimizer)\u001b[39m\n\u001b[32m     22\u001b[39m loss = loss_function(gold, predictions)\n\u001b[32m     24\u001b[39m \u001b[38;5;66;03m# 역전파 수행 및 파라미터 업데이트\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m25\u001b[39m \u001b[43mloss\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     26\u001b[39m optimizer.step()\n\u001b[32m     28\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m loss, enc_attns, dec_attns, dec_enc_attns\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/lib/python3.12/site-packages/torch/_tensor.py:648\u001b[39m, in \u001b[36mTensor.backward\u001b[39m\u001b[34m(self, gradient, retain_graph, create_graph, inputs)\u001b[39m\n\u001b[32m    638\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    639\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[32m    640\u001b[39m         Tensor.backward,\n\u001b[32m    641\u001b[39m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[32m   (...)\u001b[39m\u001b[32m    646\u001b[39m         inputs=inputs,\n\u001b[32m    647\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m648\u001b[39m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mautograd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    649\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs\u001b[49m\n\u001b[32m    650\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/lib/python3.12/site-packages/torch/autograd/__init__.py:353\u001b[39m, in \u001b[36mbackward\u001b[39m\u001b[34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[39m\n\u001b[32m    348\u001b[39m     retain_graph = create_graph\n\u001b[32m    350\u001b[39m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[32m    351\u001b[39m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[32m    352\u001b[39m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m353\u001b[39m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    354\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    355\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    356\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    357\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    358\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    359\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    360\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    361\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/lib/python3.12/site-packages/torch/autograd/graph.py:824\u001b[39m, in \u001b[36m_engine_run_backward\u001b[39m\u001b[34m(t_outputs, *args, **kwargs)\u001b[39m\n\u001b[32m    822\u001b[39m     unregister_hooks = _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[32m    823\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m824\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_execution_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[32m    825\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    826\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[32m    827\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    828\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "EPOCHS = 15\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    total_loss = 0.0\n",
    "    dataset_count = len(train_dataloader)  # train_loader는 PyTorch DataLoader입니다.\n",
    "    tqdm_bar = tqdm(total=dataset_count)\n",
    "\n",
    "    for batch, (src, tgt) in enumerate(train_dataloader):\n",
    "        # train_step 함수는 (loss, enc_attns, dec_attns, dec_enc_attns)를 반환합니다.\n",
    "        loss, enc_attns, dec_attns, dec_enc_attns = train_step(src, tgt, transformer, optimizer)\n",
    "\n",
    "        total_loss += loss.item()  # PyTorch에서는 loss.numpy() 대신 loss.item() 사용\n",
    "        tqdm_bar.set_postfix({\"Batch Loss\": f\"{loss.item():.4f}\"})\n",
    "        tqdm_bar.update(1)\n",
    "\n",
    "    tqdm_bar.close()\n",
    "    print(f\"Epoch {epoch+1}, Loss: {total_loss / dataset_count:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "61a741ba-d7ef-4c9e-a611-b9c40fa2e82f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8에포크 모델 저장 완료!\n"
     ]
    }
   ],
   "source": [
    "# 현재 모델 상태 저장\n",
    "torch.save({\n",
    "    'epoch': 8,\n",
    "    'model_state_dict': transformer.state_dict(),\n",
    "    'optimizer_state_dict': optimizer.state_dict(),\n",
    "    'train_losses': train_losses if 'train_losses' in globals() else [],\n",
    "}, 'transformer_epoch8_backup.pt')\n",
    "\n",
    "print(\"8에포크 모델 저장 완료!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "1750d945-2a0e-4a8d-aa93-e683d673b4ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "새 모델 생성 완료!\n",
      "파라미터 수: 3,899,200\n"
     ]
    }
   ],
   "source": [
    "# 하이퍼파라미터 수정된 새 모델\n",
    "transformer_new = Transformer(\n",
    "    n_layers=1,           # 2 → 1로 감소\n",
    "    d_model=256,          # 512 → 256으로 감소\n",
    "    n_heads=4,            # 8 → 4로 감소\n",
    "    d_ff=1024,            # 2048 → 1024로 감소\n",
    "    src_vocab_size=VOCAB_SIZE,  # 빠진 인자\n",
    "    tgt_vocab_size=VOCAB_SIZE,  # 빠진 인자\n",
    "    pos_len=200,          # 빠진 인자 (기존과 동일)\n",
    "    dropout=0.5,          # 0.3 → 0.5로 증가\n",
    "    shared_fc=True,       # 기존과 동일\n",
    "    shared_emb=True       # 기존과 동일\n",
    ").to(device)\n",
    "\n",
    "# 새 옵티마이저\n",
    "optimizer_new = torch.optim.Adam(transformer_new.parameters(), lr=0.0005)\n",
    "\n",
    "print(\"새 모델 생성 완료!\")\n",
    "print(f\"파라미터 수: {sum(p.numel() for p in transformer_new.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "5deed3de-0513-4326-a592-7439073e7bd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 26%|██▋       | 39/148 [04:55<13:46,  7.58s/it, Batch Loss=3049.3149]\n",
      "\n",
      "  0%|          | 0/148 [00:00<?, ?it/s, Batch Loss=2901.6145]\u001b[A\n",
      "  1%|          | 1/148 [00:00<00:25,  5.76it/s, Batch Loss=2901.6145]\u001b[A\n",
      "  1%|          | 1/148 [00:00<00:25,  5.76it/s, Batch Loss=2990.3000]\u001b[A\n",
      "  1%|▏         | 2/148 [00:00<00:22,  6.39it/s, Batch Loss=2990.3000]\u001b[A\n",
      "  1%|▏         | 2/148 [00:00<00:22,  6.39it/s, Batch Loss=3182.1257]\u001b[A\n",
      "  2%|▏         | 3/148 [00:00<00:20,  7.24it/s, Batch Loss=3182.1257]\u001b[A\n",
      "  2%|▏         | 3/148 [00:00<00:20,  7.24it/s, Batch Loss=3180.3779]\u001b[A\n",
      "  3%|▎         | 4/148 [00:00<00:18,  7.72it/s, Batch Loss=3180.3779]\u001b[A\n",
      "  3%|▎         | 4/148 [00:00<00:18,  7.72it/s, Batch Loss=3083.6580]\u001b[A\n",
      "  3%|▎         | 5/148 [00:00<00:17,  8.08it/s, Batch Loss=3083.6580]\u001b[A\n",
      "  3%|▎         | 5/148 [00:00<00:17,  8.08it/s, Batch Loss=2715.8281]\u001b[A\n",
      "  4%|▍         | 6/148 [00:00<00:17,  8.27it/s, Batch Loss=2715.8281]\u001b[A\n",
      "  4%|▍         | 6/148 [00:00<00:17,  8.27it/s, Batch Loss=3104.1433]\u001b[A\n",
      "  5%|▍         | 7/148 [00:00<00:16,  8.41it/s, Batch Loss=3104.1433]\u001b[A\n",
      "  5%|▍         | 7/148 [00:01<00:16,  8.41it/s, Batch Loss=2924.0505]\u001b[A\n",
      "  5%|▌         | 8/148 [00:01<00:16,  8.52it/s, Batch Loss=2924.0505]\u001b[A\n",
      "  5%|▌         | 8/148 [00:01<00:16,  8.52it/s, Batch Loss=3078.2227]\u001b[A\n",
      "  6%|▌         | 9/148 [00:01<00:16,  8.53it/s, Batch Loss=3078.2227]\u001b[A\n",
      "  6%|▌         | 9/148 [00:01<00:16,  8.53it/s, Batch Loss=3156.0244]\u001b[A\n",
      "  7%|▋         | 10/148 [00:01<00:16,  8.50it/s, Batch Loss=3156.0244]\u001b[A\n",
      "  7%|▋         | 10/148 [00:01<00:16,  8.50it/s, Batch Loss=2914.6729]\u001b[A\n",
      "  7%|▋         | 11/148 [00:01<00:16,  8.50it/s, Batch Loss=2914.6729]\u001b[A\n",
      "  7%|▋         | 11/148 [00:01<00:16,  8.50it/s, Batch Loss=3139.0044]\u001b[A\n",
      "  8%|▊         | 12/148 [00:01<00:15,  8.55it/s, Batch Loss=3139.0044]\u001b[A\n",
      "  8%|▊         | 12/148 [00:01<00:15,  8.55it/s, Batch Loss=3095.5125]\u001b[A\n",
      "  9%|▉         | 13/148 [00:01<00:15,  8.59it/s, Batch Loss=3095.5125]\u001b[A\n",
      "  9%|▉         | 13/148 [00:01<00:15,  8.59it/s, Batch Loss=3032.3652]\u001b[A\n",
      "  9%|▉         | 14/148 [00:01<00:15,  8.59it/s, Batch Loss=3032.3652]\u001b[A\n",
      "  9%|▉         | 14/148 [00:01<00:15,  8.59it/s, Batch Loss=3107.3982]\u001b[A\n",
      " 10%|█         | 15/148 [00:01<00:15,  8.64it/s, Batch Loss=3107.3982]\u001b[A\n",
      " 10%|█         | 15/148 [00:01<00:15,  8.64it/s, Batch Loss=3154.0237]\u001b[A\n",
      " 11%|█         | 16/148 [00:01<00:15,  8.65it/s, Batch Loss=3154.0237]\u001b[A\n",
      " 11%|█         | 16/148 [00:02<00:15,  8.65it/s, Batch Loss=3395.7424]\u001b[A\n",
      " 11%|█▏        | 17/148 [00:02<00:15,  8.61it/s, Batch Loss=3395.7424]\u001b[A\n",
      " 11%|█▏        | 17/148 [00:02<00:15,  8.61it/s, Batch Loss=2938.3794]\u001b[A\n",
      " 12%|█▏        | 18/148 [00:02<00:15,  8.61it/s, Batch Loss=2938.3794]\u001b[A\n",
      " 12%|█▏        | 18/148 [00:02<00:15,  8.61it/s, Batch Loss=3237.9514]\u001b[A\n",
      " 13%|█▎        | 19/148 [00:02<00:14,  8.66it/s, Batch Loss=3237.9514]\u001b[A\n",
      " 13%|█▎        | 19/148 [00:02<00:14,  8.66it/s, Batch Loss=2741.3464]\u001b[A\n",
      " 14%|█▎        | 20/148 [00:02<00:14,  8.57it/s, Batch Loss=2741.3464]\u001b[A\n",
      " 14%|█▎        | 20/148 [00:02<00:14,  8.57it/s, Batch Loss=3118.5769]\u001b[A\n",
      " 14%|█▍        | 21/148 [00:02<00:14,  8.51it/s, Batch Loss=3118.5769]\u001b[A\n",
      " 14%|█▍        | 21/148 [00:02<00:14,  8.51it/s, Batch Loss=3060.8601]\u001b[A\n",
      " 15%|█▍        | 22/148 [00:02<00:14,  8.50it/s, Batch Loss=3060.8601]\u001b[A\n",
      " 15%|█▍        | 22/148 [00:02<00:14,  8.50it/s, Batch Loss=3139.7244]\u001b[A\n",
      " 16%|█▌        | 23/148 [00:02<00:14,  8.55it/s, Batch Loss=3139.7244]\u001b[A\n",
      " 16%|█▌        | 23/148 [00:02<00:14,  8.55it/s, Batch Loss=2730.5410]\u001b[A\n",
      " 16%|█▌        | 24/148 [00:02<00:14,  8.57it/s, Batch Loss=2730.5410]\u001b[A\n",
      " 16%|█▌        | 24/148 [00:02<00:14,  8.57it/s, Batch Loss=3019.4014]\u001b[A\n",
      " 17%|█▋        | 25/148 [00:02<00:14,  8.54it/s, Batch Loss=3019.4014]\u001b[A\n",
      " 17%|█▋        | 25/148 [00:03<00:14,  8.54it/s, Batch Loss=3021.6067]\u001b[A\n",
      " 18%|█▊        | 26/148 [00:03<00:14,  8.49it/s, Batch Loss=3021.6067]\u001b[A\n",
      " 18%|█▊        | 26/148 [00:03<00:14,  8.49it/s, Batch Loss=2925.2900]\u001b[A\n",
      " 18%|█▊        | 27/148 [00:03<00:14,  8.52it/s, Batch Loss=2925.2900]\u001b[A\n",
      " 18%|█▊        | 27/148 [00:03<00:14,  8.52it/s, Batch Loss=3113.4583]\u001b[A\n",
      " 19%|█▉        | 28/148 [00:03<00:14,  8.52it/s, Batch Loss=3113.4583]\u001b[A\n",
      " 19%|█▉        | 28/148 [00:03<00:14,  8.52it/s, Batch Loss=3338.2878]\u001b[A\n",
      " 20%|█▉        | 29/148 [00:03<00:13,  8.53it/s, Batch Loss=3338.2878]\u001b[A\n",
      " 20%|█▉        | 29/148 [00:03<00:13,  8.53it/s, Batch Loss=3001.0388]\u001b[A\n",
      " 20%|██        | 30/148 [00:03<00:14,  8.41it/s, Batch Loss=3001.0388]\u001b[A\n",
      " 20%|██        | 30/148 [00:03<00:14,  8.41it/s, Batch Loss=2967.7664]\u001b[A\n",
      " 21%|██        | 31/148 [00:03<00:13,  8.44it/s, Batch Loss=2967.7664]\u001b[A\n",
      " 21%|██        | 31/148 [00:03<00:13,  8.44it/s, Batch Loss=3004.5334]\u001b[A\n",
      " 22%|██▏       | 32/148 [00:03<00:13,  8.39it/s, Batch Loss=3004.5334]\u001b[A\n",
      " 22%|██▏       | 32/148 [00:03<00:13,  8.39it/s, Batch Loss=3069.6614]\u001b[A\n",
      " 22%|██▏       | 33/148 [00:03<00:13,  8.43it/s, Batch Loss=3069.6614]\u001b[A\n",
      " 22%|██▏       | 33/148 [00:04<00:13,  8.43it/s, Batch Loss=2981.6243]\u001b[A\n",
      " 23%|██▎       | 34/148 [00:04<00:13,  8.45it/s, Batch Loss=2981.6243]\u001b[A\n",
      " 23%|██▎       | 34/148 [00:04<00:13,  8.45it/s, Batch Loss=3165.4177]\u001b[A\n",
      " 24%|██▎       | 35/148 [00:04<00:13,  8.46it/s, Batch Loss=3165.4177]\u001b[A\n",
      " 24%|██▎       | 35/148 [00:04<00:13,  8.46it/s, Batch Loss=3217.9294]\u001b[A\n",
      " 24%|██▍       | 36/148 [00:04<00:13,  8.50it/s, Batch Loss=3217.9294]\u001b[A\n",
      " 24%|██▍       | 36/148 [00:04<00:13,  8.50it/s, Batch Loss=2823.4167]\u001b[A\n",
      " 25%|██▌       | 37/148 [00:04<00:13,  8.47it/s, Batch Loss=2823.4167]\u001b[A\n",
      " 25%|██▌       | 37/148 [00:04<00:13,  8.47it/s, Batch Loss=3011.2185]\u001b[A\n",
      " 26%|██▌       | 38/148 [00:04<00:13,  8.45it/s, Batch Loss=3011.2185]\u001b[A\n",
      " 26%|██▌       | 38/148 [00:04<00:13,  8.45it/s, Batch Loss=3113.1145]\u001b[A\n",
      " 26%|██▋       | 39/148 [00:04<00:12,  8.49it/s, Batch Loss=3113.1145]\u001b[A\n",
      " 26%|██▋       | 39/148 [00:04<00:12,  8.49it/s, Batch Loss=2759.5374]\u001b[A\n",
      " 27%|██▋       | 40/148 [00:04<00:12,  8.48it/s, Batch Loss=2759.5374]\u001b[A\n",
      " 27%|██▋       | 40/148 [00:04<00:12,  8.48it/s, Batch Loss=2963.9832]\u001b[A\n",
      " 28%|██▊       | 41/148 [00:04<00:12,  8.39it/s, Batch Loss=2963.9832]\u001b[A\n",
      " 28%|██▊       | 41/148 [00:05<00:12,  8.39it/s, Batch Loss=2816.9563]\u001b[A\n",
      " 28%|██▊       | 42/148 [00:05<00:12,  8.44it/s, Batch Loss=2816.9563]\u001b[A\n",
      " 28%|██▊       | 42/148 [00:05<00:12,  8.44it/s, Batch Loss=2780.5999]\u001b[A\n",
      " 29%|██▉       | 43/148 [00:05<00:12,  8.40it/s, Batch Loss=2780.5999]\u001b[A\n",
      " 29%|██▉       | 43/148 [00:05<00:12,  8.40it/s, Batch Loss=2999.3826]\u001b[A\n",
      " 30%|██▉       | 44/148 [00:05<00:12,  8.41it/s, Batch Loss=2999.3826]\u001b[A\n",
      " 30%|██▉       | 44/148 [00:05<00:12,  8.41it/s, Batch Loss=3095.8999]\u001b[A\n",
      " 30%|███       | 45/148 [00:05<00:12,  8.45it/s, Batch Loss=3095.8999]\u001b[A\n",
      " 30%|███       | 45/148 [00:05<00:12,  8.45it/s, Batch Loss=3025.7166]\u001b[A\n",
      " 31%|███       | 46/148 [00:05<00:12,  8.50it/s, Batch Loss=3025.7166]\u001b[A\n",
      " 31%|███       | 46/148 [00:05<00:12,  8.50it/s, Batch Loss=3172.9705]\u001b[A\n",
      " 32%|███▏      | 47/148 [00:05<00:11,  8.49it/s, Batch Loss=3172.9705]\u001b[A\n",
      " 32%|███▏      | 47/148 [00:05<00:11,  8.49it/s, Batch Loss=3168.9854]\u001b[A\n",
      " 32%|███▏      | 48/148 [00:05<00:11,  8.45it/s, Batch Loss=3168.9854]\u001b[A\n",
      " 32%|███▏      | 48/148 [00:05<00:11,  8.45it/s, Batch Loss=2900.6929]\u001b[A\n",
      " 33%|███▎      | 49/148 [00:05<00:11,  8.34it/s, Batch Loss=2900.6929]\u001b[A\n",
      " 33%|███▎      | 49/148 [00:05<00:11,  8.34it/s, Batch Loss=2960.7803]\u001b[A\n",
      " 34%|███▍      | 50/148 [00:05<00:11,  8.33it/s, Batch Loss=2960.7803]\u001b[A\n",
      " 34%|███▍      | 50/148 [00:06<00:11,  8.33it/s, Batch Loss=3026.6013]\u001b[A\n",
      " 34%|███▍      | 51/148 [00:06<00:11,  8.33it/s, Batch Loss=3026.6013]\u001b[A\n",
      " 34%|███▍      | 51/148 [00:06<00:11,  8.33it/s, Batch Loss=3314.7737]\u001b[A\n",
      " 35%|███▌      | 52/148 [00:06<00:11,  8.30it/s, Batch Loss=3314.7737]\u001b[A\n",
      " 35%|███▌      | 52/148 [00:06<00:11,  8.30it/s, Batch Loss=2990.1636]\u001b[A\n",
      " 36%|███▌      | 53/148 [00:06<00:11,  8.34it/s, Batch Loss=2990.1636]\u001b[A\n",
      " 36%|███▌      | 53/148 [00:06<00:11,  8.34it/s, Batch Loss=2861.8633]\u001b[A\n",
      " 36%|███▋      | 54/148 [00:06<00:11,  8.37it/s, Batch Loss=2861.8633]\u001b[A\n",
      " 36%|███▋      | 54/148 [00:06<00:11,  8.37it/s, Batch Loss=2999.6282]\u001b[A\n",
      " 37%|███▋      | 55/148 [00:06<00:11,  8.43it/s, Batch Loss=2999.6282]\u001b[A\n",
      " 37%|███▋      | 55/148 [00:06<00:11,  8.43it/s, Batch Loss=2987.5000]\u001b[A\n",
      " 38%|███▊      | 56/148 [00:06<00:10,  8.48it/s, Batch Loss=2987.5000]\u001b[A\n",
      " 38%|███▊      | 56/148 [00:06<00:10,  8.48it/s, Batch Loss=3067.5596]\u001b[A\n",
      " 39%|███▊      | 57/148 [00:06<00:10,  8.36it/s, Batch Loss=3067.5596]\u001b[A\n",
      " 39%|███▊      | 57/148 [00:06<00:10,  8.36it/s, Batch Loss=3051.9849]\u001b[A\n",
      " 39%|███▉      | 58/148 [00:06<00:10,  8.36it/s, Batch Loss=3051.9849]\u001b[A\n",
      " 39%|███▉      | 58/148 [00:07<00:10,  8.36it/s, Batch Loss=2824.7869]\u001b[A\n",
      " 40%|███▉      | 59/148 [00:07<00:10,  8.34it/s, Batch Loss=2824.7869]\u001b[A\n",
      " 40%|███▉      | 59/148 [00:07<00:10,  8.34it/s, Batch Loss=2795.3652]\u001b[A\n",
      " 41%|████      | 60/148 [00:07<00:10,  8.32it/s, Batch Loss=2795.3652]\u001b[A\n",
      " 41%|████      | 60/148 [00:07<00:10,  8.32it/s, Batch Loss=3102.1951]\u001b[A\n",
      " 41%|████      | 61/148 [00:07<00:10,  8.25it/s, Batch Loss=3102.1951]\u001b[A\n",
      " 41%|████      | 61/148 [00:07<00:10,  8.25it/s, Batch Loss=3136.9922]\u001b[A\n",
      " 42%|████▏     | 62/148 [00:07<00:10,  8.24it/s, Batch Loss=3136.9922]\u001b[A\n",
      " 42%|████▏     | 62/148 [00:07<00:10,  8.24it/s, Batch Loss=2905.2439]\u001b[A\n",
      " 43%|████▎     | 63/148 [00:07<00:10,  8.23it/s, Batch Loss=2905.2439]\u001b[A\n",
      " 43%|████▎     | 63/148 [00:07<00:10,  8.23it/s, Batch Loss=3317.0234]\u001b[A\n",
      " 43%|████▎     | 64/148 [00:07<00:10,  8.32it/s, Batch Loss=3317.0234]\u001b[A\n",
      " 43%|████▎     | 64/148 [00:07<00:10,  8.32it/s, Batch Loss=2869.5942]\u001b[A\n",
      " 44%|████▍     | 65/148 [00:07<00:09,  8.31it/s, Batch Loss=2869.5942]\u001b[A\n",
      " 44%|████▍     | 65/148 [00:07<00:09,  8.31it/s, Batch Loss=2714.0627]\u001b[A\n",
      " 45%|████▍     | 66/148 [00:07<00:09,  8.28it/s, Batch Loss=2714.0627]\u001b[A\n",
      " 45%|████▍     | 66/148 [00:07<00:09,  8.28it/s, Batch Loss=3101.0613]\u001b[A\n",
      " 45%|████▌     | 67/148 [00:07<00:09,  8.33it/s, Batch Loss=3101.0613]\u001b[A\n",
      " 45%|████▌     | 67/148 [00:08<00:09,  8.33it/s, Batch Loss=2856.4075]\u001b[A\n",
      " 46%|████▌     | 68/148 [00:08<00:09,  8.37it/s, Batch Loss=2856.4075]\u001b[A\n",
      " 46%|████▌     | 68/148 [00:08<00:09,  8.37it/s, Batch Loss=3078.5764]\u001b[A\n",
      " 47%|████▋     | 69/148 [00:08<00:09,  8.36it/s, Batch Loss=3078.5764]\u001b[A\n",
      " 47%|████▋     | 69/148 [00:08<00:09,  8.36it/s, Batch Loss=3165.5723]\u001b[A\n",
      " 47%|████▋     | 70/148 [00:08<00:09,  8.31it/s, Batch Loss=3165.5723]\u001b[A\n",
      " 47%|████▋     | 70/148 [00:08<00:09,  8.31it/s, Batch Loss=3184.8892]\u001b[A\n",
      " 48%|████▊     | 71/148 [00:08<00:09,  8.34it/s, Batch Loss=3184.8892]\u001b[A\n",
      " 48%|████▊     | 71/148 [00:08<00:09,  8.34it/s, Batch Loss=2764.5554]\u001b[A\n",
      " 49%|████▊     | 72/148 [00:08<00:09,  8.34it/s, Batch Loss=2764.5554]\u001b[A\n",
      " 49%|████▊     | 72/148 [00:08<00:09,  8.34it/s, Batch Loss=3042.4053]\u001b[A\n",
      " 49%|████▉     | 73/148 [00:08<00:08,  8.35it/s, Batch Loss=3042.4053]\u001b[A\n",
      " 49%|████▉     | 73/148 [00:08<00:08,  8.35it/s, Batch Loss=2887.7375]\u001b[A\n",
      " 50%|█████     | 74/148 [00:08<00:08,  8.25it/s, Batch Loss=2887.7375]\u001b[A\n",
      " 50%|█████     | 74/148 [00:08<00:08,  8.25it/s, Batch Loss=2819.3225]\u001b[A\n",
      " 51%|█████     | 75/148 [00:08<00:08,  8.21it/s, Batch Loss=2819.3225]\u001b[A\n",
      " 51%|█████     | 75/148 [00:09<00:08,  8.21it/s, Batch Loss=3285.2546]\u001b[A\n",
      " 51%|█████▏    | 76/148 [00:09<00:08,  8.21it/s, Batch Loss=3285.2546]\u001b[A\n",
      " 51%|█████▏    | 76/148 [00:09<00:08,  8.21it/s, Batch Loss=3168.1807]\u001b[A\n",
      " 52%|█████▏    | 77/148 [00:09<00:08,  8.32it/s, Batch Loss=3168.1807]\u001b[A\n",
      " 52%|█████▏    | 77/148 [00:09<00:08,  8.32it/s, Batch Loss=3083.1367]\u001b[A\n",
      " 53%|█████▎    | 78/148 [00:09<00:08,  8.22it/s, Batch Loss=3083.1367]\u001b[A\n",
      " 53%|█████▎    | 78/148 [00:09<00:08,  8.22it/s, Batch Loss=3060.4360]\u001b[A\n",
      " 53%|█████▎    | 79/148 [00:09<00:08,  8.20it/s, Batch Loss=3060.4360]\u001b[A\n",
      " 53%|█████▎    | 79/148 [00:09<00:08,  8.20it/s, Batch Loss=3136.5723]\u001b[A\n",
      " 54%|█████▍    | 80/148 [00:09<00:08,  8.24it/s, Batch Loss=3136.5723]\u001b[A\n",
      " 54%|█████▍    | 80/148 [00:09<00:08,  8.24it/s, Batch Loss=2616.2891]\u001b[A\n",
      " 55%|█████▍    | 81/148 [00:09<00:08,  8.27it/s, Batch Loss=2616.2891]\u001b[A\n",
      " 55%|█████▍    | 81/148 [00:09<00:08,  8.27it/s, Batch Loss=2989.8318]\u001b[A\n",
      " 55%|█████▌    | 82/148 [00:09<00:08,  8.21it/s, Batch Loss=2989.8318]\u001b[A\n",
      " 55%|█████▌    | 82/148 [00:09<00:08,  8.21it/s, Batch Loss=2857.3948]\u001b[A\n",
      " 56%|█████▌    | 83/148 [00:09<00:07,  8.17it/s, Batch Loss=2857.3948]\u001b[A\n",
      " 56%|█████▌    | 83/148 [00:10<00:07,  8.17it/s, Batch Loss=2976.7935]\u001b[A\n",
      " 57%|█████▋    | 84/148 [00:10<00:07,  8.19it/s, Batch Loss=2976.7935]\u001b[A\n",
      " 57%|█████▋    | 84/148 [00:10<00:07,  8.19it/s, Batch Loss=2862.9961]\u001b[A\n",
      " 57%|█████▋    | 85/148 [00:10<00:07,  8.28it/s, Batch Loss=2862.9961]\u001b[A\n",
      " 57%|█████▋    | 85/148 [00:10<00:07,  8.28it/s, Batch Loss=2787.2734]\u001b[A\n",
      " 58%|█████▊    | 86/148 [00:10<00:07,  8.28it/s, Batch Loss=2787.2734]\u001b[A\n",
      " 58%|█████▊    | 86/148 [00:10<00:07,  8.28it/s, Batch Loss=3250.6423]\u001b[A\n",
      " 59%|█████▉    | 87/148 [00:10<00:07,  8.18it/s, Batch Loss=3250.6423]\u001b[A\n",
      " 59%|█████▉    | 87/148 [00:10<00:07,  8.18it/s, Batch Loss=3015.3713]\u001b[A\n",
      " 59%|█████▉    | 88/148 [00:10<00:07,  8.16it/s, Batch Loss=3015.3713]\u001b[A\n",
      " 59%|█████▉    | 88/148 [00:10<00:07,  8.16it/s, Batch Loss=2878.5791]\u001b[A\n",
      " 60%|██████    | 89/148 [00:10<00:07,  8.25it/s, Batch Loss=2878.5791]\u001b[A\n",
      " 60%|██████    | 89/148 [00:10<00:07,  8.25it/s, Batch Loss=2794.0022]\u001b[A\n",
      " 61%|██████    | 90/148 [00:10<00:07,  8.19it/s, Batch Loss=2794.0022]\u001b[A\n",
      " 61%|██████    | 90/148 [00:10<00:07,  8.19it/s, Batch Loss=2855.1067]\u001b[A\n",
      " 61%|██████▏   | 91/148 [00:10<00:06,  8.18it/s, Batch Loss=2855.1067]\u001b[A\n",
      " 61%|██████▏   | 91/148 [00:11<00:06,  8.18it/s, Batch Loss=2967.6343]\u001b[A\n",
      " 62%|██████▏   | 92/148 [00:11<00:06,  8.12it/s, Batch Loss=2967.6343]\u001b[A\n",
      " 62%|██████▏   | 92/148 [00:11<00:06,  8.12it/s, Batch Loss=2819.1194]\u001b[A\n",
      " 63%|██████▎   | 93/148 [00:11<00:06,  8.22it/s, Batch Loss=2819.1194]\u001b[A\n",
      " 63%|██████▎   | 93/148 [00:11<00:06,  8.22it/s, Batch Loss=3106.5491]\u001b[A\n",
      " 64%|██████▎   | 94/148 [00:11<00:06,  8.28it/s, Batch Loss=3106.5491]\u001b[A\n",
      " 64%|██████▎   | 94/148 [00:11<00:06,  8.28it/s, Batch Loss=2864.5125]\u001b[A\n",
      " 64%|██████▍   | 95/148 [00:11<00:06,  8.26it/s, Batch Loss=2864.5125]\u001b[A\n",
      " 64%|██████▍   | 95/148 [00:11<00:06,  8.26it/s, Batch Loss=3123.5837]\u001b[A\n",
      " 65%|██████▍   | 96/148 [00:11<00:06,  8.18it/s, Batch Loss=3123.5837]\u001b[A\n",
      " 65%|██████▍   | 96/148 [00:11<00:06,  8.18it/s, Batch Loss=2802.8469]\u001b[A\n",
      " 66%|██████▌   | 97/148 [00:11<00:06,  8.17it/s, Batch Loss=2802.8469]\u001b[A\n",
      " 66%|██████▌   | 97/148 [00:11<00:06,  8.17it/s, Batch Loss=3130.5830]\u001b[A\n",
      " 66%|██████▌   | 98/148 [00:11<00:06,  8.16it/s, Batch Loss=3130.5830]\u001b[A\n",
      " 66%|██████▌   | 98/148 [00:11<00:06,  8.16it/s, Batch Loss=2813.0583]\u001b[A\n",
      " 67%|██████▋   | 99/148 [00:11<00:05,  8.23it/s, Batch Loss=2813.0583]\u001b[A\n",
      " 67%|██████▋   | 99/148 [00:12<00:05,  8.23it/s, Batch Loss=2863.0906]\u001b[A\n",
      " 68%|██████▊   | 100/148 [00:12<00:06,  7.76it/s, Batch Loss=2863.0906]\u001b[A\n",
      " 68%|██████▊   | 100/148 [00:12<00:06,  7.76it/s, Batch Loss=2888.5234]\u001b[A\n",
      " 68%|██████▊   | 101/148 [00:12<00:06,  7.42it/s, Batch Loss=2888.5234]\u001b[A\n",
      " 68%|██████▊   | 101/148 [00:12<00:06,  7.42it/s, Batch Loss=3092.8386]\u001b[A\n",
      " 69%|██████▉   | 102/148 [00:12<00:06,  7.15it/s, Batch Loss=3092.8386]\u001b[A\n",
      " 69%|██████▉   | 102/148 [00:12<00:06,  7.15it/s, Batch Loss=3036.9983]\u001b[A\n",
      " 70%|██████▉   | 103/148 [00:12<00:06,  6.95it/s, Batch Loss=3036.9983]\u001b[A\n",
      " 70%|██████▉   | 103/148 [00:12<00:06,  6.95it/s, Batch Loss=3348.3235]\u001b[A\n",
      " 70%|███████   | 104/148 [00:12<00:06,  6.84it/s, Batch Loss=3348.3235]\u001b[A\n",
      " 70%|███████   | 104/148 [00:12<00:06,  6.84it/s, Batch Loss=2973.4412]\u001b[A\n",
      " 71%|███████   | 105/148 [00:12<00:06,  6.73it/s, Batch Loss=2973.4412]\u001b[A\n",
      " 71%|███████   | 105/148 [00:12<00:06,  6.73it/s, Batch Loss=3014.7424]\u001b[A\n",
      " 72%|███████▏  | 106/148 [00:12<00:06,  6.68it/s, Batch Loss=3014.7424]\u001b[A\n",
      " 72%|███████▏  | 106/148 [00:13<00:06,  6.68it/s, Batch Loss=2810.5081]\u001b[A\n",
      " 72%|███████▏  | 107/148 [00:13<00:06,  6.62it/s, Batch Loss=2810.5081]\u001b[A\n",
      " 72%|███████▏  | 107/148 [00:13<00:06,  6.62it/s, Batch Loss=3111.0947]\u001b[A\n",
      " 73%|███████▎  | 108/148 [00:13<00:06,  6.60it/s, Batch Loss=3111.0947]\u001b[A\n",
      " 73%|███████▎  | 108/148 [00:13<00:06,  6.60it/s, Batch Loss=3011.5068]\u001b[A\n",
      " 74%|███████▎  | 109/148 [00:13<00:05,  6.56it/s, Batch Loss=3011.5068]\u001b[A\n",
      " 74%|███████▎  | 109/148 [00:13<00:05,  6.56it/s, Batch Loss=3097.1387]\u001b[A\n",
      " 74%|███████▍  | 110/148 [00:13<00:05,  6.52it/s, Batch Loss=3097.1387]\u001b[A\n",
      " 74%|███████▍  | 110/148 [00:13<00:05,  6.52it/s, Batch Loss=3093.7576]\u001b[A\n",
      " 75%|███████▌  | 111/148 [00:13<00:05,  6.53it/s, Batch Loss=3093.7576]\u001b[A\n",
      " 75%|███████▌  | 111/148 [00:13<00:05,  6.53it/s, Batch Loss=3230.4248]\u001b[A\n",
      " 76%|███████▌  | 112/148 [00:13<00:05,  6.54it/s, Batch Loss=3230.4248]\u001b[A\n",
      " 76%|███████▌  | 112/148 [00:14<00:05,  6.54it/s, Batch Loss=2913.1067]\u001b[A\n",
      " 76%|███████▋  | 113/148 [00:14<00:05,  6.54it/s, Batch Loss=2913.1067]\u001b[A\n",
      " 76%|███████▋  | 113/148 [00:14<00:05,  6.54it/s, Batch Loss=3113.4968]\u001b[A\n",
      " 77%|███████▋  | 114/148 [00:14<00:05,  6.54it/s, Batch Loss=3113.4968]\u001b[A\n",
      " 77%|███████▋  | 114/148 [00:14<00:05,  6.54it/s, Batch Loss=2949.4724]\u001b[A\n",
      " 78%|███████▊  | 115/148 [00:14<00:05,  6.56it/s, Batch Loss=2949.4724]\u001b[A\n",
      " 78%|███████▊  | 115/148 [00:14<00:05,  6.56it/s, Batch Loss=3261.3545]\u001b[A\n",
      " 78%|███████▊  | 116/148 [00:14<00:04,  6.52it/s, Batch Loss=3261.3545]\u001b[A\n",
      " 78%|███████▊  | 116/148 [00:14<00:04,  6.52it/s, Batch Loss=2867.4824]\u001b[A\n",
      " 79%|███████▉  | 117/148 [00:14<00:04,  6.52it/s, Batch Loss=2867.4824]\u001b[A\n",
      " 79%|███████▉  | 117/148 [00:14<00:04,  6.52it/s, Batch Loss=3163.6812]\u001b[A\n",
      " 80%|███████▉  | 118/148 [00:14<00:04,  6.52it/s, Batch Loss=3163.6812]\u001b[A\n",
      " 80%|███████▉  | 118/148 [00:14<00:04,  6.52it/s, Batch Loss=3060.8132]\u001b[A\n",
      " 80%|████████  | 119/148 [00:14<00:04,  6.53it/s, Batch Loss=3060.8132]\u001b[A\n",
      " 80%|████████  | 119/148 [00:15<00:04,  6.53it/s, Batch Loss=3072.9006]\u001b[A\n",
      " 81%|████████  | 120/148 [00:15<00:04,  6.52it/s, Batch Loss=3072.9006]\u001b[A\n",
      " 81%|████████  | 120/148 [00:15<00:04,  6.52it/s, Batch Loss=3028.8538]\u001b[A\n",
      " 82%|████████▏ | 121/148 [00:15<00:04,  6.54it/s, Batch Loss=3028.8538]\u001b[A\n",
      " 82%|████████▏ | 121/148 [00:15<00:04,  6.54it/s, Batch Loss=3084.7009]\u001b[A\n",
      " 82%|████████▏ | 122/148 [00:15<00:03,  6.55it/s, Batch Loss=3084.7009]\u001b[A\n",
      " 82%|████████▏ | 122/148 [00:15<00:03,  6.55it/s, Batch Loss=3090.4768]\u001b[A\n",
      " 83%|████████▎ | 123/148 [00:15<00:03,  6.57it/s, Batch Loss=3090.4768]\u001b[A\n",
      " 83%|████████▎ | 123/148 [00:15<00:03,  6.57it/s, Batch Loss=3057.5813]\u001b[A\n",
      " 84%|████████▍ | 124/148 [00:15<00:03,  6.55it/s, Batch Loss=3057.5813]\u001b[A\n",
      " 84%|████████▍ | 124/148 [00:15<00:03,  6.55it/s, Batch Loss=3001.8481]\u001b[A\n",
      " 84%|████████▍ | 125/148 [00:15<00:03,  6.55it/s, Batch Loss=3001.8481]\u001b[A\n",
      " 84%|████████▍ | 125/148 [00:16<00:03,  6.55it/s, Batch Loss=3092.3406]\u001b[A\n",
      " 85%|████████▌ | 126/148 [00:16<00:03,  6.55it/s, Batch Loss=3092.3406]\u001b[A\n",
      " 85%|████████▌ | 126/148 [00:16<00:03,  6.55it/s, Batch Loss=2864.3616]\u001b[A\n",
      " 86%|████████▌ | 127/148 [00:16<00:03,  6.53it/s, Batch Loss=2864.3616]\u001b[A\n",
      " 86%|████████▌ | 127/148 [00:16<00:03,  6.53it/s, Batch Loss=3258.4109]\u001b[A\n",
      " 86%|████████▋ | 128/148 [00:16<00:03,  6.54it/s, Batch Loss=3258.4109]\u001b[A\n",
      " 86%|████████▋ | 128/148 [00:16<00:03,  6.54it/s, Batch Loss=3036.7402]\u001b[A\n",
      " 87%|████████▋ | 129/148 [00:16<00:02,  6.52it/s, Batch Loss=3036.7402]\u001b[A\n",
      " 87%|████████▋ | 129/148 [00:16<00:02,  6.52it/s, Batch Loss=2863.8179]\u001b[A\n",
      " 88%|████████▊ | 130/148 [00:16<00:02,  6.54it/s, Batch Loss=2863.8179]\u001b[A\n",
      " 88%|████████▊ | 130/148 [00:16<00:02,  6.54it/s, Batch Loss=2970.6367]\u001b[A\n",
      " 89%|████████▊ | 131/148 [00:16<00:02,  6.57it/s, Batch Loss=2970.6367]\u001b[A\n",
      " 89%|████████▊ | 131/148 [00:16<00:02,  6.57it/s, Batch Loss=3023.7407]\u001b[A\n",
      " 89%|████████▉ | 132/148 [00:16<00:02,  6.57it/s, Batch Loss=3023.7407]\u001b[A\n",
      " 89%|████████▉ | 132/148 [00:17<00:02,  6.57it/s, Batch Loss=3156.6824]\u001b[A\n",
      " 90%|████████▉ | 133/148 [00:17<00:02,  6.58it/s, Batch Loss=3156.6824]\u001b[A\n",
      " 90%|████████▉ | 133/148 [00:17<00:02,  6.58it/s, Batch Loss=2871.0859]\u001b[A\n",
      " 91%|█████████ | 134/148 [00:17<00:02,  6.59it/s, Batch Loss=2871.0859]\u001b[A\n",
      " 91%|█████████ | 134/148 [00:17<00:02,  6.59it/s, Batch Loss=3067.1355]\u001b[A\n",
      " 91%|█████████ | 135/148 [00:17<00:01,  6.61it/s, Batch Loss=3067.1355]\u001b[A\n",
      " 91%|█████████ | 135/148 [00:17<00:01,  6.61it/s, Batch Loss=2932.8882]\u001b[A\n",
      " 92%|█████████▏| 136/148 [00:17<00:01,  6.61it/s, Batch Loss=2932.8882]\u001b[A\n",
      " 92%|█████████▏| 136/148 [00:17<00:01,  6.61it/s, Batch Loss=3052.9619]\u001b[A\n",
      " 93%|█████████▎| 137/148 [00:17<00:01,  6.62it/s, Batch Loss=3052.9619]\u001b[A\n",
      " 93%|█████████▎| 137/148 [00:17<00:01,  6.62it/s, Batch Loss=3015.5305]\u001b[A\n",
      " 93%|█████████▎| 138/148 [00:17<00:01,  6.58it/s, Batch Loss=3015.5305]\u001b[A\n",
      " 93%|█████████▎| 138/148 [00:17<00:01,  6.58it/s, Batch Loss=3101.2678]\u001b[A\n",
      " 94%|█████████▍| 139/148 [00:17<00:01,  6.94it/s, Batch Loss=3101.2678]\u001b[A\n",
      " 94%|█████████▍| 139/148 [00:18<00:01,  6.94it/s, Batch Loss=3066.6982]\u001b[A\n",
      " 95%|█████████▍| 140/148 [00:18<00:01,  7.29it/s, Batch Loss=3066.6982]\u001b[A\n",
      " 95%|█████████▍| 140/148 [00:18<00:01,  7.29it/s, Batch Loss=3148.5149]\u001b[A\n",
      " 95%|█████████▌| 141/148 [00:18<00:00,  7.50it/s, Batch Loss=3148.5149]\u001b[A\n",
      " 95%|█████████▌| 141/148 [00:18<00:00,  7.50it/s, Batch Loss=3027.7058]\u001b[A\n",
      " 96%|█████████▌| 142/148 [00:18<00:00,  7.78it/s, Batch Loss=3027.7058]\u001b[A\n",
      " 96%|█████████▌| 142/148 [00:18<00:00,  7.78it/s, Batch Loss=3239.0593]\u001b[A\n",
      " 97%|█████████▋| 143/148 [00:18<00:00,  7.94it/s, Batch Loss=3239.0593]\u001b[A\n",
      " 97%|█████████▋| 143/148 [00:18<00:00,  7.94it/s, Batch Loss=3139.8542]\u001b[A\n",
      " 97%|█████████▋| 144/148 [00:18<00:00,  8.09it/s, Batch Loss=3139.8542]\u001b[A\n",
      " 97%|█████████▋| 144/148 [00:18<00:00,  8.09it/s, Batch Loss=2792.5232]\u001b[A\n",
      " 98%|█████████▊| 145/148 [00:18<00:00,  8.18it/s, Batch Loss=2792.5232]\u001b[A\n",
      " 98%|█████████▊| 145/148 [00:18<00:00,  8.18it/s, Batch Loss=2966.8281]\u001b[A\n",
      " 99%|█████████▊| 146/148 [00:18<00:00,  8.24it/s, Batch Loss=2966.8281]\u001b[A\n",
      " 99%|█████████▊| 146/148 [00:18<00:00,  8.24it/s, Batch Loss=2938.5024]\u001b[A\n",
      " 99%|█████████▉| 147/148 [00:18<00:00,  8.22it/s, Batch Loss=2938.5024]\u001b[A\n",
      "100%|██████████| 148/148 [00:19<00:00,  7.78it/s, Batch Loss=2915.2217]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 3016.4459\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 148/148 [00:17<00:00,  8.62it/s, Batch Loss=2813.2981]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2, Loss: 2953.6212\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 148/148 [00:16<00:00,  8.98it/s, Batch Loss=2807.0603]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3, Loss: 2889.8489\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 148/148 [00:16<00:00,  9.06it/s, Batch Loss=2641.7432]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4, Loss: 2837.5059\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 148/148 [00:16<00:00,  8.98it/s, Batch Loss=2621.1321]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5, Loss: 2778.0912\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 148/148 [00:16<00:00,  8.84it/s, Batch Loss=2671.8867]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6, Loss: 2715.6643\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 148/148 [00:16<00:00,  8.78it/s, Batch Loss=2705.0022]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7, Loss: 2667.9321\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 148/148 [00:16<00:00,  8.83it/s, Batch Loss=2651.1821]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8, Loss: 2620.8610\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 148/148 [00:16<00:00,  8.92it/s, Batch Loss=2444.5098]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9, Loss: 2578.0820\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 148/148 [00:16<00:00,  8.93it/s, Batch Loss=2360.3074]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10, Loss: 2527.0121\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 148/148 [00:16<00:00,  8.90it/s, Batch Loss=2423.6187]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11, Loss: 2472.5253\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 148/148 [00:16<00:00,  8.87it/s, Batch Loss=2237.1553]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12, Loss: 2437.1412\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 148/148 [00:16<00:00,  8.86it/s, Batch Loss=2478.6985]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13, Loss: 2392.9724\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 148/148 [00:16<00:00,  8.86it/s, Batch Loss=2180.3779]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14, Loss: 2345.9440\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 148/148 [00:16<00:00,  8.88it/s, Batch Loss=2110.0759]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15, Loss: 2303.0242\n",
      "CPU times: user 4min 44s, sys: 1.44 s, total: 4min 46s\n",
      "Wall time: 4min 12s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "EPOCHS = 15\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    total_loss = 0.0\n",
    "    dataset_count = len(train_dataloader)  # train_loader는 PyTorch DataLoader입니다.\n",
    "    tqdm_bar = tqdm(total=dataset_count)\n",
    "\n",
    "    for batch, (src, tgt) in enumerate(train_dataloader):\n",
    "        # train_step 함수는 (loss, enc_attns, dec_attns, dec_enc_attns)를 반환합니다.\n",
    "        loss, enc_attns, dec_attns, dec_enc_attns = train_step(src, tgt, transformer, optimizer)\n",
    "\n",
    "        total_loss += loss.item()  # PyTorch에서는 loss.numpy() 대신 loss.item() 사용\n",
    "        tqdm_bar.set_postfix({\"Batch Loss\": f\"{loss.item():.4f}\"})\n",
    "        tqdm_bar.update(1)\n",
    "\n",
    "    tqdm_bar.close()\n",
    "    print(f\"Epoch {epoch+1}, Loss: {total_loss / dataset_count:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "519e1b65-08cb-4f79-9575-c545a3714e66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15에포크 모델 저장 완료!\n"
     ]
    }
   ],
   "source": [
    "# 현재 모델 상태 저장\n",
    "torch.save({\n",
    "    'epoch': 15,\n",
    "    'model_state_dict': transformer.state_dict(),\n",
    "    'optimizer_state_dict': optimizer.state_dict(),\n",
    "    'loss': 2303.0242\n",
    "}, 'transformer_epoch15.pt')\n",
    "\n",
    "print(\"15에포크 모델 저장 완료!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "47477e7d-af50-433a-8a81-c5fe15a482e9",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'NUM_ENCODER_LAYERS' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[51]\u001b[39m\u001b[32m, line 10\u001b[39m\n\u001b[32m      6\u001b[39m val_dataloader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=\u001b[38;5;28;01mFalse\u001b[39;00m, pin_memory=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m      8\u001b[39m \u001b[38;5;66;03m# 새 모델 (기존 이름 그대로)\u001b[39;00m\n\u001b[32m      9\u001b[39m transformer = Transformer(\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m     n_layers=\u001b[43mNUM_ENCODER_LAYERS\u001b[49m,  \u001b[38;5;66;03m# 3\u001b[39;00m\n\u001b[32m     11\u001b[39m     d_model=D_MODEL,              \u001b[38;5;66;03m# 256\u001b[39;00m\n\u001b[32m     12\u001b[39m     n_heads=N_HEAD,               \u001b[38;5;66;03m# 8\u001b[39;00m\n\u001b[32m     13\u001b[39m     d_ff=D_FF,                    \u001b[38;5;66;03m# 512\u001b[39;00m\n\u001b[32m     14\u001b[39m     src_vocab_size=VOCAB_SIZE,    \n\u001b[32m     15\u001b[39m     tgt_vocab_size=VOCAB_SIZE,    \n\u001b[32m     16\u001b[39m     pos_len=\u001b[32m200\u001b[39m,                  \n\u001b[32m     17\u001b[39m     dropout=DROPOUT,              \u001b[38;5;66;03m# 0.2\u001b[39;00m\n\u001b[32m     18\u001b[39m     shared_fc=\u001b[38;5;28;01mTrue\u001b[39;00m,               \n\u001b[32m     19\u001b[39m     shared_emb=\u001b[38;5;28;01mTrue\u001b[39;00m               \n\u001b[32m     20\u001b[39m ).to(device)\n\u001b[32m     22\u001b[39m \u001b[38;5;66;03m# 기존 옵티마이저 업데이트\u001b[39;00m\n\u001b[32m     23\u001b[39m optimizer = torch.optim.Adam(transformer.parameters(), lr=LEARNING_RATE)\n",
      "\u001b[31mNameError\u001b[39m: name 'NUM_ENCODER_LAYERS' is not defined"
     ]
    }
   ],
   "source": [
    "# 배치 사이즈 수정\n",
    "BATCH_SIZE = 128\n",
    "\n",
    "# 기존 DataLoader 업데이트\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, pin_memory=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, pin_memory=True)\n",
    "\n",
    "# 새 모델 (기존 이름 그대로)\n",
    "transformer = Transformer(\n",
    "    n_layers=NUM_ENCODER_LAYERS,  # 3\n",
    "    d_model=D_MODEL,              # 256\n",
    "    n_heads=N_HEAD,               # 8\n",
    "    d_ff=D_FF,                    # 512\n",
    "    src_vocab_size=VOCAB_SIZE,    \n",
    "    tgt_vocab_size=VOCAB_SIZE,    \n",
    "    pos_len=200,                  \n",
    "    dropout=DROPOUT,              # 0.2\n",
    "    shared_fc=True,               \n",
    "    shared_emb=True               \n",
    ").to(device)\n",
    "\n",
    "# 기존 옵티마이저 업데이트\n",
    "optimizer = torch.optim.Adam(transformer.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "print(\"설정 업데이트 완료!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "0d904e16-6d72-4720-9252-ee2161f19e01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 생성\n",
    "transformer = Transformer(\n",
    "   n_layers=3,\n",
    "   d_model=256,\n",
    "   n_heads=8,\n",
    "   d_ff=512,\n",
    "   src_vocab_size=VOCAB_SIZE,\n",
    "   tgt_vocab_size=VOCAB_SIZE,\n",
    "   pos_len=200,\n",
    "   dropout=0.2,\n",
    "   shared_fc=True,\n",
    "   shared_emb=True\n",
    ").to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(transformer.parameters(), lr=0.0001)\n",
    "BATCH_SIZE = 128\n",
    "EPOCHS = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "383828c5-7a50-4613-851e-99b3c4a4e11a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "훈련 시작\n",
      "Epoch 1, Batch 0, Loss: 4134.5352\n",
      "Epoch: 01 | Time: 0m 8s\n",
      "\tTrain Loss: 3154.857 | Val. Loss: 2391.747\n",
      "\t-> 검증 손실 감소, 모델 저장 완료.\n",
      "Epoch 2, Batch 0, Loss: 2611.2571\n",
      "Epoch: 02 | Time: 0m 11s\n",
      "\tTrain Loss: 2181.410 | Val. Loss: 1601.866\n",
      "\t-> 검증 손실 감소, 모델 저장 완료.\n",
      "Epoch 3, Batch 0, Loss: 1807.4886\n",
      "Epoch: 03 | Time: 0m 8s\n",
      "\tTrain Loss: 1710.566 | Val. Loss: 1357.760\n",
      "\t-> 검증 손실 감소, 모델 저장 완료.\n",
      "Epoch 4, Batch 0, Loss: 1612.2356\n",
      "Epoch: 04 | Time: 0m 7s\n",
      "\tTrain Loss: 1558.879 | Val. Loss: 1230.790\n",
      "\t-> 검증 손실 감소, 모델 저장 완료.\n",
      "Epoch 5, Batch 0, Loss: 1468.7509\n",
      "Epoch: 05 | Time: 0m 7s\n",
      "\tTrain Loss: 1465.160 | Val. Loss: 1153.562\n",
      "\t-> 검증 손실 감소, 모델 저장 완료.\n",
      "Epoch 6, Batch 0, Loss: 1415.3254\n",
      "Epoch: 06 | Time: 0m 7s\n",
      "\tTrain Loss: 1397.196 | Val. Loss: 1103.059\n",
      "\t-> 검증 손실 감소, 모델 저장 완료.\n",
      "Epoch 7, Batch 0, Loss: 1381.6857\n",
      "Epoch: 07 | Time: 0m 7s\n",
      "\tTrain Loss: 1348.144 | Val. Loss: 1055.165\n",
      "\t-> 검증 손실 감소, 모델 저장 완료.\n",
      "Epoch 8, Batch 0, Loss: 1342.2532\n",
      "Epoch: 08 | Time: 0m 7s\n",
      "\tTrain Loss: 1297.734 | Val. Loss: 1011.351\n",
      "\t-> 검증 손실 감소, 모델 저장 완료.\n",
      "Epoch 9, Batch 0, Loss: 1313.3372\n",
      "Epoch: 09 | Time: 0m 7s\n",
      "\tTrain Loss: 1259.217 | Val. Loss: 983.556\n",
      "\t-> 검증 손실 감소, 모델 저장 완료.\n",
      "Epoch 10, Batch 0, Loss: 1252.2252\n",
      "Epoch: 10 | Time: 0m 7s\n",
      "\tTrain Loss: 1216.850 | Val. Loss: 953.547\n",
      "\t-> 검증 손실 감소, 모델 저장 완료.\n",
      "Epoch 11, Batch 0, Loss: 1237.2736\n",
      "Epoch: 11 | Time: 0m 7s\n",
      "\tTrain Loss: 1182.189 | Val. Loss: 919.218\n",
      "\t-> 검증 손실 감소, 모델 저장 완료.\n",
      "Epoch 12, Batch 0, Loss: 1179.6919\n",
      "Epoch: 12 | Time: 0m 7s\n",
      "\tTrain Loss: 1145.222 | Val. Loss: 888.795\n",
      "\t-> 검증 손실 감소, 모델 저장 완료.\n",
      "Epoch 13, Batch 0, Loss: 1162.8899\n",
      "Epoch: 13 | Time: 0m 7s\n",
      "\tTrain Loss: 1111.694 | Val. Loss: 859.503\n",
      "\t-> 검증 손실 감소, 모델 저장 완료.\n",
      "Epoch 14, Batch 0, Loss: 1091.1064\n",
      "Epoch: 14 | Time: 0m 7s\n",
      "\tTrain Loss: 1080.167 | Val. Loss: 834.368\n",
      "\t-> 검증 손실 감소, 모델 저장 완료.\n",
      "Epoch 15, Batch 0, Loss: 1067.7472\n",
      "Epoch: 15 | Time: 0m 7s\n",
      "\tTrain Loss: 1048.983 | Val. Loss: 805.044\n",
      "\t-> 검증 손실 감소, 모델 저장 완료.\n",
      "Epoch 16, Batch 0, Loss: 1039.9008\n",
      "Epoch: 16 | Time: 0m 7s\n",
      "\tTrain Loss: 1019.777 | Val. Loss: 776.574\n",
      "\t-> 검증 손실 감소, 모델 저장 완료.\n",
      "Epoch 17, Batch 0, Loss: 1009.4420\n",
      "Epoch: 17 | Time: 0m 7s\n",
      "\tTrain Loss: 988.762 | Val. Loss: 757.807\n",
      "\t-> 검증 손실 감소, 모델 저장 완료.\n",
      "Epoch 18, Batch 0, Loss: 936.7968\n",
      "Epoch: 18 | Time: 0m 7s\n",
      "\tTrain Loss: 962.321 | Val. Loss: 734.506\n",
      "\t-> 검증 손실 감소, 모델 저장 완료.\n",
      "Epoch 19, Batch 0, Loss: 923.0200\n",
      "Epoch: 19 | Time: 0m 7s\n",
      "\tTrain Loss: 936.660 | Val. Loss: 714.666\n",
      "\t-> 검증 손실 감소, 모델 저장 완료.\n",
      "Epoch 20, Batch 0, Loss: 912.2527\n",
      "Epoch: 20 | Time: 0m 7s\n",
      "\tTrain Loss: 910.730 | Val. Loss: 690.864\n",
      "\t-> 검증 손실 감소, 모델 저장 완료.\n",
      "Epoch 21, Batch 0, Loss: 885.4619\n",
      "Epoch: 21 | Time: 0m 7s\n",
      "\tTrain Loss: 886.998 | Val. Loss: 680.460\n",
      "\t-> 검증 손실 감소, 모델 저장 완료.\n",
      "Epoch 22, Batch 0, Loss: 865.9549\n",
      "Epoch: 22 | Time: 0m 7s\n",
      "\tTrain Loss: 863.737 | Val. Loss: 656.876\n",
      "\t-> 검증 손실 감소, 모델 저장 완료.\n",
      "Epoch 23, Batch 0, Loss: 821.8220\n",
      "Epoch: 23 | Time: 0m 7s\n",
      "\tTrain Loss: 838.024 | Val. Loss: 637.221\n",
      "\t-> 검증 손실 감소, 모델 저장 완료.\n",
      "Epoch 24, Batch 0, Loss: 880.2260\n",
      "Epoch: 24 | Time: 0m 7s\n",
      "\tTrain Loss: 815.161 | Val. Loss: 617.520\n",
      "\t-> 검증 손실 감소, 모델 저장 완료.\n",
      "Epoch 25, Batch 0, Loss: 828.2784\n",
      "Epoch: 25 | Time: 0m 7s\n",
      "\tTrain Loss: 793.021 | Val. Loss: 601.184\n",
      "\t-> 검증 손실 감소, 모델 저장 완료.\n",
      "Epoch 26, Batch 0, Loss: 768.9282\n",
      "Epoch: 26 | Time: 0m 7s\n",
      "\tTrain Loss: 771.201 | Val. Loss: 584.300\n",
      "\t-> 검증 손실 감소, 모델 저장 완료.\n",
      "Epoch 27, Batch 0, Loss: 756.0493\n",
      "Epoch: 27 | Time: 0m 7s\n",
      "\tTrain Loss: 750.105 | Val. Loss: 570.760\n",
      "\t-> 검증 손실 감소, 모델 저장 완료.\n",
      "Epoch 28, Batch 0, Loss: 750.1631\n",
      "Epoch: 28 | Time: 0m 7s\n",
      "\tTrain Loss: 727.199 | Val. Loss: 550.524\n",
      "\t-> 검증 손실 감소, 모델 저장 완료.\n",
      "Epoch 29, Batch 0, Loss: 742.7670\n",
      "Epoch: 29 | Time: 0m 7s\n",
      "\tTrain Loss: 706.737 | Val. Loss: 539.232\n",
      "\t-> 검증 손실 감소, 모델 저장 완료.\n",
      "Epoch 30, Batch 0, Loss: 690.1314\n",
      "Epoch: 30 | Time: 0m 7s\n",
      "\tTrain Loss: 686.318 | Val. Loss: 518.765\n",
      "\t-> 검증 손실 감소, 모델 저장 완료.\n",
      "Epoch 31, Batch 0, Loss: 681.7167\n",
      "Epoch: 31 | Time: 0m 7s\n",
      "\tTrain Loss: 666.018 | Val. Loss: 501.449\n",
      "\t-> 검증 손실 감소, 모델 저장 완료.\n",
      "Epoch 32, Batch 0, Loss: 684.6953\n",
      "Epoch: 32 | Time: 0m 7s\n",
      "\tTrain Loss: 645.757 | Val. Loss: 488.264\n",
      "\t-> 검증 손실 감소, 모델 저장 완료.\n",
      "Epoch 33, Batch 0, Loss: 621.5747\n",
      "Epoch: 33 | Time: 0m 7s\n",
      "\tTrain Loss: 628.514 | Val. Loss: 475.759\n",
      "\t-> 검증 손실 감소, 모델 저장 완료.\n",
      "Epoch 34, Batch 0, Loss: 616.1016\n",
      "Epoch: 34 | Time: 0m 7s\n",
      "\tTrain Loss: 609.337 | Val. Loss: 460.829\n",
      "\t-> 검증 손실 감소, 모델 저장 완료.\n",
      "Epoch 35, Batch 0, Loss: 604.6709\n",
      "Epoch: 35 | Time: 0m 7s\n",
      "\tTrain Loss: 589.496 | Val. Loss: 447.746\n",
      "\t-> 검증 손실 감소, 모델 저장 완료.\n",
      "Epoch 36, Batch 0, Loss: 591.9081\n",
      "Epoch: 36 | Time: 0m 7s\n",
      "\tTrain Loss: 572.454 | Val. Loss: 431.867\n",
      "\t-> 검증 손실 감소, 모델 저장 완료.\n",
      "Epoch 37, Batch 0, Loss: 546.4990\n",
      "Epoch: 37 | Time: 0m 7s\n",
      "\tTrain Loss: 554.893 | Val. Loss: 418.982\n",
      "\t-> 검증 손실 감소, 모델 저장 완료.\n",
      "Epoch 38, Batch 0, Loss: 562.7734\n",
      "Epoch: 38 | Time: 0m 7s\n",
      "\tTrain Loss: 536.675 | Val. Loss: 400.878\n",
      "\t-> 검증 손실 감소, 모델 저장 완료.\n",
      "Epoch 39, Batch 0, Loss: 512.5559\n",
      "Epoch: 39 | Time: 0m 7s\n",
      "\tTrain Loss: 519.947 | Val. Loss: 386.933\n",
      "\t-> 검증 손실 감소, 모델 저장 완료.\n",
      "Epoch 40, Batch 0, Loss: 508.4684\n",
      "Epoch: 40 | Time: 0m 7s\n",
      "\tTrain Loss: 503.933 | Val. Loss: 373.551\n",
      "\t-> 검증 손실 감소, 모델 저장 완료.\n",
      "Epoch 41, Batch 0, Loss: 502.1525\n",
      "Epoch: 41 | Time: 0m 7s\n",
      "\tTrain Loss: 487.138 | Val. Loss: 357.182\n",
      "\t-> 검증 손실 감소, 모델 저장 완료.\n",
      "Epoch 42, Batch 0, Loss: 476.3045\n",
      "Epoch: 42 | Time: 0m 7s\n",
      "\tTrain Loss: 471.642 | Val. Loss: 343.074\n",
      "\t-> 검증 손실 감소, 모델 저장 완료.\n",
      "Epoch 43, Batch 0, Loss: 465.4581\n",
      "Epoch: 43 | Time: 0m 7s\n",
      "\tTrain Loss: 456.540 | Val. Loss: 333.533\n",
      "\t-> 검증 손실 감소, 모델 저장 완료.\n",
      "Epoch 44, Batch 0, Loss: 469.2610\n",
      "Epoch: 44 | Time: 0m 7s\n",
      "\tTrain Loss: 440.912 | Val. Loss: 313.894\n",
      "\t-> 검증 손실 감소, 모델 저장 완료.\n",
      "Epoch 45, Batch 0, Loss: 438.6973\n",
      "Epoch: 45 | Time: 0m 7s\n",
      "\tTrain Loss: 426.209 | Val. Loss: 303.426\n",
      "\t-> 검증 손실 감소, 모델 저장 완료.\n",
      "Epoch 46, Batch 0, Loss: 409.4625\n",
      "Epoch: 46 | Time: 0m 7s\n",
      "\tTrain Loss: 412.664 | Val. Loss: 294.446\n",
      "\t-> 검증 손실 감소, 모델 저장 완료.\n",
      "Epoch 47, Batch 0, Loss: 414.9710\n",
      "Epoch: 47 | Time: 0m 7s\n",
      "\tTrain Loss: 398.326 | Val. Loss: 281.073\n",
      "\t-> 검증 손실 감소, 모델 저장 완료.\n",
      "Epoch 48, Batch 0, Loss: 363.8871\n",
      "Epoch: 48 | Time: 0m 7s\n",
      "\tTrain Loss: 384.169 | Val. Loss: 272.388\n",
      "\t-> 검증 손실 감소, 모델 저장 완료.\n",
      "Epoch 49, Batch 0, Loss: 393.5669\n",
      "Epoch: 49 | Time: 0m 7s\n",
      "\tTrain Loss: 372.169 | Val. Loss: 261.636\n",
      "\t-> 검증 손실 감소, 모델 저장 완료.\n",
      "Epoch 50, Batch 0, Loss: 373.9259\n",
      "Epoch: 50 | Time: 0m 7s\n",
      "\tTrain Loss: 359.420 | Val. Loss: 251.013\n",
      "\t-> 검증 손실 감소, 모델 저장 완료.\n",
      "\n",
      "훈련 완료!\n",
      "슝=3\n"
     ]
    }
   ],
   "source": [
    "def train_epoch(model, dataloader, optimizer, criterion, device, epoch):\n",
    "   model.train()\n",
    "   epoch_loss = 0\n",
    "   \n",
    "   for batch_idx, (src, tgt) in enumerate(dataloader):\n",
    "       src, tgt = src.to(device), tgt.to(device)\n",
    "       \n",
    "       tgt_input = tgt[:, :-1]\n",
    "       tgt_output = tgt[:, 1:]\n",
    "       \n",
    "       enc_mask, dec_enc_mask, dec_mask = generate_masks(src, tgt_input)\n",
    "       \n",
    "       optimizer.zero_grad()\n",
    "       \n",
    "       model_output = model(src, tgt_input, enc_mask, dec_enc_mask, dec_mask)\n",
    "       \n",
    "       if isinstance(model_output, tuple):\n",
    "           output = model_output[0]\n",
    "       else:\n",
    "           output = model_output\n",
    "       \n",
    "       output = output.reshape(-1, output.shape[-1])\n",
    "       tgt_output = tgt_output.reshape(-1)\n",
    "       \n",
    "       loss = criterion(output, tgt_output)\n",
    "       loss.backward()\n",
    "       optimizer.step()\n",
    "       \n",
    "       epoch_loss += loss.item()\n",
    "       \n",
    "       if batch_idx % 100 == 0:\n",
    "           print(f'Epoch {epoch}, Batch {batch_idx}, Loss: {loss.item():.4f}')\n",
    "   \n",
    "   return epoch_loss / len(dataloader)\n",
    "\n",
    "def evaluate(model, dataloader, criterion, device, epoch):\n",
    "   model.eval()\n",
    "   epoch_loss = 0\n",
    "   \n",
    "   with torch.no_grad():\n",
    "       for src, tgt in dataloader:\n",
    "           src, tgt = src.to(device), tgt.to(device)\n",
    "           \n",
    "           tgt_input = tgt[:, :-1]\n",
    "           tgt_output = tgt[:, 1:]\n",
    "           \n",
    "           enc_mask, dec_enc_mask, dec_mask = generate_masks(src, tgt_input)\n",
    "           \n",
    "           model_output = model(src, tgt_input, enc_mask, dec_enc_mask, dec_mask)\n",
    "           \n",
    "           if isinstance(model_output, tuple):\n",
    "               output = model_output[0]\n",
    "           else:\n",
    "               output = model_output\n",
    "           \n",
    "           output = output.reshape(-1, output.shape[-1])\n",
    "           tgt_output = tgt_output.reshape(-1)\n",
    "           \n",
    "           loss = criterion(output, tgt_output)\n",
    "           epoch_loss += loss.item()\n",
    "   \n",
    "   return epoch_loss / len(dataloader)\n",
    "\n",
    "# 훈련 시작\n",
    "print(\"\\n훈련 시작\")\n",
    "best_val_loss = float('inf')\n",
    "early_stop_counter = 0\n",
    "patience = 3\n",
    "model_save_path = 'best_transformer.pt'\n",
    "train_losses, val_losses = [], []\n",
    "\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "   start_time = time.time()\n",
    "   \n",
    "   train_loss = train_epoch(transformer, train_dataloader, optimizer, criterion, device, epoch)\n",
    "   val_loss = evaluate(transformer, val_dataloader, criterion, device, epoch)\n",
    "   \n",
    "   end_time = time.time()\n",
    "   epoch_mins, epoch_secs = divmod(end_time - start_time, 60)\n",
    "   \n",
    "   train_losses.append(train_loss)\n",
    "   val_losses.append(val_loss)\n",
    "   \n",
    "   print(f'Epoch: {epoch:02} | Time: {int(epoch_mins)}m {int(epoch_secs)}s')\n",
    "   print(f'\\tTrain Loss: {train_loss:.3f} | Val. Loss: {val_loss:.3f}')\n",
    "   \n",
    "   if val_loss < best_val_loss:\n",
    "       best_val_loss = val_loss\n",
    "       torch.save(transformer.state_dict(), model_save_path)\n",
    "       early_stop_counter = 0\n",
    "       print(\"\\t-> 검증 손실 감소, 모델 저장 완료.\")\n",
    "   else:\n",
    "       early_stop_counter += 1\n",
    "       print(f\"\\t-> 검증 손실 증가. ({early_stop_counter}/{patience})\")\n",
    "       \n",
    "       if early_stop_counter >= patience:\n",
    "           print(\"Early stopping. 훈련을 중단합니다.\")\n",
    "           break\n",
    "\n",
    "print(\"\\n훈련 완료!\")\n",
    "print(\"슝=3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "db88ecf7-2f22-4592-95a2-134fc4bad16c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "현재 모델 백업 완료!\n",
      "\n",
      "추가 훈련 시작 (30 에포크)\n",
      "Epoch 1, Batch 0, Loss: 352.3074\n",
      "Epoch: 01 | Time: 0m 7s\n",
      "\tTrain Loss: 348.598 | Val. Loss: 242.862\n",
      "\t-> 검증 손실 감소, 모델 저장 완료.\n",
      "Epoch 2, Batch 0, Loss: 356.8356\n",
      "Epoch: 02 | Time: 0m 7s\n",
      "\tTrain Loss: 337.344 | Val. Loss: 234.976\n",
      "\t-> 검증 손실 감소, 모델 저장 완료.\n",
      "Epoch 3, Batch 0, Loss: 341.1765\n",
      "Epoch: 03 | Time: 0m 8s\n",
      "\tTrain Loss: 326.085 | Val. Loss: 229.625\n",
      "\t-> 검증 손실 감소, 모델 저장 완료.\n",
      "Epoch 4, Batch 0, Loss: 339.4576\n",
      "Epoch: 04 | Time: 0m 8s\n",
      "\tTrain Loss: 315.818 | Val. Loss: 221.009\n",
      "\t-> 검증 손실 감소, 모델 저장 완료.\n",
      "Epoch 5, Batch 0, Loss: 313.0205\n",
      "Epoch: 05 | Time: 0m 7s\n",
      "\tTrain Loss: 306.458 | Val. Loss: 212.396\n",
      "\t-> 검증 손실 감소, 모델 저장 완료.\n",
      "Epoch 6, Batch 0, Loss: 305.1356\n",
      "Epoch: 06 | Time: 0m 7s\n",
      "\tTrain Loss: 297.214 | Val. Loss: 205.636\n",
      "\t-> 검증 손실 감소, 모델 저장 완료.\n",
      "Epoch 7, Batch 0, Loss: 289.2971\n",
      "Epoch: 07 | Time: 0m 7s\n",
      "\tTrain Loss: 288.701 | Val. Loss: 198.955\n",
      "\t-> 검증 손실 감소, 모델 저장 완료.\n",
      "Epoch 8, Batch 0, Loss: 278.5552\n",
      "Epoch: 08 | Time: 0m 7s\n",
      "\tTrain Loss: 280.838 | Val. Loss: 192.011\n",
      "\t-> 검증 손실 감소, 모델 저장 완료.\n",
      "Epoch 9, Batch 0, Loss: 284.2710\n",
      "Epoch: 09 | Time: 0m 7s\n",
      "\tTrain Loss: 272.698 | Val. Loss: 190.147\n",
      "\t-> 검증 손실 감소, 모델 저장 완료.\n",
      "Epoch 10, Batch 0, Loss: 258.7901\n",
      "Epoch: 10 | Time: 0m 7s\n",
      "\tTrain Loss: 266.532 | Val. Loss: 184.152\n",
      "\t-> 검증 손실 감소, 모델 저장 완료.\n",
      "Epoch 11, Batch 0, Loss: 256.1694\n",
      "Epoch: 11 | Time: 0m 7s\n",
      "\tTrain Loss: 258.442 | Val. Loss: 179.028\n",
      "\t-> 검증 손실 감소, 모델 저장 완료.\n",
      "Epoch 12, Batch 0, Loss: 262.5402\n",
      "Epoch: 12 | Time: 0m 7s\n",
      "\tTrain Loss: 252.051 | Val. Loss: 173.967\n",
      "\t-> 검증 손실 감소, 모델 저장 완료.\n",
      "Epoch 13, Batch 0, Loss: 236.9684\n",
      "Epoch: 13 | Time: 0m 7s\n",
      "\tTrain Loss: 245.623 | Val. Loss: 167.720\n",
      "\t-> 검증 손실 감소, 모델 저장 완료.\n",
      "Epoch 14, Batch 0, Loss: 246.6664\n",
      "Epoch: 14 | Time: 0m 7s\n",
      "\tTrain Loss: 240.657 | Val. Loss: 165.028\n",
      "\t-> 검증 손실 감소, 모델 저장 완료.\n",
      "Epoch 15, Batch 0, Loss: 245.4480\n",
      "Epoch: 15 | Time: 0m 7s\n",
      "\tTrain Loss: 234.654 | Val. Loss: 159.331\n",
      "\t-> 검증 손실 감소, 모델 저장 완료.\n",
      "Epoch 16, Batch 0, Loss: 240.4548\n",
      "Epoch: 16 | Time: 0m 7s\n",
      "\tTrain Loss: 229.571 | Val. Loss: 157.507\n",
      "\t-> 검증 손실 감소, 모델 저장 완료.\n",
      "Epoch 17, Batch 0, Loss: 231.4863\n",
      "Epoch: 17 | Time: 0m 7s\n",
      "\tTrain Loss: 224.811 | Val. Loss: 152.001\n",
      "\t-> 검증 손실 감소, 모델 저장 완료.\n",
      "Epoch 18, Batch 0, Loss: 225.8301\n",
      "Epoch: 18 | Time: 0m 7s\n",
      "\tTrain Loss: 219.132 | Val. Loss: 149.230\n",
      "\t-> 검증 손실 감소, 모델 저장 완료.\n",
      "Epoch 19, Batch 0, Loss: 221.1110\n",
      "Epoch: 19 | Time: 0m 7s\n",
      "\tTrain Loss: 214.839 | Val. Loss: 144.937\n",
      "\t-> 검증 손실 감소, 모델 저장 완료.\n",
      "Epoch 20, Batch 0, Loss: 215.0995\n",
      "Epoch: 20 | Time: 0m 7s\n",
      "\tTrain Loss: 210.715 | Val. Loss: 143.254\n",
      "\t-> 검증 손실 감소, 모델 저장 완료.\n",
      "Epoch 21, Batch 0, Loss: 200.8218\n",
      "Epoch: 21 | Time: 0m 7s\n",
      "\tTrain Loss: 205.664 | Val. Loss: 137.526\n",
      "\t-> 검증 손실 감소, 모델 저장 완료.\n",
      "Epoch 22, Batch 0, Loss: 198.4845\n",
      "Epoch: 22 | Time: 0m 7s\n",
      "\tTrain Loss: 202.306 | Val. Loss: 134.524\n",
      "\t-> 검증 손실 감소, 모델 저장 완료.\n",
      "Epoch 23, Batch 0, Loss: 202.0428\n",
      "Epoch: 23 | Time: 0m 7s\n",
      "\tTrain Loss: 197.904 | Val. Loss: 132.179\n",
      "\t-> 검증 손실 감소, 모델 저장 완료.\n",
      "Epoch 24, Batch 0, Loss: 202.1869\n",
      "Epoch: 24 | Time: 0m 7s\n",
      "\tTrain Loss: 193.699 | Val. Loss: 128.260\n",
      "\t-> 검증 손실 감소, 모델 저장 완료.\n",
      "Epoch 25, Batch 0, Loss: 178.2144\n",
      "Epoch: 25 | Time: 0m 7s\n",
      "\tTrain Loss: 190.194 | Val. Loss: 125.104\n",
      "\t-> 검증 손실 감소, 모델 저장 완료.\n",
      "Epoch 26, Batch 0, Loss: 194.4342\n",
      "Epoch: 26 | Time: 0m 7s\n",
      "\tTrain Loss: 186.802 | Val. Loss: 122.568\n",
      "\t-> 검증 손실 감소, 모델 저장 완료.\n",
      "Epoch 27, Batch 0, Loss: 178.7759\n",
      "Epoch: 27 | Time: 0m 7s\n",
      "\tTrain Loss: 183.619 | Val. Loss: 120.351\n",
      "\t-> 검증 손실 감소, 모델 저장 완료.\n",
      "Epoch 28, Batch 0, Loss: 177.6021\n",
      "Epoch: 28 | Time: 0m 7s\n",
      "\tTrain Loss: 180.164 | Val. Loss: 116.735\n",
      "\t-> 검증 손실 감소, 모델 저장 완료.\n",
      "Epoch 29, Batch 0, Loss: 176.9153\n",
      "Epoch: 29 | Time: 0m 7s\n",
      "\tTrain Loss: 176.223 | Val. Loss: 112.768\n",
      "\t-> 검증 손실 감소, 모델 저장 완료.\n",
      "Epoch 30, Batch 0, Loss: 180.7927\n",
      "Epoch: 30 | Time: 0m 7s\n",
      "\tTrain Loss: 173.287 | Val. Loss: 112.937\n",
      "\t-> 검증 손실 증가. (1/3)\n",
      "\n",
      "추가 훈련 완료!\n",
      "슝=3\n"
     ]
    }
   ],
   "source": [
    "# 현재 모델 상태 저장 (백업용)\n",
    "torch.save({\n",
    "    'model_state_dict': transformer.state_dict(),\n",
    "    'optimizer_state_dict': optimizer.state_dict(),\n",
    "    'train_losses': train_losses,\n",
    "    'val_losses': val_losses\n",
    "}, 'transformer_loss300_backup.pt')\n",
    "\n",
    "print(\"현재 모델 백업 완료!\")\n",
    "\n",
    "# 추가 훈련 (더 많은 에포크)\n",
    "EPOCHS = 30  # 추가로 30 에포크 더\n",
    "print(f\"\\n추가 훈련 시작 ({EPOCHS} 에포크)\")\n",
    "\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    start_time = time.time()\n",
    "    \n",
    "    train_loss = train_epoch(transformer, train_dataloader, optimizer, criterion, device, epoch)\n",
    "    val_loss = evaluate(transformer, val_dataloader, criterion, device, epoch)\n",
    "    \n",
    "    end_time = time.time()\n",
    "    epoch_mins, epoch_secs = divmod(end_time - start_time, 60)\n",
    "    \n",
    "    train_losses.append(train_loss)\n",
    "    val_losses.append(val_loss)\n",
    "    \n",
    "    print(f'Epoch: {epoch:02} | Time: {int(epoch_mins)}m {int(epoch_secs)}s')\n",
    "    print(f'\\tTrain Loss: {train_loss:.3f} | Val. Loss: {val_loss:.3f}')\n",
    "    \n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        torch.save(transformer.state_dict(), model_save_path)\n",
    "        early_stop_counter = 0\n",
    "        print(\"\\t-> 검증 손실 감소, 모델 저장 완료.\")\n",
    "    else:\n",
    "        early_stop_counter += 1\n",
    "        print(f\"\\t-> 검증 손실 증가. ({early_stop_counter}/{patience})\")\n",
    "        \n",
    "        if early_stop_counter >= patience:\n",
    "            print(\"Early stopping. 훈련을 중단합니다.\")\n",
    "            break\n",
    "\n",
    "print(\"\\n추가 훈련 완료!\")\n",
    "print(\"슝=3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "1f54aa4d-6a8d-487a-8b4a-0b0fba1a044a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# 예문\n",
      "1. 지루하다, 놀러가고 싶어.\n",
      "2. 오늘 일찍 일어났더니 피곤하다.\n",
      "3. 간만에 여자친구랑 데이트 하기로 했어.\n",
      "4. 집에 있는다는 소리야.\n",
      "---\n",
      "# 제출\n",
      "Translations\n",
      "> 1. 잠깐 쉬 어도 돼요 . <end>\n",
      "> 2. 맛난 거 드세요 . <end>\n",
      "> 3. 떨리 겠 죠 . <end>\n",
      "> 4. 좋 아 하 면 그럴 수 있 어요 . <end>\n",
      "Hyperparameters\n",
      "> n_layers: 1\n",
      "> d_model: 368\n",
      "> n_heads: 8\n",
      "> d_ff: 1024\n",
      "> dropout: 0.2\n",
      "Training Parameters\n",
      "> Warmup Steps: 1000\n",
      "> Batch Size: 64\n",
      "> Epoch At: 10\n",
      "슝=3\n"
     ]
    }
   ],
   "source": [
    "# 현재 상태 저장\n",
    "torch.save({\n",
    "    'model_state_dict': transformer.state_dict(),\n",
    "    'train_loss': 173.287,\n",
    "    'val_loss': 112.937\n",
    "}, 'transformer_final_loss112.pt')\n",
    "\n",
    "# 예측 테스트 실행\n",
    "full_evaluation_report()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "238b7dcd-f255-46a4-8111-92181b6e64e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# 예문\n",
      "1. 지루하다, 놀러가고 싶어.\n",
      "2. 오늘 일찍 일어났더니 피곤하다.\n",
      "3. 간만에 여자친구랑 데이트 하기로 했어.\n",
      "4. 집에 있는다는 소리야.\n",
      "---\n",
      "# 제출\n",
      "Translations\n",
      "> 1. 잠깐 쉬 어도 돼요 . <end>\n",
      "> 2. 맛난 거 드세요 . <end>\n",
      "> 3. 떨리 겠 죠 . <end>\n",
      "> 4. 좋 아 하 면 그럴 수 있 어요 . <end>\n",
      "Hyperparameters\n",
      "> n_layers: 1\n",
      "> d_model: 368\n",
      "> n_heads: 8\n",
      "> d_ff: 1024\n",
      "> dropout: 0.2\n",
      "Training Parameters\n",
      "> Warmup Steps: 1000\n",
      "> Batch Size: 64\n",
      "> Epoch At: 10\n",
      "슝=3\n"
     ]
    }
   ],
   "source": [
    "# 현재 상태 저장\n",
    "torch.save({\n",
    "    'model_state_dict': transformer.state_dict(),\n",
    "    'train_loss': 173.287,\n",
    "    'val_loss': 112.937\n",
    "}, 'transformer_final_loss112.pt')\n",
    "\n",
    "# 예측 테스트 실행\n",
    "full_evaluation_report()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "443c89f2-b88f-4dcb-a72d-f30e2d0f2443",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. 질문: 집나가도 갈 데가 없어\n",
      "   정답: 선생님이나 기관에 연락해보세요.\n",
      "   예측: 나올까 무슨방비네요.\n",
      "------------------------------\n",
      "2. 질문: 비가 오니 이 노래가 생각 나네\n",
      "   정답: 비와 당신.\n",
      "   예측: 나올까를 울적하.\n",
      "------------------------------\n",
      "3. 질문: 썸 타고 있는데 주변에 말해?\n",
      "   정답: 굳이 말할 필요는 없는 거 같아요.\n",
      "   예측: 단순하을 마음은이라니요.\n",
      "------------------------------\n",
      "4. 질문: 섬유유연제 향 좋은거 사야겠지\n",
      "   정답: 향이 많은 걸 말해주죠.\n",
      "   예측: 싫어네요.\n",
      "------------------------------\n",
      "5. 질문: 6개월 가량의 이별과 재회의 끝\n",
      "   정답: 아쉬운 이야기네요.\n",
      "   예측: 용 볼 합니다.\n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "def predict_single(model, tokenizer, question):\n",
    "   model.eval()\n",
    "   \n",
    "   # 토큰화 및 패딩\n",
    "   q_tokens = tokenizer.encode_as_ids(question)[:50]\n",
    "   q_tokens += [0] * (50 - len(q_tokens))\n",
    "   \n",
    "   src = torch.tensor([q_tokens]).to(device)\n",
    "   tgt = torch.tensor([[1]]).to(device)  # 시작 토큰\n",
    "   \n",
    "   with torch.no_grad():\n",
    "       for _ in range(30):\n",
    "           enc_mask, dec_enc_mask, dec_mask = generate_masks(src, tgt)\n",
    "           output = model(src, tgt, enc_mask, dec_enc_mask, dec_mask)\n",
    "           \n",
    "           if isinstance(output, tuple):\n",
    "               output = output[0]\n",
    "           \n",
    "           next_token = output[0, -1].argmax().item()\n",
    "           if next_token == 2:  # 종료 토큰\n",
    "               break\n",
    "           tgt = torch.cat([tgt, torch.tensor([[next_token]]).to(device)], dim=1)\n",
    "   \n",
    "   return tokenizer.decode_ids(tgt[0].tolist()[1:])\n",
    "\n",
    "# 랜덤 테스트\n",
    "random_indices = random.sample(range(len(questions)), 5)\n",
    "\n",
    "for i, idx in enumerate(random_indices, 1):\n",
    "   q = questions[idx]\n",
    "   a = answers[idx]\n",
    "   pred = predict_single(transformer, tokenizer, q)\n",
    "   \n",
    "   print(f\"{i}. 질문: {q}\")\n",
    "   print(f\"   정답: {a}\")\n",
    "   print(f\"   예측: {pred}\")\n",
    "   print(\"-\" * 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "064cd107-aae2-406a-8fd7-d034f40801dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# 랜덤 질문 테스트\n",
      "==================================================\n",
      "\n",
      "1. 질문: 나 둘다 좋아하는 것 같애\n",
      "   정답: 마음의 방이 두 개 있나봐요.\n",
      "   예측: 명절그램 거예요.\n",
      "------------------------------\n",
      "\n",
      "2. 질문: 지금 가장 힘든 거는\n",
      "   정답: 자신일 것입니다.\n",
      "   예측: 또 다시 결혼하기은이라니요.\n",
      "------------------------------\n",
      "\n",
      "3. 질문: 동거하다가 헤어질까봐 걱정이야\n",
      "   정답: 걱정하면 아무것도 못해요\n",
      "   예측: 나올까인지밖요.\n",
      "------------------------------\n",
      "\n",
      "4. 질문: 먹을 게 일도 없어\n",
      "   정답: 장 보러 갈 타이밍이네요.\n",
      "   예측: 다가네요.\n",
      "------------------------------\n",
      "\n",
      "5. 질문: 뉴스는 역시 지루해\n",
      "   정답: 흥미를 가져보세요.\n",
      "   예측: 단순하을 마음은 괜찮아 맞은세요.\n",
      "------------------------------\n",
      "\n",
      "6. 질문: 무슨 매주 결혼식이야\n",
      "   정답: 인맥이 넓으신가봐요.\n",
      "   예측: 저도넌도름죠.\n",
      "------------------------------\n",
      "\n",
      "7. 질문: 집 아예 샀어\n",
      "   정답: 내 집 마련 축하드려요.\n",
      "   예측: 한났네고 있어요.\n",
      "------------------------------\n",
      "슝=3\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "def test_random_questions(model, tokenizer, num_samples=5):\n",
    "    \"\"\"랜덤 질문으로 챗봇 테스트\"\"\"\n",
    "    \n",
    "    # 훈련 데이터에서 랜덤 질문 선택\n",
    "    random_indices = random.sample(range(len(questions)), num_samples)\n",
    "    \n",
    "    print(\"# 랜덤 질문 테스트\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    for i, idx in enumerate(random_indices, 1):\n",
    "        original_question = questions[idx]\n",
    "        original_answer = answers[idx]\n",
    "        \n",
    "        print(f\"\\n{i}. 질문: {original_question}\")\n",
    "        print(f\"   정답: {original_answer}\")\n",
    "        \n",
    "        try:\n",
    "            # 모델 예측\n",
    "            predicted_answer = predict_single(model, tokenizer, original_question)\n",
    "            print(f\"   예측: {predicted_answer}\")\n",
    "        except Exception as e:\n",
    "            print(f\"   예측: [실패: {str(e)}]\")\n",
    "        \n",
    "        print(\"-\" * 30)\n",
    "\n",
    "# 실행\n",
    "test_random_questions(transformer, tokenizer, num_samples=7)\n",
    "print(\"슝=3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "15b57a7e-8834-4c43-a5f2-19a6de83303f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "원본 데이터로 학습 준비\n",
      "원본 데이터: 11823개\n",
      "토큰화 완료: 11823개\n",
      "데이터셋 크기: torch.Size([11823, 50])\n",
      "\n",
      "원본 데이터 학습 시작\n",
      "Epoch 1, Batch 0, Loss: 3270.3552\n",
      "Epoch: 01 | Train Loss: 2383.629\n",
      "Epoch 2, Batch 0, Loss: 1927.5568\n",
      "Epoch: 02 | Train Loss: 1594.460\n",
      "Epoch 3, Batch 0, Loss: 1366.5895\n",
      "Epoch: 03 | Train Loss: 1316.358\n",
      "Epoch 4, Batch 0, Loss: 1215.2081\n",
      "Epoch: 04 | Train Loss: 1210.763\n",
      "Epoch 5, Batch 0, Loss: 1140.7401\n",
      "Epoch: 05 | Train Loss: 1139.874\n",
      "Epoch 6, Batch 0, Loss: 1136.1343\n",
      "Epoch: 06 | Train Loss: 1084.573\n",
      "Epoch 7, Batch 0, Loss: 1118.2805\n",
      "Epoch: 07 | Train Loss: 1037.138\n",
      "Epoch 8, Batch 0, Loss: 1046.7931\n",
      "Epoch: 08 | Train Loss: 993.556\n",
      "Epoch 9, Batch 0, Loss: 994.2520\n",
      "Epoch: 09 | Train Loss: 954.346\n",
      "Epoch 10, Batch 0, Loss: 943.1646\n",
      "Epoch: 10 | Train Loss: 917.952\n",
      "Epoch 11, Batch 0, Loss: 924.1323\n",
      "Epoch: 11 | Train Loss: 881.817\n",
      "Epoch 12, Batch 0, Loss: 891.4734\n",
      "Epoch: 12 | Train Loss: 850.255\n",
      "Epoch 13, Batch 0, Loss: 840.0701\n",
      "Epoch: 13 | Train Loss: 819.386\n",
      "Epoch 14, Batch 0, Loss: 846.3079\n",
      "Epoch: 14 | Train Loss: 793.363\n",
      "Epoch 15, Batch 0, Loss: 728.2281\n",
      "Epoch: 15 | Train Loss: 764.987\n",
      "Epoch 16, Batch 0, Loss: 750.0800\n",
      "Epoch: 16 | Train Loss: 739.190\n",
      "Epoch 17, Batch 0, Loss: 707.6422\n",
      "Epoch: 17 | Train Loss: 716.972\n",
      "Epoch 18, Batch 0, Loss: 702.2712\n",
      "Epoch: 18 | Train Loss: 694.787\n",
      "Epoch 19, Batch 0, Loss: 661.3498\n",
      "Epoch: 19 | Train Loss: 673.404\n",
      "Epoch 20, Batch 0, Loss: 649.2861\n",
      "Epoch: 20 | Train Loss: 653.207\n",
      "학습 완료!\n"
     ]
    }
   ],
   "source": [
    "# 원본 데이터만 사용 (증강 제거)\n",
    "print(\"원본 데이터로 학습 준비\")\n",
    "print(f\"원본 데이터: {len(questions)}개\")\n",
    "\n",
    "# 원본 데이터 토큰화\n",
    "original_que_corpus = []\n",
    "original_ans_corpus = []\n",
    "\n",
    "for q, a in zip(questions, answers):\n",
    "    q_tokens = tokenizer.encode_as_ids(q)\n",
    "    a_tokens = [1] + tokenizer.encode_as_ids(a) + [2]  # 시작/끝 토큰 추가\n",
    "    \n",
    "    if len(q_tokens) <= 40 and len(a_tokens) <= 40:\n",
    "        original_que_corpus.append(q_tokens)\n",
    "        original_ans_corpus.append(a_tokens)\n",
    "\n",
    "print(f\"토큰화 완료: {len(original_que_corpus)}개\")\n",
    "\n",
    "# 패딩\n",
    "original_enc_ndarray = pad_sequences_custom(original_que_corpus, max_len=MAX_LEN, pad_value=0)\n",
    "original_dec_ndarray = pad_sequences_custom(original_ans_corpus, max_len=MAX_LEN, pad_value=0)\n",
    "\n",
    "# 새 데이터로더\n",
    "original_dataset = TensorDataset(original_enc_ndarray, original_dec_ndarray)\n",
    "original_dataloader = DataLoader(original_dataset, batch_size=128, shuffle=True)\n",
    "\n",
    "print(f\"데이터셋 크기: {original_enc_ndarray.shape}\")\n",
    "\n",
    "# 새 모델로 학습\n",
    "transformer_original = Transformer(\n",
    "    n_layers=3, d_model=256, n_heads=8, d_ff=512,\n",
    "    src_vocab_size=VOCAB_SIZE, tgt_vocab_size=VOCAB_SIZE,\n",
    "    pos_len=200, dropout=0.2, shared_fc=True, shared_emb=True\n",
    ").to(device)\n",
    "\n",
    "optimizer_original = torch.optim.Adam(transformer_original.parameters(), lr=0.0001)\n",
    "\n",
    "# 학습 시작\n",
    "print(\"\\n원본 데이터 학습 시작\")\n",
    "for epoch in range(1, 21):\n",
    "    train_loss = train_epoch(transformer_original, original_dataloader, optimizer_original, criterion, device, epoch)\n",
    "    print(f'Epoch: {epoch:02} | Train Loss: {train_loss:.3f}')\n",
    "\n",
    "print(\"학습 완료!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "ee1e30c5-0195-49d6-9e96-085088a330e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== 모델 성능 비교 ===\n",
      "질문: 딱 알아봤는데\n",
      "정답: 여전하던가요?\n",
      "증강 모델: 랜선 구썸남이 귀찮은.\n",
      "원본 모델: \n"
     ]
    }
   ],
   "source": [
    "# 두 모델 성능 비교\n",
    "print(\"=== 모델 성능 비교 ===\")\n",
    "random_idx = random.randint(0, len(questions)-1)\n",
    "test_q = questions[random_idx]\n",
    "test_a = answers[random_idx]\n",
    "\n",
    "print(f\"질문: {test_q}\")\n",
    "print(f\"정답: {test_a}\")\n",
    "\n",
    "# 증강 모델 예측\n",
    "pred1 = predict_single(transformer, tokenizer, test_q)\n",
    "print(f\"증강 모델: {pred1}\")\n",
    "\n",
    "# 원본 모델 예측  \n",
    "pred2 = predict_single(transformer_original, tokenizer, test_q)\n",
    "print(f\"원본 모델: {pred2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "e6158d7f-784d-4063-b0d5-e6558ae11d16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== 모델 성능 비교 ===\n",
      "질문: 할 줄 아는거!\n",
      "정답: 많은걸 하고 싶은데 아직 못하는게 많아요.\n",
      "증강 모델: 것도 중요한도 진심으로 오래 해보세요.\n",
      "원본 모델: \n"
     ]
    }
   ],
   "source": [
    "# 두 모델 성능 비교\n",
    "print(\"=== 모델 성능 비교 ===\")\n",
    "random_idx = random.randint(0, len(questions)-1)\n",
    "test_q = questions[random_idx]\n",
    "test_a = answers[random_idx]\n",
    "\n",
    "print(f\"질문: {test_q}\")\n",
    "print(f\"정답: {test_a}\")\n",
    "\n",
    "# 증강 모델 예측\n",
    "pred1 = predict_single(transformer, tokenizer, test_q)\n",
    "print(f\"증강 모델: {pred1}\")\n",
    "\n",
    "# 원본 모델 예측  \n",
    "pred2 = predict_single(transformer_original, tokenizer, test_q)\n",
    "print(f\"원본 모델: {pred2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "cf3db865-ff8b-4f7c-b88f-38e3eb9fbbf2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== 모델 성능 비교 ===\n",
      "질문: 나를 좋아하는 사람을 만나는 건 축복일거야.\n",
      "정답: 서로 사랑하는 건 기적이죠.\n",
      "증강 모델: 단순하을 마음은이라니가 누나.\n",
      "원본 모델: \n"
     ]
    }
   ],
   "source": [
    "# 두 모델 성능 비교\n",
    "print(\"=== 모델 성능 비교 ===\")\n",
    "random_idx = random.randint(0, len(questions)-1)\n",
    "test_q = questions[random_idx]\n",
    "test_a = answers[random_idx]\n",
    "\n",
    "print(f\"질문: {test_q}\")\n",
    "print(f\"정답: {test_a}\")\n",
    "\n",
    "# 증강 모델 예측\n",
    "pred1 = predict_single(transformer, tokenizer, test_q)\n",
    "print(f\"증강 모델: {pred1}\")\n",
    "\n",
    "# 원본 모델 예측  \n",
    "pred2 = predict_single(transformer_original, tokenizer, test_q)\n",
    "print(f\"원본 모델: {pred2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "913cfddd-a17c-4825-acc3-80fd6ae2f6f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21, Batch 0, Loss: 611.7399\n",
      "Epoch: 21 | Train Loss: 632.132\n",
      "Epoch 22, Batch 0, Loss: 599.3767\n",
      "Epoch: 22 | Train Loss: 614.755\n",
      "Epoch 23, Batch 0, Loss: 611.4449\n",
      "Epoch: 23 | Train Loss: 593.914\n",
      "Epoch 24, Batch 0, Loss: 585.0062\n",
      "Epoch: 24 | Train Loss: 575.738\n",
      "Epoch 25, Batch 0, Loss: 544.5309\n",
      "Epoch: 25 | Train Loss: 558.846\n",
      "Epoch 26, Batch 0, Loss: 530.2510\n",
      "Epoch: 26 | Train Loss: 540.296\n",
      "Epoch 27, Batch 0, Loss: 535.3284\n",
      "Epoch: 27 | Train Loss: 523.848\n",
      "Epoch 28, Batch 0, Loss: 533.0620\n",
      "Epoch: 28 | Train Loss: 507.021\n",
      "Epoch 29, Batch 0, Loss: 490.6982\n",
      "Epoch: 29 | Train Loss: 489.746\n",
      "Epoch 30, Batch 0, Loss: 489.9977\n",
      "Epoch: 30 | Train Loss: 474.935\n",
      "Epoch 31, Batch 0, Loss: 445.6504\n",
      "Epoch: 31 | Train Loss: 458.030\n",
      "Epoch 32, Batch 0, Loss: 481.8080\n",
      "Epoch: 32 | Train Loss: 442.879\n",
      "Epoch 33, Batch 0, Loss: 433.1244\n",
      "Epoch: 33 | Train Loss: 428.662\n",
      "Epoch 34, Batch 0, Loss: 402.5423\n",
      "Epoch: 34 | Train Loss: 414.925\n",
      "Epoch 35, Batch 0, Loss: 421.4415\n",
      "Epoch: 35 | Train Loss: 400.803\n",
      "Epoch 36, Batch 0, Loss: 391.6187\n",
      "Epoch: 36 | Train Loss: 385.497\n",
      "Epoch 37, Batch 0, Loss: 376.2094\n",
      "Epoch: 37 | Train Loss: 373.934\n",
      "Epoch 38, Batch 0, Loss: 369.4039\n",
      "Epoch: 38 | Train Loss: 359.161\n",
      "Epoch 39, Batch 0, Loss: 343.5360\n",
      "Epoch: 39 | Train Loss: 347.824\n",
      "Epoch 40, Batch 0, Loss: 350.3675\n",
      "Epoch: 40 | Train Loss: 334.648\n",
      "Epoch 41, Batch 0, Loss: 328.5777\n",
      "Epoch: 41 | Train Loss: 322.896\n",
      "Epoch 42, Batch 0, Loss: 328.7926\n",
      "Epoch: 42 | Train Loss: 311.477\n",
      "Epoch 43, Batch 0, Loss: 285.1488\n",
      "Epoch: 43 | Train Loss: 300.731\n",
      "Epoch 44, Batch 0, Loss: 322.5168\n",
      "Epoch: 44 | Train Loss: 289.843\n",
      "Epoch 45, Batch 0, Loss: 287.1198\n",
      "Epoch: 45 | Train Loss: 280.877\n",
      "Epoch 46, Batch 0, Loss: 293.2443\n",
      "Epoch: 46 | Train Loss: 270.320\n",
      "Epoch 47, Batch 0, Loss: 245.0713\n",
      "Epoch: 47 | Train Loss: 261.773\n",
      "Epoch 48, Batch 0, Loss: 273.0190\n",
      "Epoch: 48 | Train Loss: 252.798\n",
      "Epoch 49, Batch 0, Loss: 254.4825\n",
      "Epoch: 49 | Train Loss: 244.832\n",
      "Epoch 50, Batch 0, Loss: 248.2002\n",
      "Epoch: 50 | Train Loss: 237.027\n"
     ]
    }
   ],
   "source": [
    "# 1. 원본 데이터만으로 더 오래 학습\n",
    "EPOCHS = 50\n",
    "for epoch in range(21, EPOCHS + 1):\n",
    "    train_loss = train_epoch(transformer_original, original_dataloader, optimizer_original, criterion, device, epoch)\n",
    "    print(f'Epoch: {epoch:02} | Train Loss: {train_loss:.3f}')\n",
    "\n",
    "# 2. 또는 학습률 조정\n",
    "optimizer_original = torch.optim.Adam(transformer_original.parameters(), lr=0.0005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "52bb88c6-9c71-449b-ab14-6baa932a580d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "질문: 의욕이 없어\n",
      "정답: 가벼운 산책을 해보세요.\n",
      "예측: \n",
      "------------------------------\n",
      "질문: 한잔 했습니다\n",
      "정답: 한 잔 하기 좋은 날이네요.\n",
      "예측: \n",
      "------------------------------\n",
      "질문: 연락하지 말라고 했어\n",
      "정답: 때로는 단호한 것도 필요해요.\n",
      "예측: \n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "# 몇 개 질문으로 응답 품질 확인\n",
    "for i in range(3):\n",
    "    idx = random.randint(0, len(questions)-1)\n",
    "    q = questions[idx]\n",
    "    a = answers[idx]\n",
    "    pred = predict_single(transformer_original, tokenizer, q)\n",
    "    \n",
    "    print(f\"질문: {q}\")\n",
    "    print(f\"정답: {a}\")\n",
    "    print(f\"예측: {pred}\")\n",
    "    print(\"-\" * 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "7f7d481b-d40a-4c4e-b6ef-c89f0393cae8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "입력 토큰: [1, 3160, 2, 0, 0, 0, 0, 0, 0, 0]...\n",
      "시작 토큰 ID: 1\n",
      "스텝 0: 예측 토큰 1\n",
      "스텝 1: 예측 토큰 1\n",
      "스텝 2: 예측 토큰 1\n",
      "스텝 3: 예측 토큰 1\n",
      "스텝 4: 예측 토큰 1\n",
      "최종 결과: \n"
     ]
    }
   ],
   "source": [
    "# 예측 과정 디버깅\n",
    "def debug_predict(model, tokenizer, question):\n",
    "    model.eval()\n",
    "    q_tokens = tokenizer.encode_as_ids(question)[:50]\n",
    "    q_tokens += [0] * (50 - len(q_tokens))\n",
    "    \n",
    "    src = torch.tensor([q_tokens]).to(device)\n",
    "    tgt = torch.tensor([[1]]).to(device)  # 시작 토큰\n",
    "    \n",
    "    print(f\"입력 토큰: {q_tokens[:10]}...\")\n",
    "    print(f\"시작 토큰 ID: 1\")\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for step in range(5):  # 처음 5스텝만 확인\n",
    "            enc_mask, dec_enc_mask, dec_mask = generate_masks(src, tgt)\n",
    "            output = model(src, tgt, enc_mask, dec_enc_mask, dec_mask)\n",
    "            \n",
    "            if isinstance(output, tuple):\n",
    "                output = output[0]\n",
    "            \n",
    "            next_token = output[0, -1].argmax().item()\n",
    "            print(f\"스텝 {step}: 예측 토큰 {next_token}\")\n",
    "            \n",
    "            if next_token == 2:  # 종료 토큰\n",
    "                print(\"종료 토큰 생성됨\")\n",
    "                break\n",
    "            tgt = torch.cat([tgt, torch.tensor([[next_token]]).to(device)], dim=1)\n",
    "    \n",
    "    return tokenizer.decode_ids(tgt[0].tolist()[1:])\n",
    "\n",
    "# 테스트\n",
    "test_q = \"안녕하세요\"\n",
    "result = debug_predict(transformer_original, tokenizer, test_q)\n",
    "print(f\"최종 결과: {result}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "1504b2d0-fdd1-44ac-94e4-3177800e01b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "안정적인 학습률로 재시작\n",
      "Epoch 1, Batch 0, Loss: 1276.8003\n",
      "Epoch 1, Batch 100, Loss: 895.1368\n",
      "Epoch 1, Batch 200, Loss: 893.0211\n",
      "Epoch 1, Batch 300, Loss: 734.6543\n",
      "Epoch: 01 | Train Loss: 897.526\n",
      "Epoch 2, Batch 0, Loss: 881.9675\n",
      "Epoch 2, Batch 100, Loss: 926.4646\n",
      "Epoch 2, Batch 200, Loss: 807.7474\n",
      "Epoch 2, Batch 300, Loss: 756.7810\n",
      "Epoch: 02 | Train Loss: 781.712\n",
      "손실 급등 감지, 학습률 추가 감소\n",
      "Epoch 3, Batch 0, Loss: 758.0356\n",
      "Epoch 3, Batch 100, Loss: 732.5965\n",
      "Epoch 3, Batch 200, Loss: 642.7484\n",
      "Epoch 3, Batch 300, Loss: 701.4048\n",
      "Epoch: 03 | Train Loss: 713.104\n",
      "손실 급등 감지, 학습률 추가 감소\n",
      "Epoch 4, Batch 0, Loss: 634.2913\n",
      "Epoch 4, Batch 100, Loss: 676.2718\n",
      "Epoch 4, Batch 200, Loss: 709.1272\n",
      "Epoch 4, Batch 300, Loss: 615.0367\n",
      "Epoch: 04 | Train Loss: 682.388\n",
      "손실 급등 감지, 학습률 추가 감소\n",
      "Epoch 5, Batch 0, Loss: 734.1345\n",
      "Epoch 5, Batch 100, Loss: 775.0132\n",
      "Epoch 5, Batch 200, Loss: 661.0947\n",
      "Epoch 5, Batch 300, Loss: 692.5763\n",
      "Epoch: 05 | Train Loss: 667.813\n",
      "손실 급등 감지, 학습률 추가 감소\n",
      "Epoch 6, Batch 0, Loss: 713.5786\n",
      "Epoch 6, Batch 100, Loss: 653.7986\n",
      "Epoch 6, Batch 200, Loss: 662.4658\n",
      "Epoch 6, Batch 300, Loss: 761.9327\n",
      "Epoch: 06 | Train Loss: 662.537\n",
      "손실 급등 감지, 학습률 추가 감소\n",
      "Epoch 7, Batch 0, Loss: 618.4557\n",
      "Epoch 7, Batch 100, Loss: 654.5742\n",
      "Epoch 7, Batch 200, Loss: 674.5513\n",
      "Epoch 7, Batch 300, Loss: 596.4640\n",
      "Epoch: 07 | Train Loss: 657.806\n",
      "손실 급등 감지, 학습률 추가 감소\n",
      "Epoch 8, Batch 0, Loss: 660.8252\n",
      "Epoch 8, Batch 100, Loss: 635.3217\n",
      "Epoch 8, Batch 200, Loss: 675.2317\n",
      "Epoch 8, Batch 300, Loss: 646.2101\n",
      "Epoch: 08 | Train Loss: 656.414\n",
      "손실 급등 감지, 학습률 추가 감소\n",
      "Epoch 9, Batch 0, Loss: 604.3841\n",
      "Epoch 9, Batch 100, Loss: 642.1884\n",
      "Epoch 9, Batch 200, Loss: 751.3860\n",
      "Epoch 9, Batch 300, Loss: 668.1661\n",
      "Epoch: 09 | Train Loss: 656.088\n",
      "손실 급등 감지, 학습률 추가 감소\n",
      "Epoch 10, Batch 0, Loss: 587.5112\n",
      "Epoch 10, Batch 100, Loss: 559.6469\n",
      "Epoch 10, Batch 200, Loss: 687.7127\n",
      "Epoch 10, Batch 300, Loss: 614.9417\n",
      "Epoch: 10 | Train Loss: 654.627\n",
      "손실 급등 감지, 학습률 추가 감소\n",
      "Epoch 11, Batch 0, Loss: 724.7927\n",
      "Epoch 11, Batch 100, Loss: 610.2006\n",
      "Epoch 11, Batch 200, Loss: 652.2677\n",
      "Epoch 11, Batch 300, Loss: 612.0444\n",
      "Epoch: 11 | Train Loss: 655.564\n",
      "손실 급등 감지, 학습률 추가 감소\n",
      "Epoch 12, Batch 0, Loss: 632.4920\n",
      "Epoch 12, Batch 100, Loss: 663.5336\n",
      "Epoch 12, Batch 200, Loss: 658.2722\n",
      "Epoch 12, Batch 300, Loss: 597.6003\n",
      "Epoch: 12 | Train Loss: 652.917\n",
      "손실 급등 감지, 학습률 추가 감소\n",
      "Epoch 13, Batch 0, Loss: 630.4277\n",
      "Epoch 13, Batch 100, Loss: 665.5826\n",
      "Epoch 13, Batch 200, Loss: 621.2293\n",
      "Epoch 13, Batch 300, Loss: 596.5437\n",
      "Epoch: 13 | Train Loss: 653.132\n",
      "손실 급등 감지, 학습률 추가 감소\n",
      "Epoch 14, Batch 0, Loss: 713.8499\n",
      "Epoch 14, Batch 100, Loss: 645.4888\n",
      "Epoch 14, Batch 200, Loss: 668.1821\n",
      "Epoch 14, Batch 300, Loss: 684.0912\n",
      "Epoch: 14 | Train Loss: 655.280\n",
      "손실 급등 감지, 학습률 추가 감소\n",
      "Epoch 15, Batch 0, Loss: 672.6001\n",
      "Epoch 15, Batch 100, Loss: 622.4624\n",
      "Epoch 15, Batch 200, Loss: 543.3329\n",
      "Epoch 15, Batch 300, Loss: 648.3096\n",
      "Epoch: 15 | Train Loss: 653.515\n",
      "손실 급등 감지, 학습률 추가 감소\n",
      "Epoch 16, Batch 0, Loss: 586.1867\n",
      "Epoch 16, Batch 100, Loss: 738.3297\n",
      "Epoch 16, Batch 200, Loss: 621.2389\n",
      "Epoch 16, Batch 300, Loss: 674.8033\n",
      "Epoch: 16 | Train Loss: 654.577\n",
      "손실 급등 감지, 학습률 추가 감소\n",
      "Epoch 17, Batch 0, Loss: 657.0184\n",
      "Epoch 17, Batch 100, Loss: 665.3692\n",
      "Epoch 17, Batch 200, Loss: 698.3150\n",
      "Epoch 17, Batch 300, Loss: 672.6771\n",
      "Epoch: 17 | Train Loss: 653.610\n",
      "손실 급등 감지, 학습률 추가 감소\n",
      "Epoch 18, Batch 0, Loss: 667.8214\n",
      "Epoch 18, Batch 100, Loss: 684.3364\n",
      "Epoch 18, Batch 200, Loss: 703.6167\n",
      "Epoch 18, Batch 300, Loss: 662.9631\n",
      "Epoch: 18 | Train Loss: 653.972\n",
      "손실 급등 감지, 학습률 추가 감소\n",
      "Epoch 19, Batch 0, Loss: 681.6962\n",
      "Epoch 19, Batch 100, Loss: 630.6237\n",
      "Epoch 19, Batch 200, Loss: 624.7490\n",
      "Epoch 19, Batch 300, Loss: 635.2647\n",
      "Epoch: 19 | Train Loss: 654.740\n",
      "손실 급등 감지, 학습률 추가 감소\n",
      "Epoch 20, Batch 0, Loss: 711.3275\n",
      "Epoch 20, Batch 100, Loss: 643.0573\n",
      "Epoch 20, Batch 200, Loss: 704.9866\n",
      "Epoch 20, Batch 300, Loss: 636.8285\n",
      "Epoch: 20 | Train Loss: 654.386\n",
      "손실 급등 감지, 학습률 추가 감소\n",
      "Epoch 21, Batch 0, Loss: 608.0800\n",
      "Epoch 21, Batch 100, Loss: 670.2298\n",
      "Epoch 21, Batch 200, Loss: 657.8678\n",
      "Epoch 21, Batch 300, Loss: 601.7692\n",
      "Epoch: 21 | Train Loss: 652.967\n",
      "손실 급등 감지, 학습률 추가 감소\n",
      "Epoch 22, Batch 0, Loss: 659.9515\n",
      "Epoch 22, Batch 100, Loss: 606.2169\n",
      "Epoch 22, Batch 200, Loss: 706.8182\n",
      "Epoch 22, Batch 300, Loss: 645.7605\n",
      "Epoch: 22 | Train Loss: 654.923\n",
      "손실 급등 감지, 학습률 추가 감소\n",
      "Epoch 23, Batch 0, Loss: 648.1145\n",
      "Epoch 23, Batch 100, Loss: 663.1021\n",
      "Epoch 23, Batch 200, Loss: 632.6611\n",
      "Epoch 23, Batch 300, Loss: 635.4503\n",
      "Epoch: 23 | Train Loss: 655.529\n",
      "손실 급등 감지, 학습률 추가 감소\n",
      "Epoch 24, Batch 0, Loss: 626.2984\n",
      "Epoch 24, Batch 100, Loss: 662.7009\n",
      "Epoch 24, Batch 200, Loss: 590.7859\n",
      "Epoch 24, Batch 300, Loss: 679.6494\n",
      "Epoch: 24 | Train Loss: 654.167\n",
      "손실 급등 감지, 학습률 추가 감소\n",
      "Epoch 25, Batch 0, Loss: 667.9814\n",
      "Epoch 25, Batch 100, Loss: 695.4017\n",
      "Epoch 25, Batch 200, Loss: 699.3837\n",
      "Epoch 25, Batch 300, Loss: 753.3553\n",
      "Epoch: 25 | Train Loss: 653.980\n",
      "손실 급등 감지, 학습률 추가 감소\n",
      "Epoch 26, Batch 0, Loss: 625.0972\n",
      "Epoch 26, Batch 100, Loss: 736.7369\n",
      "Epoch 26, Batch 200, Loss: 645.7880\n",
      "Epoch 26, Batch 300, Loss: 606.2624\n",
      "Epoch: 26 | Train Loss: 654.952\n",
      "손실 급등 감지, 학습률 추가 감소\n",
      "Epoch 27, Batch 0, Loss: 632.4570\n",
      "Epoch 27, Batch 100, Loss: 655.5580\n",
      "Epoch 27, Batch 200, Loss: 647.9238\n",
      "Epoch 27, Batch 300, Loss: 639.0692\n",
      "Epoch: 27 | Train Loss: 653.626\n",
      "손실 급등 감지, 학습률 추가 감소\n",
      "Epoch 28, Batch 0, Loss: 646.2144\n",
      "Epoch 28, Batch 100, Loss: 621.4172\n",
      "Epoch 28, Batch 200, Loss: 560.8066\n",
      "Epoch 28, Batch 300, Loss: 596.5677\n",
      "Epoch: 28 | Train Loss: 653.136\n",
      "손실 급등 감지, 학습률 추가 감소\n",
      "Epoch 29, Batch 0, Loss: 677.3069\n",
      "Epoch 29, Batch 100, Loss: 585.3746\n",
      "Epoch 29, Batch 200, Loss: 665.3421\n",
      "Epoch 29, Batch 300, Loss: 648.3243\n",
      "Epoch: 29 | Train Loss: 652.788\n",
      "손실 급등 감지, 학습률 추가 감소\n",
      "Epoch 30, Batch 0, Loss: 552.5472\n",
      "Epoch 30, Batch 100, Loss: 647.9131\n",
      "Epoch 30, Batch 200, Loss: 607.4721\n",
      "Epoch 30, Batch 300, Loss: 680.0372\n",
      "Epoch: 30 | Train Loss: 654.513\n",
      "손실 급등 감지, 학습률 추가 감소\n"
     ]
    }
   ],
   "source": [
    "# 1. 11에포크 이전 모델로 복원 (만약 저장했다면)\n",
    "# transformer_new.load_state_dict(torch.load('epoch_10_model.pt'))\n",
    "\n",
    "# 2. 학습률 대폭 감소\n",
    "optimizer_stable = torch.optim.Adam(transformer_new.parameters(), lr=0.0001)  # 원래대로\n",
    "\n",
    "# 3. 안정적인 학습 재시작\n",
    "print(\"안정적인 학습률로 재시작\")\n",
    "for epoch in range(1, 31):\n",
    "    train_loss = train_epoch(transformer_new, small_dataloader, optimizer_stable, criterion, device, epoch)\n",
    "    print(f'Epoch: {epoch:02} | Train Loss: {train_loss:.3f}')\n",
    "    \n",
    "    # 손실 급등 방지\n",
    "    if epoch > 1 and train_loss > 500:\n",
    "        print(\"손실 급등 감지, 학습률 추가 감소\")\n",
    "        for param_group in optimizer_stable.param_groups:\n",
    "            param_group['lr'] *= 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd643ab7-13be-49b6-8d39-2f9624451233",
   "metadata": {},
   "source": [
    "## 회고\n",
    "\n",
    "우선 이번프로젝트 내내 감기기운 + 약기운 때문에 집중도 낮고 그래서 진도따라가기도 어려웠고 프로젝트를 번역노드쪽을 진행하다가 잘못진행한걸 알게되어서 프로젝트 자체도 늦게 시작.. 해결하지못하는 오류 때문에 계속 버벅임\n",
    "### RuntimeError: CUDA error: device-side assert triggered\n",
    "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n",
    "For debugging consider passing CUDA_LAUNCH_BLOCKING=1\n",
    "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
    "\n",
    "최대한 해보려고 했으나,, 데이터 증강의 퀄리티도 낮고, 왜낮은지도 모르겠고,, ko파일 받아서 했는데 데이터 증강 품질이 너무 안좋아서 챗봇 대답이 이상한 외계어가 나옴.. 그렇다고 원본 데이터로 하려고하니 학습이 잘 되지않고 챗봇 대답 또한 낮음,, \n",
    "\n",
    "전반적으로 몸 컨디션 관리에 실패해서 팀원분들과 속도 맞추는데도 실패했음.. 그래도 포기하지않고 약의 힘으로 버텨보지만 내일,, 병원 갓다와서 다른분들 하신거 보면서 다시 공부해봐야겠다........ 그래도 어느정도 트랜스포머 모델 구조나 코드, 실험하는 방법 설계 등등 조금씩은 배워가는 것 같다. 포기하지않고,, 계속해보겠습니다.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e13aedd-35f0-47ad-a872-7dedb25463bb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
