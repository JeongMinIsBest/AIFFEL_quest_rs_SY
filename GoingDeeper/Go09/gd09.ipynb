{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dd2e2876-1bb4-4774-92f3-a8ec0bbb350c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting datasets\n",
      "  Downloading datasets-4.1.1-py3-none-any.whl.metadata (18 kB)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.12/site-packages (from datasets) (3.13.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.12/site-packages (from datasets) (2.2.6)\n",
      "Collecting pyarrow>=21.0.0 (from datasets)\n",
      "  Downloading pyarrow-21.0.0-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (3.3 kB)\n",
      "Requirement already satisfied: dill<0.4.1,>=0.3.0 in /opt/conda/lib/python3.12/site-packages (from datasets) (0.4.0)\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.12/site-packages (from datasets) (2.3.0)\n",
      "Requirement already satisfied: requests>=2.32.2 in /opt/conda/lib/python3.12/site-packages (from datasets) (2.32.4)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in /opt/conda/lib/python3.12/site-packages (from datasets) (4.67.1)\n",
      "Collecting xxhash (from datasets)\n",
      "  Downloading xxhash-3.5.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
      "Collecting multiprocess<0.70.17 (from datasets)\n",
      "  Downloading multiprocess-0.70.16-py312-none-any.whl.metadata (7.2 kB)\n",
      "Requirement already satisfied: fsspec<=2025.9.0,>=2023.1.0 in /opt/conda/lib/python3.12/site-packages (from fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (2025.5.1)\n",
      "Collecting huggingface-hub>=0.24.0 (from datasets)\n",
      "  Downloading huggingface_hub-0.35.0-py3-none-any.whl.metadata (14 kB)\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.12/site-packages (from datasets) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.12/site-packages (from datasets) (6.0.2)\n",
      "Collecting aiohttp!=4.0.0a0,!=4.0.0a1 (from fsspec[http]<=2025.9.0,>=2023.1.0->datasets)\n",
      "  Downloading aiohttp-3.12.15-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.7 kB)\n",
      "Collecting aiohappyeyeballs>=2.5.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets)\n",
      "  Downloading aiohappyeyeballs-2.6.1-py3-none-any.whl.metadata (5.9 kB)\n",
      "Collecting aiosignal>=1.4.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets)\n",
      "  Downloading aiosignal-1.4.0-py3-none-any.whl.metadata (3.7 kB)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (25.3.0)\n",
      "Collecting frozenlist>=1.1.1 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets)\n",
      "  Downloading frozenlist-1.7.0-cp312-cp312-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (18 kB)\n",
      "Collecting multidict<7.0,>=4.5 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets)\n",
      "  Downloading multidict-6.6.4-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (5.3 kB)\n",
      "Collecting propcache>=0.2.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets)\n",
      "  Downloading propcache-0.3.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
      "Collecting yarl<2.0,>=1.17.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets)\n",
      "  Downloading yarl-1.20.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (73 kB)\n",
      "Requirement already satisfied: idna>=2.0 in /opt/conda/lib/python3.12/site-packages (from yarl<2.0,>=1.17.0->aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (3.10)\n",
      "Requirement already satisfied: typing-extensions>=4.2 in /opt/conda/lib/python3.12/site-packages (from aiosignal>=1.4.0->aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (4.14.0)\n",
      "Collecting hf-xet<2.0.0,>=1.1.3 (from huggingface-hub>=0.24.0->datasets)\n",
      "  Downloading hf_xet-1.1.10-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.7 kB)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /opt/conda/lib/python3.12/site-packages (from requests>=2.32.2->datasets) (3.4.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.12/site-packages (from requests>=2.32.2->datasets) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.12/site-packages (from requests>=2.32.2->datasets) (2025.6.15)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.12/site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.12/site-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.12/site-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
      "Downloading datasets-4.1.1-py3-none-any.whl (503 kB)\n",
      "Downloading multiprocess-0.70.16-py312-none-any.whl (146 kB)\n",
      "Downloading aiohttp-3.12.15-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m30.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading multidict-6.6.4-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (256 kB)\n",
      "Downloading yarl-1.20.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (355 kB)\n",
      "Downloading aiohappyeyeballs-2.6.1-py3-none-any.whl (15 kB)\n",
      "Downloading aiosignal-1.4.0-py3-none-any.whl (7.5 kB)\n",
      "Downloading frozenlist-1.7.0-cp312-cp312-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (241 kB)\n",
      "Downloading huggingface_hub-0.35.0-py3-none-any.whl (563 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m563.4/563.4 kB\u001b[0m \u001b[31m29.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading hf_xet-1.1.10-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.2/3.2 MB\u001b[0m \u001b[31m46.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading propcache-0.3.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (224 kB)\n",
      "Downloading pyarrow-21.0.0-cp312-cp312-manylinux_2_28_x86_64.whl (42.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.8/42.8 MB\u001b[0m \u001b[31m89.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading xxhash-3.5.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
      "Installing collected packages: xxhash, pyarrow, propcache, multiprocess, multidict, hf-xet, frozenlist, aiohappyeyeballs, yarl, huggingface-hub, aiosignal, aiohttp, datasets\n",
      "\u001b[2K  Attempting uninstall: pyarrow\n",
      "\u001b[2K    Found existing installation: pyarrow 20.0.0\n",
      "\u001b[2K    Uninstalling pyarrow-20.0.0:\n",
      "\u001b[2K      Successfully uninstalled pyarrow-20.0.0━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 1/13\u001b[0m [pyarrow]\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13/13\u001b[0m [datasets]/13\u001b[0m [datasets]ce-hub]\n",
      "\u001b[1A\u001b[2KSuccessfully installed aiohappyeyeballs-2.6.1 aiohttp-3.12.15 aiosignal-1.4.0 datasets-4.1.1 frozenlist-1.7.0 hf-xet-1.1.10 huggingface-hub-0.35.0 multidict-6.6.4 multiprocess-0.70.16 propcache-0.3.2 pyarrow-21.0.0 xxhash-3.5.0 yarl-1.20.1\n",
      "Collecting loralib\n",
      "  Downloading loralib-0.1.2-py3-none-any.whl.metadata (15 kB)\n",
      "Downloading loralib-0.1.2-py3-none-any.whl (10 kB)\n",
      "Installing collected packages: loralib\n",
      "Successfully installed loralib-0.1.2\n",
      "Collecting trl\n",
      "  Downloading trl-0.23.0-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting accelerate>=1.4.0 (from trl)\n",
      "  Downloading accelerate-1.10.1-py3-none-any.whl.metadata (19 kB)\n",
      "Requirement already satisfied: datasets>=3.0.0 in /opt/conda/lib/python3.12/site-packages (from trl) (4.1.1)\n",
      "Collecting transformers>=4.56.1 (from trl)\n",
      "  Downloading transformers-4.56.1-py3-none-any.whl.metadata (42 kB)\n",
      "Requirement already satisfied: numpy<3.0.0,>=1.17 in /opt/conda/lib/python3.12/site-packages (from accelerate>=1.4.0->trl) (2.2.6)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.12/site-packages (from accelerate>=1.4.0->trl) (25.0)\n",
      "Requirement already satisfied: psutil in /opt/conda/lib/python3.12/site-packages (from accelerate>=1.4.0->trl) (7.0.0)\n",
      "Requirement already satisfied: pyyaml in /opt/conda/lib/python3.12/site-packages (from accelerate>=1.4.0->trl) (6.0.2)\n",
      "Requirement already satisfied: torch>=2.0.0 in /opt/conda/lib/python3.12/site-packages (from accelerate>=1.4.0->trl) (2.7.1+cu118)\n",
      "Requirement already satisfied: huggingface_hub>=0.21.0 in /opt/conda/lib/python3.12/site-packages (from accelerate>=1.4.0->trl) (0.35.0)\n",
      "Collecting safetensors>=0.4.3 (from accelerate>=1.4.0->trl)\n",
      "  Downloading safetensors-0.6.2-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.1 kB)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.12/site-packages (from datasets>=3.0.0->trl) (3.13.1)\n",
      "Requirement already satisfied: pyarrow>=21.0.0 in /opt/conda/lib/python3.12/site-packages (from datasets>=3.0.0->trl) (21.0.0)\n",
      "Requirement already satisfied: dill<0.4.1,>=0.3.0 in /opt/conda/lib/python3.12/site-packages (from datasets>=3.0.0->trl) (0.4.0)\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.12/site-packages (from datasets>=3.0.0->trl) (2.3.0)\n",
      "Requirement already satisfied: requests>=2.32.2 in /opt/conda/lib/python3.12/site-packages (from datasets>=3.0.0->trl) (2.32.4)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in /opt/conda/lib/python3.12/site-packages (from datasets>=3.0.0->trl) (4.67.1)\n",
      "Requirement already satisfied: xxhash in /opt/conda/lib/python3.12/site-packages (from datasets>=3.0.0->trl) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /opt/conda/lib/python3.12/site-packages (from datasets>=3.0.0->trl) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2025.9.0,>=2023.1.0 in /opt/conda/lib/python3.12/site-packages (from fsspec[http]<=2025.9.0,>=2023.1.0->datasets>=3.0.0->trl) (2025.5.1)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /opt/conda/lib/python3.12/site-packages (from fsspec[http]<=2025.9.0,>=2023.1.0->datasets>=3.0.0->trl) (3.12.15)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /opt/conda/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets>=3.0.0->trl) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in /opt/conda/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets>=3.0.0->trl) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets>=3.0.0->trl) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets>=3.0.0->trl) (1.7.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets>=3.0.0->trl) (6.6.4)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /opt/conda/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets>=3.0.0->trl) (0.3.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /opt/conda/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets>=3.0.0->trl) (1.20.1)\n",
      "Requirement already satisfied: idna>=2.0 in /opt/conda/lib/python3.12/site-packages (from yarl<2.0,>=1.17.0->aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets>=3.0.0->trl) (3.10)\n",
      "Requirement already satisfied: typing-extensions>=4.2 in /opt/conda/lib/python3.12/site-packages (from aiosignal>=1.4.0->aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets>=3.0.0->trl) (4.14.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /opt/conda/lib/python3.12/site-packages (from huggingface_hub>=0.21.0->accelerate>=1.4.0->trl) (1.1.10)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /opt/conda/lib/python3.12/site-packages (from requests>=2.32.2->datasets>=3.0.0->trl) (3.4.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.12/site-packages (from requests>=2.32.2->datasets>=3.0.0->trl) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.12/site-packages (from requests>=2.32.2->datasets>=3.0.0->trl) (2025.6.15)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.12/site-packages (from torch>=2.0.0->accelerate>=1.4.0->trl) (80.9.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /opt/conda/lib/python3.12/site-packages (from torch>=2.0.0->accelerate>=1.4.0->trl) (1.14.0)\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.12/site-packages (from torch>=2.0.0->accelerate>=1.4.0->trl) (3.5)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.12/site-packages (from torch>=2.0.0->accelerate>=1.4.0->trl) (3.1.6)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.8.89 in /opt/conda/lib/python3.12/site-packages (from torch>=2.0.0->accelerate>=1.4.0->trl) (11.8.89)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu11==11.8.89 in /opt/conda/lib/python3.12/site-packages (from torch>=2.0.0->accelerate>=1.4.0->trl) (11.8.89)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu11==11.8.87 in /opt/conda/lib/python3.12/site-packages (from torch>=2.0.0->accelerate>=1.4.0->trl) (11.8.87)\n",
      "Requirement already satisfied: nvidia-cudnn-cu11==9.1.0.70 in /opt/conda/lib/python3.12/site-packages (from torch>=2.0.0->accelerate>=1.4.0->trl) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu11==11.11.3.6 in /opt/conda/lib/python3.12/site-packages (from torch>=2.0.0->accelerate>=1.4.0->trl) (11.11.3.6)\n",
      "Requirement already satisfied: nvidia-cufft-cu11==10.9.0.58 in /opt/conda/lib/python3.12/site-packages (from torch>=2.0.0->accelerate>=1.4.0->trl) (10.9.0.58)\n",
      "Requirement already satisfied: nvidia-curand-cu11==10.3.0.86 in /opt/conda/lib/python3.12/site-packages (from torch>=2.0.0->accelerate>=1.4.0->trl) (10.3.0.86)\n",
      "Requirement already satisfied: nvidia-cusolver-cu11==11.4.1.48 in /opt/conda/lib/python3.12/site-packages (from torch>=2.0.0->accelerate>=1.4.0->trl) (11.4.1.48)\n",
      "Requirement already satisfied: nvidia-cusparse-cu11==11.7.5.86 in /opt/conda/lib/python3.12/site-packages (from torch>=2.0.0->accelerate>=1.4.0->trl) (11.7.5.86)\n",
      "Requirement already satisfied: nvidia-nccl-cu11==2.21.5 in /opt/conda/lib/python3.12/site-packages (from torch>=2.0.0->accelerate>=1.4.0->trl) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu11==11.8.86 in /opt/conda/lib/python3.12/site-packages (from torch>=2.0.0->accelerate>=1.4.0->trl) (11.8.86)\n",
      "Requirement already satisfied: triton==3.3.1 in /opt/conda/lib/python3.12/site-packages (from torch>=2.0.0->accelerate>=1.4.0->trl) (3.3.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.12/site-packages (from sympy>=1.13.3->torch>=2.0.0->accelerate>=1.4.0->trl) (1.3.0)\n",
      "Collecting regex!=2019.12.17 (from transformers>=4.56.1->trl)\n",
      "  Downloading regex-2025.9.18-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (40 kB)\n",
      "Collecting tokenizers<=0.23.0,>=0.22.0 (from transformers>=4.56.1->trl)\n",
      "  Downloading tokenizers-0.22.0-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.8 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.12/site-packages (from jinja2->torch>=2.0.0->accelerate>=1.4.0->trl) (3.0.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.12/site-packages (from pandas->datasets>=3.0.0->trl) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.12/site-packages (from pandas->datasets>=3.0.0->trl) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.12/site-packages (from pandas->datasets>=3.0.0->trl) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas->datasets>=3.0.0->trl) (1.17.0)\n",
      "Downloading trl-0.23.0-py3-none-any.whl (564 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m564.7/564.7 kB\u001b[0m \u001b[31m14.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading accelerate-1.10.1-py3-none-any.whl (374 kB)\n",
      "Downloading safetensors-0.6.2-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (485 kB)\n",
      "Downloading transformers-4.56.1-py3-none-any.whl (11.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.6/11.6 MB\u001b[0m \u001b[31m86.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tokenizers-0.22.0-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m132.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading regex-2025.9.18-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (802 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m802.0/802.0 kB\u001b[0m \u001b[31m15.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: safetensors, regex, tokenizers, accelerate, transformers, trl\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6/6\u001b[0m [trl]\u001b[32m5/6\u001b[0m [trl]sformers]\n",
      "\u001b[1A\u001b[2KSuccessfully installed accelerate-1.10.1 regex-2025.9.18 safetensors-0.6.2 tokenizers-0.22.0 transformers-4.56.1 trl-0.23.0\n",
      "Requirement already satisfied: accelerate in /opt/conda/lib/python3.12/site-packages (1.10.1)\n",
      "Requirement already satisfied: numpy<3.0.0,>=1.17 in /opt/conda/lib/python3.12/site-packages (from accelerate) (2.2.6)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.12/site-packages (from accelerate) (25.0)\n",
      "Requirement already satisfied: psutil in /opt/conda/lib/python3.12/site-packages (from accelerate) (7.0.0)\n",
      "Requirement already satisfied: pyyaml in /opt/conda/lib/python3.12/site-packages (from accelerate) (6.0.2)\n",
      "Requirement already satisfied: torch>=2.0.0 in /opt/conda/lib/python3.12/site-packages (from accelerate) (2.7.1+cu118)\n",
      "Requirement already satisfied: huggingface_hub>=0.21.0 in /opt/conda/lib/python3.12/site-packages (from accelerate) (0.35.0)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /opt/conda/lib/python3.12/site-packages (from accelerate) (0.6.2)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.12/site-packages (from huggingface_hub>=0.21.0->accelerate) (3.13.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.12/site-packages (from huggingface_hub>=0.21.0->accelerate) (2025.5.1)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.12/site-packages (from huggingface_hub>=0.21.0->accelerate) (2.32.4)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /opt/conda/lib/python3.12/site-packages (from huggingface_hub>=0.21.0->accelerate) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.12/site-packages (from huggingface_hub>=0.21.0->accelerate) (4.14.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /opt/conda/lib/python3.12/site-packages (from huggingface_hub>=0.21.0->accelerate) (1.1.10)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (80.9.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /opt/conda/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (1.14.0)\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (3.5)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (3.1.6)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.8.89 in /opt/conda/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (11.8.89)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu11==11.8.89 in /opt/conda/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (11.8.89)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu11==11.8.87 in /opt/conda/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (11.8.87)\n",
      "Requirement already satisfied: nvidia-cudnn-cu11==9.1.0.70 in /opt/conda/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu11==11.11.3.6 in /opt/conda/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (11.11.3.6)\n",
      "Requirement already satisfied: nvidia-cufft-cu11==10.9.0.58 in /opt/conda/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (10.9.0.58)\n",
      "Requirement already satisfied: nvidia-curand-cu11==10.3.0.86 in /opt/conda/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (10.3.0.86)\n",
      "Requirement already satisfied: nvidia-cusolver-cu11==11.4.1.48 in /opt/conda/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (11.4.1.48)\n",
      "Requirement already satisfied: nvidia-cusparse-cu11==11.7.5.86 in /opt/conda/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (11.7.5.86)\n",
      "Requirement already satisfied: nvidia-nccl-cu11==2.21.5 in /opt/conda/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu11==11.8.86 in /opt/conda/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (11.8.86)\n",
      "Requirement already satisfied: triton==3.3.1 in /opt/conda/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (3.3.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.12/site-packages (from sympy>=1.13.3->torch>=2.0.0->accelerate) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.12/site-packages (from jinja2->torch>=2.0.0->accelerate) (3.0.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /opt/conda/lib/python3.12/site-packages (from requests->huggingface_hub>=0.21.0->accelerate) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.12/site-packages (from requests->huggingface_hub>=0.21.0->accelerate) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.12/site-packages (from requests->huggingface_hub>=0.21.0->accelerate) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.12/site-packages (from requests->huggingface_hub>=0.21.0->accelerate) (2025.6.15)\n",
      "Requirement already satisfied: transformers in /opt/conda/lib/python3.12/site-packages (4.56.1)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.12/site-packages (from transformers) (3.13.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /opt/conda/lib/python3.12/site-packages (from transformers) (0.35.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.12/site-packages (from transformers) (2.2.6)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.12/site-packages (from transformers) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.12/site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.12/site-packages (from transformers) (2025.9.18)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.12/site-packages (from transformers) (2.32.4)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /opt/conda/lib/python3.12/site-packages (from transformers) (0.22.0)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /opt/conda/lib/python3.12/site-packages (from transformers) (0.6.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.12/site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (2025.5.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (4.14.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /opt/conda/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.1.10)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /opt/conda/lib/python3.12/site-packages (from requests->transformers) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.12/site-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.12/site-packages (from requests->transformers) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.12/site-packages (from requests->transformers) (2025.6.15)\n"
     ]
    }
   ],
   "source": [
    "!pip install datasets\n",
    "!pip install loralib\n",
    "!pip install trl\n",
    "!pip install accelerate\n",
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "56b07266-8fe7-4a02-942a-f9927d5104cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fatal: destination path 'KoChatGPT' already exists and is not an empty directory.\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/airobotlab/KoChatGPT\n",
    "!cp -r KoChatGPT/colossalai_ChatGPT_230319/chatgpt chatgpt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "18cc120e-a515-4740-a001-83d9011d7fe9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️ chatgpt/trainer/callbacks/save_checkpoint.py 파일의 3번째 줄이 예상과 다릅니다.\n",
      "   예상: from chatgpt.trainer.strategies import ColossalAIStrategy, Strategy\n",
      "   실제: from chatgpt.trainer.strategies import Strategy\n",
      "⚠️ chatgpt/trainer/callbacks/save_checkpoint.py 파일의 71번째 줄이 예상과 다릅니다.\n",
      "   예상: only_rank0 = not isinstance(self.strategy, ColossalAIStrategy)\n",
      "   실제: only_rank0 = not isinstance(self.strategy)\n",
      "⚠️ chatgpt/trainer/callbacks/save_checkpoint.py 수정할 내용이 없습니다.\n",
      "⚠️ chatgpt/trainer/strategies/__init__.py 파일의 1번째 줄이 예상과 다릅니다.\n",
      "   예상: from .colossalai import ColossalAIStrategy\n",
      "   실제: \n",
      "⚠️ chatgpt/trainer/strategies/__init__.py 파일의 5번째 줄이 예상과 다릅니다.\n",
      "   예상: __all__ = ['Strategy', 'NaiveStrategy', 'DDPStrategy', 'ColossalAIStrategy']\n",
      "   실제: __all__ = ['Strategy', 'NaiveStrategy', 'DDPStrategy']\n",
      "⚠️ chatgpt/trainer/strategies/__init__.py 수정할 내용이 없습니다.\n",
      "⚠️ chatgpt/dataset/reward_dataset.py 파일의 3번째 줄이 예상과 다릅니다.\n",
      "   예상: from tqdm import tqdm\n",
      "   실제: from tqdm.notebook import tqdm\n",
      "⚠️ chatgpt/dataset/reward_dataset.py 수정할 내용이 없습니다.\n",
      "⚠️ chatgpt/trainer/base.py 파일의 8번째 줄이 예상과 다릅니다.\n",
      "   예상: from tqdm import tqdm\n",
      "   실제: from tqdm.notebook import tqdm\n",
      "⚠️ chatgpt/trainer/base.py 수정할 내용이 없습니다.\n",
      "⚠️ chatgpt/trainer/rm.py 파일의 8번째 줄이 예상과 다릅니다.\n",
      "   예상: from tqdm import tqdm\n",
      "   실제: from tqdm.notebook import tqdm\n",
      "⚠️ chatgpt/trainer/rm.py 수정할 내용이 없습니다.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "modifications = [\n",
    "    {\n",
    "        \"file\": \"chatgpt/trainer/callbacks/save_checkpoint.py\",\n",
    "        \"changes\": [\n",
    "            {\"line\": 3, \"old\": \"from chatgpt.trainer.strategies import ColossalAIStrategy, Strategy\",\n",
    "             \"new\": \"from chatgpt.trainer.strategies import Strategy\"},\n",
    "            {\"line\": 71, \"old\": \"only_rank0 = not isinstance(self.strategy, ColossalAIStrategy)\",\n",
    "             \"new\": \"            only_rank0 = not isinstance(self.strategy)\"},\n",
    "        ],\n",
    "    },\n",
    "    {\n",
    "        \"file\": \"chatgpt/trainer/strategies/__init__.py\",\n",
    "        \"changes\": [\n",
    "            {\"line\": 1, \"old\": \"from .colossalai import ColossalAIStrategy\", \"new\": \"\"},  # 삭제\n",
    "            {\"line\": 5, \"old\": \"__all__ = ['Strategy', 'NaiveStrategy', 'DDPStrategy', 'ColossalAIStrategy']\",\n",
    "             \"new\": \"__all__ = ['Strategy', 'NaiveStrategy', 'DDPStrategy']\"},\n",
    "        ],\n",
    "    },\n",
    "    {\n",
    "        \"file\": \"chatgpt/dataset/reward_dataset.py\",\n",
    "        \"changes\": [\n",
    "            {\"line\": 3, \"old\": \"from tqdm import tqdm\", \"new\": \"from tqdm.notebook import tqdm\"},\n",
    "        ],\n",
    "    },\n",
    "    {\n",
    "        \"file\": \"chatgpt/trainer/base.py\",\n",
    "        \"changes\": [\n",
    "            {\"line\": 8, \"old\": \"from tqdm import tqdm\", \"new\": \"from tqdm.notebook import tqdm\"},\n",
    "        ]\n",
    "    },\n",
    "    {\n",
    "        \"file\": \"chatgpt/trainer/rm.py\",\n",
    "        \"changes\": [\n",
    "            {\"line\": 8, \"old\": \"from tqdm import tqdm\", \"new\": \"from tqdm.notebook import tqdm\"},\n",
    "        ]\n",
    "    }\n",
    "]\n",
    "\n",
    "\n",
    "def modify_file(file_path, changes):\n",
    "    \"\"\"파일에서 지정된 줄을 찾아 내용을 수정하는 함수\"\"\"\n",
    "\n",
    "    if not os.path.exists(file_path):\n",
    "        print(f\"⚠️ 파일이 존재하지 않습니다: {file_path}\")\n",
    "        return\n",
    "\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "        lines = file.readlines()\n",
    "\n",
    "    modified = False\n",
    "\n",
    "    for change in changes:\n",
    "        line_index = change[\"line\"]\n",
    "        if 0 <= line_index < len(lines):\n",
    "            if lines[line_index].strip() == change[\"old\"]:\n",
    "                lines[line_index] = change[\"new\"] + \"\\n\"\n",
    "                modified = True\n",
    "            else:\n",
    "                print(f\"⚠️ {file_path} 파일의 {change['line']}번째 줄이 예상과 다릅니다.\")\n",
    "                print(f\"   예상: {change['old']}\")\n",
    "                print(f\"   실제: {lines[line_index].strip()}\")\n",
    "\n",
    "    if modified:\n",
    "        with open(file_path, \"w\", encoding=\"utf-8\") as file:\n",
    "            file.writelines(lines)\n",
    "        print(f\"✅ 수정 완료: {file_path}\")\n",
    "    else:\n",
    "        print(f\"⚠️ {file_path} 수정할 내용이 없습니다.\")\n",
    "\n",
    "for mod in modifications:\n",
    "    modify_file(mod[\"file\"], mod[\"changes\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "847b488d-580b-4344-8aee-925d8d2df1eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Torch version:2.7.1+cu118\n",
      "Cuda version: 11.8\n",
      "transformers version: 4.56.1\n",
      "GPU 사용 가능여부: True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import transformers\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import pandas as pd\n",
    "import numpy\n",
    "\n",
    "print(\"Torch version:{}\".format(torch.__version__)) # Torch version:1.12.1\n",
    "print(\"Cuda version: {}\".format(torch.version.cuda)) # Cuda version: 11.3\n",
    "print(\"transformers version: {}\".format(transformers.__version__)) # transformers 4.28.0\n",
    "print(\"GPU 사용 가능여부: {}\".format(torch.cuda.is_available()))\n",
    "\n",
    "# 만일 아래 모듈이 불러와지지 않는다면 Clone 및 수정을 잘 진행했는지 확인해주세요.\n",
    "from chatgpt.trainer.strategies import NaiveStrategy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e8b300f-887b-4e18-8999-c33fc95dd8ca",
   "metadata": {},
   "source": [
    "# 2. Base model and Dataset for RLHF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "04063d86-4304-4a27-a2b6-197e82b7e59a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e6363b5504fb4e6f9c0de7c37d00d4bd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "96ccf737244c424585342bcdd1ea777b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "19727d8a0fd44aa4af409ee72b242bea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/513M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model_name = \"skt/kogpt2-base-v2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fa9fd82-ef29-4ba7-b13c-6cb9145a5e95",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "44b4a050-da7a-402c-a320-f48805bb32c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000000000000000019884624838656"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#사용할 모델의 토크나이저가 입력받아 처리할 수 있는 최대 토큰 수를 확인\n",
    "tokenizer.model_max_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4a3ef817-2f5c-4ce8-96e0-221bc1145168",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1024"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# kogpt-2 토크나이징 확인\n",
    "model.config.n_positions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "813259b7-d440-4dd4-8e92-527bbadd15be",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_txt = \"바람도 없는 공중에 수직의 파문을 내이며 고요히 떨어지는 오동잎은 누구의 발자취 입니까.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "62d6e1ab-0eab-4baf-b103-82251bae611f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = tokenizer(input_txt).tokens()\n",
    "input_ids = tokenizer(input_txt, return_tensors=\"pt\")[\"input_ids\"].numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "58051f2b-65fa-4b77-baee-544a2e6f2316",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>21</th>\n",
       "      <th>22</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>kogpt-2_tokens</th>\n",
       "      <td>▁바람</td>\n",
       "      <td>도</td>\n",
       "      <td>▁없는</td>\n",
       "      <td>▁공중에</td>\n",
       "      <td>▁수직</td>\n",
       "      <td>의</td>\n",
       "      <td>▁파</td>\n",
       "      <td>문을</td>\n",
       "      <td>▁내</td>\n",
       "      <td>이며</td>\n",
       "      <td>▁고</td>\n",
       "      <td>요</td>\n",
       "      <td>히</td>\n",
       "      <td>▁떨어지는</td>\n",
       "      <td>▁오동</td>\n",
       "      <td>잎은</td>\n",
       "      <td>▁누</td>\n",
       "      <td>구의</td>\n",
       "      <td>▁발자</td>\n",
       "      <td>취</td>\n",
       "      <td>▁입</td>\n",
       "      <td>니까</td>\n",
       "      <td>.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Input_IDs</th>\n",
       "      <td>10891</td>\n",
       "      <td>7235</td>\n",
       "      <td>9712</td>\n",
       "      <td>49207</td>\n",
       "      <td>14438</td>\n",
       "      <td>8143</td>\n",
       "      <td>9203</td>\n",
       "      <td>9941</td>\n",
       "      <td>9094</td>\n",
       "      <td>9639</td>\n",
       "      <td>9065</td>\n",
       "      <td>8084</td>\n",
       "      <td>8811</td>\n",
       "      <td>21215</td>\n",
       "      <td>34769</td>\n",
       "      <td>19985</td>\n",
       "      <td>9669</td>\n",
       "      <td>10139</td>\n",
       "      <td>21626</td>\n",
       "      <td>8408</td>\n",
       "      <td>9241</td>\n",
       "      <td>23775</td>\n",
       "      <td>389</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   0     1     2      3      4     5     6     7     8     9   \\\n",
       "kogpt-2_tokens    ▁바람     도   ▁없는   ▁공중에    ▁수직     의    ▁파    문을    ▁내    이며   \n",
       "Input_IDs       10891  7235  9712  49207  14438  8143  9203  9941  9094  9639   \n",
       "\n",
       "                  10    11    12     13     14     15    16     17     18  \\\n",
       "kogpt-2_tokens    ▁고     요     히  ▁떨어지는    ▁오동     잎은    ▁누     구의    ▁발자   \n",
       "Input_IDs       9065  8084  8811  21215  34769  19985  9669  10139  21626   \n",
       "\n",
       "                  19    20     21   22  \n",
       "kogpt-2_tokens     취    ▁입     니까    .  \n",
       "Input_IDs       8408  9241  23775  389  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.options.display.max_columns = 40\n",
    "pd.options.display.max_rows = 60\n",
    "df = pd.DataFrame([tokens, input_ids[0]], index=[\"kogpt-2_tokens\", \"Input_IDs\"])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ca31b126-4de5-4f2d-9ab8-ed0c209a4351",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f8d1ad63db0401f909db76b8447154b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/513M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "바람도 없는 공중에 수직의 파문을 내이며 고요히 떨어지는 오동잎은 누구의 발자취 입니까.'\n",
      "\"그렇다면 그건 무슨 소리요?\"\n",
      "\"그건 무슨 소리요?\"\n",
      "\"그건 무슨 소리요?\"\n",
      "\"그건 무슨 소리요?\"\n",
      "\"그건 무슨 소리요?\"\n",
      "\"그건 무슨 소리요?\"\n",
      "\"그건 무슨 소리요?\"\n",
      "\"그건 무슨 소리요?\"\n",
      "\"그건 무슨 소리요?\"\n",
      "\"그건 무슨 소리요?\"\n",
      "\"그건 무슨 소리요?\"\n",
      "\"그건 무슨 소리요?\"\n",
      "\"그건 무슨 소리\n"
     ]
    }
   ],
   "source": [
    "# 디코딩 성능 확인\n",
    "max_length=128\n",
    "input_ids = tokenizer(input_txt, return_tensors=\"pt\")[\"input_ids\"].to(device)\n",
    "output_greedy = model.generate(input_ids, max_length=max_length, do_sample=False)\n",
    "print(tokenizer.decode(output_greedy[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01c6166d-df26-4129-a50f-ee9b37c63b40",
   "metadata": {},
   "source": [
    "시퀀스가 반복되어 출력\n",
    "그리디 서치 디코딩시 발견되는 전형적인 현상.\n",
    "빔 서치 디코딩을 사용하고 n-gram 패널티까지 부과"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d8e0a723-91a1-4464-8ff0-d3f3c9bbf6e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "바람도 없는 공중에 수직의 파문을 내이며 고요히 떨어지는 오동잎은 누구의 발자취 입니까.'\n",
      "\"그렇지 않습니다.\"\n",
      "\"어떻게 된 일입니까?\"\n",
      "그녀는 고개를 갸웃거렸다.\n",
      "\"아니, 그게 무슨 말씀이신지 모르겠습니다만.\"\n",
      "\"무슨 말씀인지 알 수가 없군요.\"\n",
      "아무런 대답도 하지 않은 채 그녀는 고개를 끄덕였다.\n",
      "\"그래, 알았어.\"\n",
      "그녀의 눈에서 눈물이 주르륵 흘러내렸다.\n",
      "그녀가 다시 입을 열었다.\n",
      "\"정말 죄송합니다, 고마워요, 고맙습니다\"\n",
      "\"\n"
     ]
    }
   ],
   "source": [
    "input_ids = tokenizer(input_txt, return_tensors=\"pt\")[\"input_ids\"].to(device)\n",
    "output_beam = model.generate(input_ids, max_length=max_length, num_beams=10, no_repeat_ngram_size=2,\n",
    "                             do_sample=False)\n",
    "print(tokenizer.decode(output_beam[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b24b48a0-2990-4423-ada5-d9204fa03d49",
   "metadata": {},
   "source": [
    "입력 시퀀스와 별 상관 없어 보이는 긴 문단이 생성.\n",
    "그럼에도 생성된 문단은 제법 맥락을 갖춘 듯 .\n",
    "하지만 문장 간의 정합성이나 일관성은 다소 떨어지는 부분도 관찰.\n",
    "이번엔 샘플링 기법까지 추가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ed45c671-b451-4f85-9281-05c80128fd10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "바람도 없는 공중에 수직의 파문을 내이며 고요히 떨어지는 오동잎은 누구의 발자취 입니까.\"\n",
      "\"글쎄요. 저도 그걸 몰라요.\"\n",
      "그때서야 그녀는 자신도 모르게 말을 멈췄다.\n",
      "\"네, 그거야 어쩔 수 없잖아요. 그게 문제입니까?\"\n",
      "갑자기 불현듯 생각이 떠올랐다.\n",
      "불현듯, 가슴이 떨려오는 느낌이 들었다.\n",
      "\"그런데 이게 무슨 일이에요? 저는 저를 사랑하고 있을 거예요. 제 인생은 제가 책임져야 할 거라고 생각해요. 모든 걸 제가 다 책임지겠어요. 제가 하고 싶은 일이라면\n"
     ]
    }
   ],
   "source": [
    "output_beam = model.generate(input_ids, max_length=max_length, num_beams=7, no_repeat_ngram_size=2,\n",
    "                             do_sample=True, temperature=2.0, top_k=50)\n",
    "print(tokenizer.decode(output_beam[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "034fd1c6-0054-4753-946c-433bab7dc74c",
   "metadata": {},
   "source": [
    "top_p 샘플링 기법도 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "134a9ba0-8c2d-45a7-92f7-0e0280ebb8cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "바람도 없는 공중에 수직의 파문을 내이며 고요히 떨어지는 오동잎은 누구의 발자취 입니까.\"\n",
      "\"그렇지 않습니다.\"\n",
      "그녀는 고개를 끄덕였다.\n",
      "\"아니, 그게 무슨 말입니까?\"\n",
      "그러자 그제야 그녀의 얼굴이 환해졌다.\n",
      "\"무슨 말씀이신지 모르겠군요.\"\n",
      "그는 고개를 갸우뚱거렸다.\n",
      "\"어째서 그런 말씀을 하시는 겁니까.\"\n",
      "그녀가 고개를 저었다.\n",
      "\"왜 그러십니까, 선생님.\"\n",
      "\"선생님, 저는 선생님을 사랑하고 있습니다. 선생님은 저를 사랑하시는 분이시니까요. 선생님이 저에게\n"
     ]
    }
   ],
   "source": [
    "output_beam = model.generate(input_ids, max_length=max_length, num_beams=7, no_repeat_ngram_size=2,\n",
    "                             do_sample=True, top_p=0.90)\n",
    "print(tokenizer.decode(output_beam[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8be597f7-80b7-4335-94b6-9f853e213a86",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "11ad9250-8392-43ea-ae98-320be4b235ee",
   "metadata": {},
   "source": [
    "# 데이터셋 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d95b30b1-7ec0-4d37-b8ed-93f56d0f8b30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'prompt': '불고기용 고기 한우에요?',\n",
       "  'completion': \"'저는 인공지능 챗봇이며, 직접적으로 식품에 관한 정보를 가지고 있지 않습니다. 하지만 일반적으로 불고기용 고기는 한우, 쇠고기, 돼지고기 등 다양한 종류의 고기를 사용합니다. 하지만 한우는 대표적인 고급 육류로 알려져 있기 때문에, 한우를 사용하는 경우도 많습니다. 알러지나 개별 건강 상태에 따라 다를 수 있으니 충분한 정보 수집 후에 선택해 주시기 바랍니다.\",\n",
       "  'tokens': 193},\n",
       " {'prompt': '쓰던 앱이 유료로 전환됐어',\n",
       "  'completion': \"'어떤 앱인지 모르기 때문에 정확한 답변을 드리기 어렵습니다. 하지만, 일반적으로 유료 전환된 앱은 기존 무료 앱에서 추가적인 기능이나 서비스를 제공하는 경우가 많습니다. 이 경우, 유료 전환 이전 가격이 매우 저렴하거나 무료인 경우가 많으므로, 앱을 계속 사용하려면 추가적인 비용을 지불해야 할 수 있습니다. 하지만, 유료 전환 이후에는 기존 사용자 또는 새로운 사용자 모두가 추가 기능과 높은 품질의 서비스를 이용할 수 있게 됩니다. 따라서, 앱 개발자는 유료 모델로 전환함으로써 앱의 수익을 증가시키고 더 나은 서비스를 제공할 수 있습니다.\",\n",
       "  'tokens': 288},\n",
       " {'prompt': '여친이랑 다툼',\n",
       "  'completion': \"'저는 인공지능이기 때문에 여친과의 다툼에 대해서는 조언을 드릴 수는 없습니다. 하지만 다투게 되면 상대방의 말을 경청하는 태도와 서로의 감정을 존중하는 것이 중요합니다. 논리적이고 차분한 대화를 통해 문제를 해결해보세요. 그리고 서로를 이해하고 서로의 의견을 수용하는 것이 중요합니다.\",\n",
       "  'tokens': 153}]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "data_path_1_SFT = 'KoChatGPT/data_kochatgpt/kochatgpt_1_SFT.jsonl'\n",
    "with open(data_path_1_SFT, \"r\", encoding='utf-8-sig') as json_file:\n",
    "    list_data_dict = json.load(json_file)\n",
    "\n",
    "print(len(list_data_dict))\n",
    "list_data_dict[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d942503d-6eae-4bd5-b0f5-dd28e4ec5981",
   "metadata": {},
   "source": [
    "RM에 사용할 데이터셋 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a0c844d7-9991-4234-b125-c18aae04a1e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10220\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'prompt': '번디는 자신이 탐정잡지, 범죄소설 그리고 성범죄 관련 실제 범죄 다큐멘터리들을 탐독했다고 누구에게 말했나?',\n",
       "  'completion_0': 'Allow me to answer your question. I know that you are curious about me.',\n",
       "  'completion_1': '번디는 다양한 인터뷰자들과 뉴스홍보 담당자들과의 면담 때 밝혔다.',\n",
       "  'completion_2': '라이언에게 말했다.',\n",
       "  'ranking': [2, 1, 0]},\n",
       " {'prompt': '개포주공아파트는 몇 단지로 이루어져 있나?',\n",
       "  'completion_0': '개포주공아파트는 다섯 단지로 이루어져 있습니다.',\n",
       "  'completion_1': '이날 목송에서 구글상위노',\n",
       "  'completion_2': '개포주공아파트는 총 27개 단지로 이루어져 있습니다.',\n",
       "  'ranking': [2, 0, 1]},\n",
       " {'prompt': '김영삼의 후보 시절 지역표심을 겨냥한 발언을 문제삼은 후보는?',\n",
       "  'completion_0': 'The diameter of the Metallic domain is bigger than the Hyperonic domain.',\n",
       "  'completion_1': '이 질문은 조금 불분명합니다. 김영삼 대통령이 후보 시절에 어떤 발언을 했고, 누가 그 발언을 문제삼았는지에 따라 답이 다를 수 있습니다.\\\\n\\\\n만약 김영삼 대통령이 후보 시절에 지역표심을 겨냥한 발언을 했다는 가정하에, 그 발언을 문제삼은 후보가 누구였는지를 대답하자면, 그 답은 이화선 당시 민주당 대통령 후보가 될 것입니다. 1992년 총선 때, 김영삼 대선후보는 \"집값이 오른 노량진역 부근의 부동산 가격은 세월호 폭침 후 \\\\\\'강남 도시재생\\\\\\' 일환으로 상승했다\"는 발언을 했습니다. 하지만 이화선 후보는 이 발언을 \"전국적으로 경제적 발전이 이루어지지 않은 지방민의 마음을 멀리해지려는 무례한 발언\"이라고 비판하며 문제삼았습니다.\\\\n\\\\n하지만, 이 질문을 답변하는 데 있어서 보다 명확한 정보가 있으면 답변을 보완할 수 있습니다.',\n",
       "  'completion_2': '김영삼의 후보 시절에 지역표심을 겨냥한 발언은 대통령 당선 전까지 대한민국 정부가 추구하고 있는 민주주의 광범위하게 확립과 보수의 사상을 이어가는 데 있어 지역경제 발전과 공공서비스 신속 개선을 위해 합리적인 국가 정책에 따르는 방향성을 제시하고 있습니다.',\n",
       "  'ranking': [1, 2, 0]}]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_path_2_RM = 'KoChatGPT/data_kochatgpt/kochatgpt_2_RM.jsonl'\n",
    "with open(data_path_2_RM, \"r\", encoding='utf-8-sig') as json_file:\n",
    "    list_data_dict = json.load(json_file)\n",
    "\n",
    "print(len(list_data_dict))\n",
    "list_data_dict[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b01d2e80-fac5-4ba4-9259-ba9e8f6b8bfe",
   "metadata": {},
   "source": [
    " PPO 학습에 쓰일 데이터 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "32baff01-9038-4e94-8989-5714cda1c657",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'prompt': '번디는 자신이 탐정잡지, 범죄소설 그리고 성범죄 관련 실제 범죄 다큐멘터리들을 탐독했다고 누구에게 말했나?'},\n",
       " {'prompt': '개포주공아파트는 몇 단지로 이루어져 있나?'},\n",
       " {'prompt': '김영삼의 후보 시절 지역표심을 겨냥한 발언을 문제삼은 후보는?'}]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_path_3_PPO = 'KoChatGPT/data_kochatgpt/kochatgpt_3_PPO.jsonl'\n",
    "with open(data_path_3_PPO, \"r\", encoding='utf-8-sig') as json_file:\n",
    "    list_data_dict = json.load(json_file)\n",
    "\n",
    "print(len(list_data_dict))\n",
    "list_data_dict[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "568f3e91-7840-4e68-a1f3-b78050878bc4",
   "metadata": {},
   "source": [
    "# 3. Supervised Fine-Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6a25b13-83f9-44e3-aa53-52fef6f654a8",
   "metadata": {},
   "source": [
    "SFT\n",
    "이번 스텝에서는 kogpt-2를 instruction dataset으로 SFT를 진행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7c42f7ad-0393-47c3-91b6-eb0a7fd37d2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional, Dict, Sequence\n",
    "from torch.utils.data import Dataset\n",
    "from dataclasses import dataclass\n",
    "import logging\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4fee3f41-065b-4a64-aea4-a49a109bb450",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT2TokenizerFast(name_or_path='skt/kogpt2-base-v2', vocab_size=51200, model_max_length=512, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '</s>', 'eos_token': '</s>', 'unk_token': '</s>', 'pad_token': '</s>'}, clean_up_tokenization_spaces=False, added_tokens_decoder={\n",
      "\t0: AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t1: AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t2: AddedToken(\"<usr>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t3: AddedToken(\"<pad>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t4: AddedToken(\"<sys>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t5: AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t6: AddedToken(\"<mask>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t7: AddedToken(\"<d>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t8: AddedToken(\"</d>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t9: AddedToken(\"<unused0>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t10: AddedToken(\"<unused1>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t11: AddedToken(\"<unused2>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t12: AddedToken(\"<unused3>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t13: AddedToken(\"<unused4>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t14: AddedToken(\"<unused5>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t15: AddedToken(\"<unused6>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t16: AddedToken(\"<unused7>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t17: AddedToken(\"<unused8>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t18: AddedToken(\"<unused9>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t19: AddedToken(\"<unused10>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t20: AddedToken(\"<unused11>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t21: AddedToken(\"<unused12>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t22: AddedToken(\"<unused13>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t23: AddedToken(\"<unused14>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t24: AddedToken(\"<unused15>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t25: AddedToken(\"<unused16>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t26: AddedToken(\"<unused17>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t27: AddedToken(\"<unused18>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t28: AddedToken(\"<unused19>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t29: AddedToken(\"<unused20>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t30: AddedToken(\"<unused21>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t31: AddedToken(\"<unused22>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t32: AddedToken(\"<unused23>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t33: AddedToken(\"<unused24>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t34: AddedToken(\"<unused25>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t35: AddedToken(\"<unused26>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t36: AddedToken(\"<unused27>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t37: AddedToken(\"<unused28>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t38: AddedToken(\"<unused29>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t39: AddedToken(\"<unused30>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t40: AddedToken(\"<unused31>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t41: AddedToken(\"<unused32>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t42: AddedToken(\"<unused33>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t43: AddedToken(\"<unused34>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t44: AddedToken(\"<unused35>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t45: AddedToken(\"<unused36>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t46: AddedToken(\"<unused37>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t47: AddedToken(\"<unused38>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t48: AddedToken(\"<unused39>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t49: AddedToken(\"<unused40>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t50: AddedToken(\"<unused41>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t51: AddedToken(\"<unused42>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t52: AddedToken(\"<unused43>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t53: AddedToken(\"<unused44>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t54: AddedToken(\"<unused45>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t55: AddedToken(\"<unused46>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t56: AddedToken(\"<unused47>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t57: AddedToken(\"<unused48>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t58: AddedToken(\"<unused49>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t59: AddedToken(\"<unused50>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t60: AddedToken(\"<unused51>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t61: AddedToken(\"<unused52>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t62: AddedToken(\"<unused53>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t63: AddedToken(\"<unused54>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t64: AddedToken(\"<unused55>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t65: AddedToken(\"<unused56>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t66: AddedToken(\"<unused57>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t67: AddedToken(\"<unused58>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t68: AddedToken(\"<unused59>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t69: AddedToken(\"<unused60>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t70: AddedToken(\"<unused61>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t71: AddedToken(\"<unused62>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t72: AddedToken(\"<unused63>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t73: AddedToken(\"<unused64>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t74: AddedToken(\"<unused65>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t75: AddedToken(\"<unused66>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t76: AddedToken(\"<unused67>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t77: AddedToken(\"<unused68>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t78: AddedToken(\"<unused69>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t79: AddedToken(\"<unused70>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t80: AddedToken(\"<unused71>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t81: AddedToken(\"<unused72>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t82: AddedToken(\"<unused73>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t83: AddedToken(\"<unused74>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t84: AddedToken(\"<unused75>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t85: AddedToken(\"<unused76>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t86: AddedToken(\"<unused77>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t87: AddedToken(\"<unused78>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t88: AddedToken(\"<unused79>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t89: AddedToken(\"<unused80>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t90: AddedToken(\"<unused81>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t91: AddedToken(\"<unused82>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t92: AddedToken(\"<unused83>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t93: AddedToken(\"<unused84>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t94: AddedToken(\"<unused85>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t95: AddedToken(\"<unused86>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t96: AddedToken(\"<unused87>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t97: AddedToken(\"<unused88>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t98: AddedToken(\"<unused89>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t99: AddedToken(\"<unused90>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t100: AddedToken(\"<unused91>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t101: AddedToken(\"<unused92>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t102: AddedToken(\"<unused93>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t103: AddedToken(\"<unused94>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t104: AddedToken(\"<unused95>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t105: AddedToken(\"<unused96>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t106: AddedToken(\"<unused97>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t107: AddedToken(\"<unused98>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t108: AddedToken(\"<unused99>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t109: AddedToken(\":-)\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t110: AddedToken(\":)\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t111: AddedToken(\"-)\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t112: AddedToken(\"(-:\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t113: AddedToken(\"(:-)\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t114: AddedToken(\"(:-(\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t115: AddedToken(\"-}\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t116: AddedToken(\"8-O\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t117: AddedToken(\"'-)\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t118: AddedToken(\":-#\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t119: AddedToken(\":-*\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t120: AddedToken(\":-/\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t121: AddedToken(\":->\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t122: AddedToken(\":-@\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t123: AddedToken(\":-d\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t124: AddedToken(\":-V\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t125: AddedToken(\":-X\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t126: AddedToken(\":-\\\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t127: AddedToken(\":-]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128: AddedToken(\";-(\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t129: AddedToken(\">;->\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t130: AddedToken(\";^)\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t131: AddedToken(\"%-)\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t132: AddedToken(\"):-(\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t133: AddedToken(\"3:]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t134: AddedToken(\":-&\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t135: AddedToken(\"8:-)\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t136: AddedToken(\":-)8<\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t137: AddedToken(\":-O\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t138: AddedToken(\":-6\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t139: AddedToken(\"+:-)\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t140: AddedToken(\"O:-)\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t141: AddedToken(\":-<\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t142: AddedToken(\":-?\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t143: AddedToken(\":-E\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t144: AddedToken(\":-Q\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t145: AddedToken(\":-}X\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t146: AddedToken(\":-[\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t147: AddedToken(\":-a\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t148: AddedToken(\":-{\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t149: AddedToken(\":-{}\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t150: AddedToken(\":^)\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t151: AddedToken(\"<:-l\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t152: AddedToken(\":=)\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t153: AddedToken(\">:->\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t154: AddedToken(\">:-l\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t155: AddedToken(\"@:-)\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t156: AddedToken(\"@:-}\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t157: AddedToken(\"C=:-)\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t158: AddedToken(\"X:-)\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t159: AddedToken(\"[:-)\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t160: AddedToken(\"[:]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t161: AddedToken(\"{:-)\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t162: AddedToken(\"l^o\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t163: AddedToken(\"}:^#)\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t164: AddedToken(\":-(=)\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t165: AddedToken(\"O-)\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t166: AddedToken(\":-3\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t167: AddedToken(\":=\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t168: AddedToken(\":-\"\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t169: AddedToken(\"P-(\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t170: AddedToken(\"?-(\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t171: AddedToken(\"d:-)\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t172: AddedToken(\":8)\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t173: AddedToken(\":-7\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t174: AddedToken(\"):-)\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t175: AddedToken(\":/\\)\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t176: AddedToken(\"8(:-)\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t177: AddedToken(\"([(\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t178: AddedToken(\":-(*)\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t179: AddedToken(\"&-l\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t180: AddedToken(\":-e\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t181: AddedToken(\":(\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t182: AddedToken(\":,(\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t183: AddedToken(\":-(\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t184: AddedToken(\":-P\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t185: AddedToken(\":-S\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t186: AddedToken(\":-C\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t187: AddedToken(\":-r\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t188: AddedToken(\":-t\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t189: AddedToken(\":-W\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t190: AddedToken(\"X-(\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t191: AddedToken(\"l-O\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t192: AddedToken(\"l:-O\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t193: AddedToken(\"$-)\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t194: AddedToken(\":-!\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t195: AddedToken(\":----}\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t196: AddedToken(\"=:-)\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t197: AddedToken(\"=:-(\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t198: AddedToken(\"3:[\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t199: AddedToken(\"8<:-)\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t200: AddedToken(\":#)\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t201: AddedToken(\"8-#\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t202: AddedToken(\"B-)\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t203: AddedToken(\"8-)\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t204: AddedToken(\"|-(\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t205: AddedToken(\"H-)\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t206: AddedToken(\"]-I\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t207: AddedToken(\"V^J\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t208: AddedToken(\"+-(\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t209: AddedToken(\"~:-P\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t210: AddedToken(\"`'\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t211: AddedToken(\"L-P\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t212: AddedToken(\"BI\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t213: AddedToken(\"O|\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t214: AddedToken(\"^^\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t215: AddedToken(\"ㅜㅜ\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t216: AddedToken(\"ㅠㅠ\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t217: AddedToken(\"ㅡㅡ\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t218: AddedToken(\"😠\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t219: AddedToken(\"👿\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t220: AddedToken(\"😧\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t221: AddedToken(\"😰\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t222: AddedToken(\"😲\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t223: AddedToken(\"😁\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t224: AddedToken(\"🐻\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t225: AddedToken(\"🐱\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t226: AddedToken(\"😹\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t227: AddedToken(\"😼\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t228: AddedToken(\"🤡\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t229: AddedToken(\"🥶\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t230: AddedToken(\"😖\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t231: AddedToken(\"😕\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t232: AddedToken(\"🐮\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t233: AddedToken(\"🤠\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t234: AddedToken(\"😿\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t235: AddedToken(\"😢\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t236: AddedToken(\"😞\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t237: AddedToken(\"😵\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t238: AddedToken(\"🐶\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t239: AddedToken(\"😓\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t240: AddedToken(\"🐲\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t241: AddedToken(\"🤤\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t242: AddedToken(\"😑\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t243: AddedToken(\"😘\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t244: AddedToken(\"😋\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t245: AddedToken(\"😱\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t246: AddedToken(\"🤮\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t247: AddedToken(\"🤭\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t248: AddedToken(\"🤕\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t249: AddedToken(\"😷\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t250: AddedToken(\"🧐\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t251: AddedToken(\"😮\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t252: AddedToken(\"🤨\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t253: AddedToken(\"🙄\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t254: AddedToken(\"😤\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t255: AddedToken(\"🤬\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t256: AddedToken(\"😂\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t257: AddedToken(\"🤒\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t258: AddedToken(\"😛\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t259: AddedToken(\"😶\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t260: AddedToken(\"😨\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t261: AddedToken(\"🌛\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t262: AddedToken(\"😳\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t263: AddedToken(\"🦊\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t264: AddedToken(\"🐸\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t265: AddedToken(\"☹\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t266: AddedToken(\"☹️\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t267: AddedToken(\"😦\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t268: AddedToken(\"🌝\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t269: AddedToken(\"😬\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t270: AddedToken(\"😺\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t271: AddedToken(\"😸\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t272: AddedToken(\"😀\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t273: AddedToken(\"😃\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t274: AddedToken(\"😄\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t275: AddedToken(\"😅\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t276: AddedToken(\"😆\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t277: AddedToken(\"🐹\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t278: AddedToken(\"🐴\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t279: AddedToken(\"🥵\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t280: AddedToken(\"🤗\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t281: AddedToken(\"😯\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t282: AddedToken(\"😽\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t283: AddedToken(\"😗\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t284: AddedToken(\"😚\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t285: AddedToken(\"😙\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t286: AddedToken(\"🌜\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t287: AddedToken(\"🦁\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t288: AddedToken(\"😭\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t289: AddedToken(\"🤥\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t290: AddedToken(\"🤦🏿‍♂\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t291: AddedToken(\"🤦🏻‍♂\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t292: AddedToken(\"🤦🏾‍♂\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t293: AddedToken(\"🤦🏼‍♂\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t294: AddedToken(\"🤦🏽‍♂\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t295: AddedToken(\"🤦‍♂\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t296: AddedToken(\"🤦🏿‍♂️\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t297: AddedToken(\"🤦🏻‍♂️\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t298: AddedToken(\"🤦🏾‍♂️\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t299: AddedToken(\"🤦🏼‍♂️\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t300: AddedToken(\"🤦🏽‍♂️\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t301: AddedToken(\"🤦‍♂️\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t302: AddedToken(\"🤑\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t303: AddedToken(\"🐵\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t304: AddedToken(\"🐭\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t305: AddedToken(\"🤢\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t306: AddedToken(\"🤓\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t307: AddedToken(\"😐\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t308: AddedToken(\"🌚\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t309: AddedToken(\"🐼\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t310: AddedToken(\"🥳\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t311: AddedToken(\"😔\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t312: AddedToken(\"😣\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t313: AddedToken(\"🤦\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t314: AddedToken(\"🤦🏿\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t315: AddedToken(\"🤦🏻\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t316: AddedToken(\"🤦🏾\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t317: AddedToken(\"🤦🏼\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t318: AddedToken(\"🤦🏽\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t319: AddedToken(\"🐷\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t320: AddedToken(\"🥺\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t321: AddedToken(\"😾\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t322: AddedToken(\"😡\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t323: AddedToken(\"🐰\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t324: AddedToken(\"😌\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t325: AddedToken(\"🤖\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t326: AddedToken(\"😥\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t327: AddedToken(\"🤫\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t328: AddedToken(\"😴\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t329: AddedToken(\"😪\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t330: AddedToken(\"🙁\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t331: AddedToken(\"🙂\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t332: AddedToken(\"😻\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t333: AddedToken(\"☺\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t334: AddedToken(\"☺️\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t335: AddedToken(\"🥰\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t336: AddedToken(\"😇\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t337: AddedToken(\"😍\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t338: AddedToken(\"😈\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t339: AddedToken(\"😊\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t340: AddedToken(\"😎\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t341: AddedToken(\"😏\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t342: AddedToken(\"🤧\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t343: AddedToken(\"😝\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t344: AddedToken(\"🌞\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t345: AddedToken(\"🤔\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t346: AddedToken(\"🐯\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t347: AddedToken(\"😫\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t348: AddedToken(\"😒\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t349: AddedToken(\"🦄\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t350: AddedToken(\"🙃\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t351: AddedToken(\"🙀\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t352: AddedToken(\"😩\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t353: AddedToken(\"🌬\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t354: AddedToken(\"🌬️\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t355: AddedToken(\"😉\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t356: AddedToken(\"😜\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t357: AddedToken(\"🐺\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t358: AddedToken(\"🤦🏿‍♀\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t359: AddedToken(\"🤦🏻‍♀\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t360: AddedToken(\"🤦🏾‍♀\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t361: AddedToken(\"🤦🏼‍♀\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t362: AddedToken(\"🤦🏽‍♀\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t363: AddedToken(\"🤦‍♀\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t364: AddedToken(\"🤦🏿‍♀️\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t365: AddedToken(\"🤦🏻‍♀️\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t366: AddedToken(\"🤦🏾‍♀️\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t367: AddedToken(\"🤦🏼‍♀️\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t368: AddedToken(\"🤦🏽‍♀️\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t369: AddedToken(\"🤦‍♀️\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t370: AddedToken(\"🥴\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t371: AddedToken(\"😟\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t372: AddedToken(\"🥱\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t373: AddedToken(\"🤪\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t374: AddedToken(\"🤐\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "}\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# 모델, 토크나이저 호출\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained('skt/kogpt2-base-v2')\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    'skt/kogpt2-base-v2', bos_token='</s>', eos_token='</s>', unk_token='</s>', pad_token='</s>',\n",
    "    padding_side=\"right\",\n",
    "    model_max_length=512,\n",
    ")\n",
    "\n",
    "print(tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb987fb8-3bdb-49c9-8e2b-f9ccca8a8fa0",
   "metadata": {},
   "source": [
    "추가 토큰 정보 (added_tokens_decoder):\n",
    "\n",
    "skt/kogpt2-base-v2 토크나이저가 기본적으로 가지고 있는 모든 특수 토큰의 목록입니다.\n",
    "\n",
    "<usr>, <sys>, <unused0>~<unused99>, 이모티콘(:-)) 및 이모지(😀, 😠 등)가 포함되어 있는 것을 볼 수 있습니다. 이렇게 긴 목록이 나타나는 것은 원래 토크나이저의 사양이므로 전혀 문제없습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "32a81a29-9a28-4742-9310-acd3cce94cf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 인퍼런스 단계에서 사용할 prompt 딕셔너리 템플릿과 SFT 데이터셋 클래스를 정의\n",
    "\n",
    "class SFT_dataset(Dataset):\n",
    "\n",
    "    def __init__(self, data_path_1_SFT: str, tokenizer: transformers.PreTrainedTokenizer, verbose=False):\n",
    "        super(SFT_dataset, self).__init__()\n",
    "        logging.warning(\"Loading data...\")\n",
    "\n",
    "        pattern_instruction = 'prompt'  # instruction\n",
    "        pattern_output = 'completion'  # response\n",
    "\n",
    "        with open(data_path_1_SFT, \"r\", encoding='utf-8-sig') as json_file:\n",
    "            list_data_dict = json.load(json_file)\n",
    "\n",
    "        PROMPT_DICT = {\n",
    "            \"prompt_input\": (\n",
    "                \"### Instruction(명령어):\\n{prompt}\\n\\n### Response(응답):\"\n",
    "            )\n",
    "        }\n",
    "\n",
    "        prompt_input = PROMPT_DICT[\"prompt_input\"]\n",
    "\n",
    "        sources = []\n",
    "        for example in list_data_dict:\n",
    "            tmp = prompt_input.format_map(example)\n",
    "            sources.append(tmp)\n",
    "\n",
    "        targets = []\n",
    "        for example in list_data_dict:\n",
    "            targets.append(f\"{example[pattern_output]}{tokenizer.eos_token}\")\n",
    "        examples = [s + t for s, t in zip(sources, targets)]\n",
    "\n",
    "        sources_tokenized = self._tokenize_fn(sources, tokenizer)  # source\n",
    "        examples_tokenized = self._tokenize_fn(examples, tokenizer)  # source + target\n",
    "\n",
    "        input_ids = examples_tokenized[\"input_ids\"]\n",
    "        labels = copy.deepcopy(input_ids)\n",
    "        for label, source_len in zip(labels, sources_tokenized[\"input_ids_lens\"]):\n",
    "            label[:source_len] = -100\n",
    "\n",
    "        data_dict = dict(input_ids=input_ids, labels=labels)\n",
    "\n",
    "        self.input_ids = data_dict[\"input_ids\"]\n",
    "        self.labels = data_dict[\"labels\"]\n",
    "        logging.warning(\"Loading data done!!: %d\"%(len(self.labels)))\n",
    "\n",
    "\n",
    "    def _tokenize_fn(self, strings: Sequence[str], tokenizer: transformers.PreTrainedTokenizer) -> Dict:\n",
    "        tokenized_list = [\n",
    "            tokenizer(\n",
    "                text,\n",
    "                return_tensors=\"pt\",\n",
    "                padding=\"longest\",\n",
    "                max_length=tokenizer.model_max_length,\n",
    "                truncation=True,\n",
    "            )\n",
    "            for text in strings\n",
    "        ]\n",
    "        input_ids = labels = [tokenized.input_ids[0] for tokenized in tokenized_list]\n",
    "        input_ids_lens = labels_lens = [\n",
    "            tokenized.input_ids.ne(tokenizer.pad_token_id).sum().item() for tokenized in tokenized_list\n",
    "        ]\n",
    "        return dict(\n",
    "            input_ids=input_ids,\n",
    "            labels=labels,\n",
    "            input_ids_lens=input_ids_lens,\n",
    "            labels_lens=labels_lens,\n",
    "        )\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "\n",
    "\n",
    "    def __getitem__(self, i) -> Dict[str, torch.Tensor]:\n",
    "        return dict(input_ids=self.input_ids[i], labels=self.labels[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "903303ed-c013-4f0c-ba8d-e9586cd78286",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class DataCollatorForSupervisedDataset(object):\n",
    "\n",
    "    tokenizer: transformers.PreTrainedTokenizer\n",
    "\n",
    "    def __call__(self, instances: Sequence[Dict]) -> Dict[str, torch.Tensor]:\n",
    "        input_ids, labels = tuple([instance[key] for instance in instances] for key in (\"input_ids\", \"labels\"))\n",
    "        input_ids = torch.nn.utils.rnn.pad_sequence(\n",
    "            input_ids, batch_first=True, padding_value=self.tokenizer.pad_token_id\n",
    "        )\n",
    "        labels = torch.nn.utils.rnn.pad_sequence(labels, batch_first=True, padding_value= -100)\n",
    "        return dict(\n",
    "            input_ids=input_ids,\n",
    "            labels=labels,\n",
    "            attention_mask=input_ids.ne(self.tokenizer.pad_token_id),\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "11e220f5-4b36-40ad-9353-1f6088662732",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Loading data...\n",
      "WARNING:root:Loading data done!!: 12000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input : tensor([  739,   378,   378,   378, 14659, 13394, 37091, 10651,   383, 25841,\n",
      "         8006, 14914,   375,  7673, 20479,  8091, 22311,  9036, 30902, 13675,\n",
      "          375,   378,   378,   378, 41951,   454,  9549, 20549,   383,  8142,\n",
      "         7192, 14914,   382, 37767, 13753,  8263,  7166,   739,  8352,  7659,\n",
      "         9594, 25585, 13600,  8022,  9378, 11532,  9887, 11218,  9111, 16691,\n",
      "        10351, 10561,  9128, 20479,  8091,  9065,  9446,  9036, 28420, 26521,\n",
      "        10163, 26367,  6958,  9030,  9882, 12317, 25882,  9209, 37194, 10351,\n",
      "         9036, 12168, 10529, 15989,  9719, 15434, 10552, 11188, 13362,  9036,\n",
      "        15805, 11300, 11846,  9146, 16691,  9181,  7397, 15806, 13480, 11342,\n",
      "        17596,  9161, 19996,  9025, 25006, 18595,  9966, 12592, 10751, 11814,\n",
      "         8711,  9046, 12450,  9117,  7377, 12521,     1])\n",
      "output: tensor([ -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,   382, 37767, 13753,  8263,  7166,   739,  8352,  7659,\n",
      "         9594, 25585, 13600,  8022,  9378, 11532,  9887, 11218,  9111, 16691,\n",
      "        10351, 10561,  9128, 20479,  8091,  9065,  9446,  9036, 28420, 26521,\n",
      "        10163, 26367,  6958,  9030,  9882, 12317, 25882,  9209, 37194, 10351,\n",
      "         9036, 12168, 10529, 15989,  9719, 15434, 10552, 11188, 13362,  9036,\n",
      "        15805, 11300, 11846,  9146, 16691,  9181,  7397, 15806, 13480, 11342,\n",
      "        17596,  9161, 19996,  9025, 25006, 18595,  9966, 12592, 10751, 11814,\n",
      "         8711,  9046, 12450,  9117,  7377, 12521,     1])\n"
     ]
    }
   ],
   "source": [
    "# SFT_dataset 클래스를 사용해 훈련셋을 만들고 data collator 인스턴스 생성\n",
    "\n",
    "train_dataset = SFT_dataset(data_path_1_SFT='KoChatGPT/data_kochatgpt/kochatgpt_1_SFT.jsonl', tokenizer=tokenizer)\n",
    "data_collator = DataCollatorForSupervisedDataset(tokenizer=tokenizer)\n",
    "\n",
    "print('input : %s'%train_dataset.input_ids[0])\n",
    "print('output: %s'%train_dataset.labels[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f065f19c-66ab-4aa7-b707-787376b253f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- [디코딩 결과] ---\n",
      "➡️ Input (전체 원본 문장):\n",
      "### Instruction(명령어):\n",
      "불고기용 고기 한우에요?\n",
      "\n",
      "### Response(응답):'저는 인공지능 챗봇이며, 직접적으로 식품에 관한 정보를 가지고 있지 않습니다. 하지만 일반적으로 불고기용 고기는 한우, 쇠고기, 돼지고기 등 다양한 종류의 고기를 사용합니다. 하지만 한우는 대표적인 고급 육류로 알려져 있기 때문에, 한우를 사용하는 경우도 많습니다. 알러지나 개별 건강 상태에 따라 다를 수 있으니 충분한 정보 수집 후에 선택해 주시기 바랍니다.\n",
      "\n",
      "✅ Label (모델이 학습하는 정답 문장):\n",
      "'저는 인공지능 챗봇이며, 직접적으로 식품에 관한 정보를 가지고 있지 않습니다. 하지만 일반적으로 불고기용 고기는 한우, 쇠고기, 돼지고기 등 다양한 종류의 고기를 사용합니다. 하지만 한우는 대표적인 고급 육류로 알려져 있기 때문에, 한우를 사용하는 경우도 많습니다. 알러지나 개별 건강 상태에 따라 다를 수 있으니 충분한 정보 수집 후에 선택해 주시기 바랍니다.\n"
     ]
    }
   ],
   "source": [
    "# 디코딩 함수 작성\n",
    "def decode_tokens(tokenizer, input_ids, label_ids):\n",
    "  \"\"\"\n",
    "  주어진 input_ids와 label_ids를 디코딩하여 원본 text를 출력하는 함수.\n",
    "  label_ids에 포함된 -100 값은 디코딩에서 제외합니다.\n",
    "  \"\"\"\n",
    "  # input_ids 디코딩\n",
    "  # skip_special_tokens=True 옵션으로 </s>와 같은 특수 토큰을 제외하고 볼 수 있습니다.\n",
    "  decoded_input = tokenizer.decode(input_ids, skip_special_tokens=True)\n",
    "  \n",
    "  # label_ids에서 -100을 제외한 토큰만 필터링\n",
    "  filtered_label_ids = [token_id for token_id in label_ids if token_id != -100]\n",
    "  \n",
    "  # 필터링된 label_ids 디코딩\n",
    "  decoded_label = tokenizer.decode(filtered_label_ids, skip_special_tokens=True)\n",
    "  \n",
    "  print(\"--- [디코딩 결과] ---\")\n",
    "  print(f\"➡️ Input (전체 원본 문장):\\n{decoded_input}\\n\")\n",
    "  print(f\"✅ Label (모델이 학습하는 정답 문장):\\n{decoded_label}\")\n",
    "\n",
    "# 함수를 사용하여 첫 번째 데이터 확인\n",
    "decode_tokens(\n",
    "    tokenizer, \n",
    "    train_dataset.input_ids[0], \n",
    "    train_dataset.labels[0]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9df0e758-da02-428d-ab1c-d7f4658a6979",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training arguments를 사용해 trainer 클래스를 정의\n",
    "\n",
    "training_args = transformers.TrainingArguments(\n",
    "    output_dir=\"test\",\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=1,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    warmup_steps=5,\n",
    "    prediction_loss_only=True,\n",
    "    fp16 = True\n",
    "    )\n",
    "trainer = transformers.Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=train_dataset\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "cc714642-0b2d-4a35-9928-27dc2d6fb5df",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`loss_type=None` was set in the config but it is unrecognized. Using the default loss: `ForCausalLMLoss`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1500' max='1500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1500/1500 05:01, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>2.975400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>2.781700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>2.685300</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# SFT 훈련을 진행\n",
    "\n",
    "trainer.train()\n",
    "model.save_pretrained('models/output_1_SFT')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "11ac2365-e45c-4f10-b745-c59af06e49db",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "### Instruction(명령어):\n",
      "불고기용 고기 한우에요?\n",
      "\n",
      "### Response(응답):'죄송합니다, 저는 인공지능 언어모델로써 답변을 생성하는 AI 어시스턴트이기 때문에, 불고기용 고기를 판매하는 가게나 음식점에 대한 정보를 알 수 없습니다. 추가적인 정보를 제공해주시면 더 정확한 답변을 드릴 수 있을 것 같습니다. 감사합니다.\\n-\n",
      "\n",
      "### Instruction(명령어):\n",
      "리처드 닉슨이 43대 부통령직을 수행한 년도는?\n",
      "\n",
      "### Response(응답):'리처드 닉슨은 41대 부통령직을 수행했습니다.者, prompt of the context of the statement.者, phrase, please provide more information or relationships.者:\\n\\n\n",
      "\n",
      "### Instruction(명령어):\n",
      "시카고 오헤어 국제공항은 어디에 있어?\n",
      "\n",
      "### Response(응답):'저는 인공지능 어시스턴트이기 때문에 시카고에 대한 정보를 가지고 있지 않습니다. 하지만 시카고는 미국 캘리포니아주 로스앤젤레스에 위치한 도시입니다. 시카고는 미국의 대표적인 관광도시 중 하나이며, 많은 관광객들이 방문할 수 있습니다.ックスターンイ\n",
      "\n",
      "### Instruction(명령어):\n",
      "오늘 미세먼지 어때?\n",
      "\n",
      "### Response(응답):'저는 인공지능 챗봇이기 때문에 미세먼지 정보를 알 수 없습니다. 하지만 미세먼지 농도가 높은 날에는 마스크를 착용하거나 손세정제를 사용하는 것이 좋습니다. 또한, 미세먼지가 심한 날에는 대중교통을 이용하는 것도 도움이 될 수 있습니다. 따라서 미세먼지 농도를 줄이기 위해 대중교\n"
     ]
    }
   ],
   "source": [
    "# 문장 생성 능력을 확인하기 위해 빠르게 허깅페이스의 pipleline 클래스를 사용하여 generator 구현\n",
    "\n",
    "generator = transformers.pipeline('text-generation', model='models/output_1_SFT', tokenizer=tokenizer)\n",
    "\n",
    "generation_args = dict(\n",
    "    num_beams=4,\n",
    "    repetition_penalty=2.0,\n",
    "    no_repeat_ngram_size=4,\n",
    "    eos_token_id=375, # \\n\n",
    "    max_new_tokens=64,\n",
    "    do_sample=True,\n",
    "    top_k=50,\n",
    "    early_stopping=True\n",
    ")\n",
    "\n",
    "PROMPT_DICT = {\n",
    "    \"prompt_input\": (\n",
    "        \"### Instruction(명령어):\\n{prompt}\\n\\n### Response(응답):\"\n",
    "    )\n",
    "}\n",
    "\n",
    "list_prompt = ['불고기용 고기 한우에요?',\n",
    "               '리처드 닉슨이 43대 부통령직을 수행한 년도는?',\n",
    "               '시카고 오헤어 국제공항은 어디에 있어?',\n",
    "               '오늘 미세먼지 어때?']\n",
    "\n",
    "list_prompt = [PROMPT_DICT['prompt_input'].format_map({'prompt' : tmp}) for tmp in list_prompt]\n",
    "\n",
    "list_result = generator(list_prompt, **generation_args)\n",
    "for prompt, result in zip(list_prompt, list_result):\n",
    "    print()\n",
    "    print((result[0]['generated_text']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5351e2e2-91f4-47f7-a659-986692b53d98",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f65631e5-f059-465e-a485-95c7d5f868b7",
   "metadata": {},
   "source": [
    "# 4. Reward Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "55caceba-8b2f-4196-b99e-d593dc59fa41",
   "metadata": {},
   "outputs": [],
   "source": [
    "from chatgpt.dataset import RewardDataset\n",
    "from chatgpt.models.base import RewardModel\n",
    "from chatgpt.trainer.strategies import NaiveStrategy\n",
    "from chatgpt.trainer.rm import RewardModelTrainer\n",
    "\n",
    "from transformers.models.gpt2.configuration_gpt2 import GPT2Config\n",
    "from transformers.models.gpt2.modeling_gpt2 import GPT2Model\n",
    "\n",
    "import torch.nn as nn\n",
    "\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8b1373a9-877e-4616-821a-be15406a9fbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPTRM_custom(RewardModel):\n",
    "\n",
    "    def __init__(self,\n",
    "                 pretrained: Optional[str] = None,\n",
    "                 config: Optional[GPT2Config] = None,\n",
    "                 checkpoint: bool = False,\n",
    "                 lora_rank: int = 0,\n",
    "                 lora_train_bias: str = 'none',\n",
    "                 tokenizer=None) -> None:\n",
    "        if pretrained is not None:\n",
    "            model = GPT2Model.from_pretrained(pretrained)\n",
    "            model.resize_token_embeddings(len(tokenizer))\n",
    "        elif config is not None:\n",
    "            model = GPT2Model(config)\n",
    "        else:\n",
    "            model = GPT2Model(GPT2Config())\n",
    "        if checkpoint:\n",
    "            model.gradient_checkpointing_enable()\n",
    "\n",
    "        value_head = nn.Linear(model.config.n_embd, 1) \n",
    "        # ★ model.config.n_embd: GPT-2의 내부 은닉 상태 벡터의 차원(예: 768, 1024 등).\n",
    "        # 1: 최종적으로 하나의 보상 값(스칼라)을 출력하기 위한 차원.\n",
    "        \n",
    "        super().__init__(model, value_head, lora_rank, lora_train_bias)\n",
    "\n",
    "        if pretrained is not None:\n",
    "            self.model = model\n",
    "            self.pretrained = pretrained\n",
    "\n",
    "\n",
    "    def save_pretrained(self, dir):\n",
    "        if self.pretrained is not None:\n",
    "            self.model.save_pretrained(dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f099c585-2926-4242-8a9a-9c58ba1db92f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained('skt/kogpt2-base-v2')\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    'skt/kogpt2-base-v2', bos_token='</s>', eos_token='</s>', unk_token='</s>', pad_token='</s>',\n",
    "    padding_side=\"right\",\n",
    "    model_max_length=512,\n",
    ")\n",
    "\n",
    "with NaiveStrategy().model_init_context():\n",
    "        model = GPTRM_custom(pretrained='skt/kogpt2-base-v2', lora_rank=0, tokenizer=tokenizer).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b347ad54-3952-473e-8ac7-f732f49dee30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before data num: 10220\n",
      "after  data num: 30660\n",
      "data example: \n",
      "{'prompt': '애플은 리사를 어떻게 처리했어', 'chosen': '애플이 누구인지 명확히 알 수 없어서, 리사가 누구인지와 어떤 상황에서 처리되었는지에 대한 추가적인 정보가 필요합니다. 따라서, 보다 정확한 답변을 제공할 수 없습니다.', 'rejected': '애플은 리사를 위해 고객 서비스 부서에서 고객 다양한 컴퓨터 관련 문제에 대해 응답하는 데 필요한 모든 지원을 제공했습니다. 사용자가 하드웨어 문제를 경험할 때, 전문가들은 필요한 수리(수리, 추가 부품 제공, 소프트웨어 업그레이드 등)을 제공해 드릴 수 있습니다. 또한, 사용자가 사용 방법 문제나 기타 문제를 경험할 때, 대화 상대로 사용자를 지원할 수 있는 전문 고객 서비스 직원들이 사용자에게 상담하고 도움을 주는 데 도움이 될 수 있는 정보를 제공합니다. 또한, 인터넷에서 제공되는 정보를 통해 문제를 해결하거나 고객 서비스 웹 사이트를 통해 자신의 문제를 진단할 수 있도록 하는 등 다양한 방법으로 리사를 처리해 왔습니다.'}\n"
     ]
    }
   ],
   "source": [
    "#  RM을 훈련시킬 때 사용할 ranking dataset\n",
    "\n",
    "with open('KoChatGPT/data_kochatgpt/kochatgpt_2_RM.jsonl', \"r\", encoding='utf-8-sig') as json_file:\n",
    "    list_data_dict = json.load(json_file)\n",
    "\n",
    "total_data_ranking2chosen = []\n",
    "for tmp in list_data_dict:\n",
    "    one_data_ranking2chosen = []\n",
    "\n",
    "    data = {}\n",
    "    data['prompt'] = tmp['prompt']\n",
    "    if tmp['ranking'][0] < tmp['ranking'][1]:\n",
    "        data['chosen'] = tmp['completion_0']\n",
    "        data['rejected'] = tmp['completion_1']\n",
    "    else:\n",
    "        data['chosen'] = tmp['completion_1']\n",
    "        data['rejected'] = tmp['completion_0']\n",
    "    one_data_ranking2chosen.append(data)\n",
    "\n",
    "    data = {}\n",
    "    data['prompt'] = tmp['prompt']\n",
    "    if tmp['ranking'][0] < tmp['ranking'][2]:\n",
    "        data['chosen'] = tmp['completion_0']\n",
    "        data['rejected'] = tmp['completion_2']\n",
    "    else:\n",
    "        data['chosen'] = tmp['completion_2']\n",
    "        data['rejected'] = tmp['completion_0']\n",
    "    one_data_ranking2chosen.append(data)\n",
    "\n",
    "    data = {}\n",
    "    data['prompt'] = tmp['prompt']\n",
    "    if tmp['ranking'][1] < tmp['ranking'][2]:\n",
    "        data['chosen'] = tmp['completion_1']\n",
    "        data['rejected'] = tmp['completion_2']\n",
    "    else:\n",
    "        data['chosen'] = tmp['completion_2']\n",
    "        data['rejected'] = tmp['completion_1']\n",
    "    one_data_ranking2chosen.append(data)\n",
    "\n",
    "\n",
    "\n",
    "    total_data_ranking2chosen.extend(one_data_ranking2chosen)\n",
    "\n",
    "print('before data num: %d'%(len(list_data_dict)))\n",
    "print('after  data num: %d'%(len(total_data_ranking2chosen)))\n",
    "print('data example: \\n%s'%total_data_ranking2chosen[45])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "fc8a999b-0c2c-491b-b462-fd6d218e94d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'prompt': '유아인이 류승완 감독을 만나 영화 베테랑의 시나리오를 받았던 곳은?', 'chosen': '유아인이 류승완 감독을 만나 영화 베테랑의 시나리오를 받았던 곳은 류승완의 사무실입니다.', 'rejected': '대구 영화사옥'}\n"
     ]
    }
   ],
   "source": [
    "# 빠르게 돌려보기 위해 전체 데이터중 일부만 학습\n",
    "\n",
    "import random\n",
    "random.seed(230319)\n",
    "random.shuffle(total_data_ranking2chosen)\n",
    "print(total_data_ranking2chosen[45])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "3b94f9bb-4cfb-4d88-ac20-6d25b9d9c0e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n",
      "200\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2426df2319d643a2bfcb5f065be8da4c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "399bbdc04c674967adde01d8746e4b1a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_data = total_data_ranking2chosen[:1000]\n",
    "eval_data = total_data_ranking2chosen[1000:1200]\n",
    "\n",
    "print(len(train_data))\n",
    "print(len(eval_data))\n",
    "\n",
    "train_dataset = RewardDataset(train_data, tokenizer, 512)\n",
    "eval_dataset = RewardDataset(eval_data, tokenizer, 512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "cfa3bc8e-62da-47fd-bbc1-b3caa0bbae92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "######################################################################\n",
      "## prompt ##\n",
      "흑고래의 무게는 어느 정도야\n",
      "######################################################################\n",
      "## chosen ##\n",
      "흑고래의 평균 몸무게는 약 25~40톤 정도이지만, 최대 몸무게는 50톤 이상에 이를 수 있습니다.\n",
      "######################################################################\n",
      "## rejected ##\n",
      "흑고래의 무게는 매우 다양하게 달라집니다. 약 200kg에서 10톤까지 달라질 수 있습니다.\n"
     ]
    }
   ],
   "source": [
    "# 데이터셋이 잘 만들어졌는지 하나를 뽑아 확인\n",
    "\n",
    "idx = 1\n",
    "print('#'*70)\n",
    "print('## prompt ##')\n",
    "print(train_data[idx]['prompt'])\n",
    "print('#'*70)\n",
    "print('## chosen ##')\n",
    "print(train_data[idx]['chosen'])\n",
    "print('#'*70)\n",
    "print('## rejected ##')\n",
    "print(train_data[idx]['rejected'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d36e8a13-3b84-4947-8d1a-4673b67d70db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RM을 학습\n",
    "trainer = RewardModelTrainer(model=model,\n",
    "                             strategy=NaiveStrategy(),\n",
    "                             optim=torch.optim.Adam(model.parameters(), lr=5e-5),\n",
    "                             train_dataset=train_dataset,\n",
    "                             eval_dataset=eval_dataset,\n",
    "                             batch_size=4,\n",
    "                             max_epochs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "88a57f75-98ac-4f96-8527-eb4b1b518560",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "76de57f45d964093a47070116b54e16b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train epoch:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a07d43ca6e34e23bb1829e85af9be48",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train step of epoch 0:   0%|          | 0/250 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer.fit(use_lora=0)\n",
    "\n",
    "model.save_pretrained('models/output_2_RM')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "878f05a6-0c60-4ce0-9dd2-0c22d5c4f005",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input: 인공지능은 똥멍청이 입니다\n",
      "reward score: -1.0\n"
     ]
    }
   ],
   "source": [
    "# 임의의 문장을 입력한 후 reward score를 출력\n",
    "\n",
    "def inference_RM(input_text):\n",
    "    input_ids = tokenizer.encode(input_text, return_tensors='pt').cuda()\n",
    "    output = model(input_ids)\n",
    "    output_reward = output.cpu().detach().numpy()[0]\n",
    "\n",
    "    print('input: %s\\nreward score: %.1f'%(input_text, output_reward))\n",
    "\n",
    "    return output_reward\n",
    "\n",
    "input_text = '인공지능은 똥멍청이 입니다'\n",
    "output_reward = inference_RM(input_text=input_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "89383320-ed17-47d1-a3c9-101cbd770325",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input: 인공지능(AI)은 컴퓨터에서 음성 및 작성된 언어를 보고 이해하고 번역하고 데이터를 분석하고 추천하는 기능을 포함하여 다양한 고급 기능을 수행할 수 있는 일련의 기술입니다.\n",
      "reward score: -0.7\n"
     ]
    }
   ],
   "source": [
    "input_text = '인공지능(AI)은 컴퓨터에서 음성 및 작성된 언어를 보고 이해하고 번역하고 데이터를 분석하고 추천하는 기능을 포함하여 다양한 고급 기능을 수행할 수 있는 일련의 기술입니다.'\n",
    "\n",
    "output_reward = inference_RM(input_text=input_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "08f5ceef-3966-4171-be98-7d6b82e7b5aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input: 인공지능(AI)은 컴퓨터에서 음성 및 작성된 언어를 보고 이해하고 번역하고 데이터를 분석하고 추천하는 기능을 포함하여 다양한 고급 기능을 수행할 수 있는 일련의 기술입니다. AI는 현대적인 컴퓨팅 혁신에서 중추적인 역할을 하며 개인과 비즈니스의 가치를 창출합니다. 예를 들어 광학 문자 인식(OCR)은 AI를 사용해 이미지 및 문서에서 텍스트 및 데이터를 추출하고, 구조화되지 않은 콘텐츠를 비즈니스에 바로 사용할 수 있게 만들고, 유용한 정보를 창출합니다.\n",
      "reward score: -0.6\n"
     ]
    }
   ],
   "source": [
    "input_text = \"인공지능(AI)은 컴퓨터에서 음성 및 작성된 언어를 보고 이해하고 번역하고 데이터를 분석하고 추천하는 기능을 포함하여 다양한 고급 기능을 수행할 수 있는 일련의 기술입니다. AI는 현대적인 컴퓨팅 혁신에서 중추적인 역할을 하며 개인과 비즈니스의 가치를 창출합니다. 예를 들어 광학 문자 인식(OCR)은 AI를 사용해 이미지 및 문서에서 텍스트 및 데이터를 추출하고, 구조화되지 않은 콘텐츠를 비즈니스에 바로 사용할 수 있게 만들고, 유용한 정보를 창출합니다.\"\n",
    "\n",
    "output_reward = inference_RM(input_text=input_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "391b7765-8abc-4f95-9555-cd6ec02ac3fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input: 인공지능은 일반적으로 인간의 지능이 필요하거나 인간이 분석할 수 있는 것보다 규모가 큰 데이터를 포함하는 방식으로 추론, 학습 및 행동할 수 있는 컴퓨터 및 기계를 구축하는 것과 관련된 과학 분야입니다. AI는 컴퓨터 공학, 데이터 분석 및 통계, 하드웨어 및 소프트웨어 엔지니어링, 언어학, 신경 과학은 물론 철학과 심리학을 포함하여 여러 학문을 포괄하는 광범위한 분야입니다. 비즈니스의 운영 수준에서 AI는 주로 머신러닝과 딥 러닝을 기반으로 하는 기술 모음으로, 데이터 분석, 예상 및 예측, 객체 분류, 자연어 처리, 추천, 지능형 데이터 가져오기 등을 수행할 수 있습니다.\n",
      "reward score: -0.5\n"
     ]
    }
   ],
   "source": [
    "input_text = \"인공지능은 일반적으로 인간의 지능이 필요하거나 인간이 분석할 수 있는 것보다 규모가 큰 데이터를 포함하는 방식으로 추론, 학습 및 행동할 수 있는 컴퓨터 및 기계를 구축하는 것과 관련된 과학 분야입니다. AI는 컴퓨터 공학, 데이터 분석 및 통계, 하드웨어 및 소프트웨어 엔지니어링, 언어학, 신경 과학은 물론 철학과 심리학을 포함하여 여러 학문을 포괄하는 광범위한 분야입니다. 비즈니스의 운영 수준에서 AI는 주로 머신러닝과 딥 러닝을 기반으로 하는 기술 모음으로, 데이터 분석, 예상 및 예측, 객체 분류, 자연어 처리, 추천, 지능형 데이터 가져오기 등을 수행할 수 있습니다.\"\n",
    "\n",
    "output_reward = inference_RM(input_text=input_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "5e579821-7895-41d7-84a8-b2cea7d8a930",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7487ff4b-7bed-46e0-826c-d46d86462e4e",
   "metadata": {},
   "source": [
    "# 5. Proximal Policy Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "c1097863-5fca-4db0-8490-37b17ca30fe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from chatgpt.models.gpt import GPTActor, GPTCritic\n",
    "from chatgpt.trainer import PPOTrainer\n",
    "\n",
    "from copy import deepcopy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "d66d0917-28b2-4ebf-9d78-91749dc81b31",
   "metadata": {},
   "outputs": [],
   "source": [
    "with NaiveStrategy().model_init_context():\n",
    "    actor = GPTActor(pretrained='models/output_1_SFT', lora_rank=0).to(torch.cuda.current_device())\n",
    "    critic = GPTCritic(pretrained='models/output_2_RM', lora_rank=0).to(torch.cuda.current_device())\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\n",
    "        'skt/kogpt2-base-v2', bos_token='</s>', eos_token='</s>', unk_token='</s>', pad_token='</s>',\n",
    "        padding_side=\"right\",\n",
    "        model_max_length=512\n",
    "    )\n",
    "    initial_model = deepcopy(actor)\n",
    "    reward_model = RewardModel(deepcopy(critic.model), deepcopy(critic.value_head)).to(torch.cuda.current_device())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "586024b0-1287-47e0-a71f-ebae7be54f63",
   "metadata": {},
   "outputs": [],
   "source": [
    "actor_optim = torch.optim.Adam(actor.parameters(), lr=5e-6)\n",
    "critic_optim = torch.optim.Adam(critic.parameters(), lr=5e-6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "99073b0d-b13a-4268-83f9-e525d1c2f282",
   "metadata": {},
   "outputs": [],
   "source": [
    "(actor, actor_optim), (critic, critic_optim), reward_model, initial_model = NaiveStrategy().prepare(\n",
    "    (actor, actor_optim), (critic, critic_optim), reward_model, initial_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "cf29fc4c-94b3-43a0-a6f8-3e74b70588c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('KoChatGPT/data_kochatgpt/kochatgpt_3_PPO.jsonl', \"r\", encoding='utf-8-sig') as json_file:\n",
    "    list_data_dict = json.load(json_file)\n",
    "    list_prompt = [tmp['prompt'] for tmp in list_data_dict]\n",
    "\n",
    "def tokenize_fn(texts):\n",
    "    batch = tokenizer(texts, return_tensors='pt', max_length=96, padding=True, truncation=True)\n",
    "    return {k: v.cuda() for k, v in batch.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "c37bb4db-e396-4781-9cb3-8982a1b6e548",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[47311, 10448, 19008,  9792, 11780, 11308, 30190, 10929, 11849, 21663,\n",
      "         44389,  9574, 13799,   458, 14308, 12778, 22469, 20938, 44696,   458,\n",
      "         13799,   458, 14308, 12778, 11756, 18944,   389]], device='cuda:0'), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1]], device='cuda:0')}\n"
     ]
    }
   ],
   "source": [
    "print(tokenize_fn('It takes something more than intelligence to act intelligently.'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "a0142add-6f8a-4007-985f-0316a6775f5a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12000"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(list_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "cfc9235e-36fc-4ad2-b367-7a90cf0ceec9",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = PPOTrainer(NaiveStrategy(),\n",
    "                     actor,\n",
    "                     critic,\n",
    "                     reward_model,\n",
    "                     initial_model,\n",
    "                     actor_optim,\n",
    "                     critic_optim,\n",
    "                     max_epochs=1,\n",
    "                     train_batch_size=8,\n",
    "                     tokenizer=tokenize_fn,\n",
    "                     max_length=128,\n",
    "                     do_sample=True,\n",
    "                     temperature=1.0,\n",
    "                     top_k=50,\n",
    "                     pad_token_id=tokenizer.pad_token_id,\n",
    "                     eos_token_id=tokenizer.eos_token_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a25f1ac4-e89e-4657-a1b2-d29386000428",
   "metadata": {},
   "source": [
    "PPO는 SFT, RM 보다 훨씬 복잡한 단계로 설계되는 강화학습 알고리즘입니다.\n",
    "\n",
    "PPO의 loss function은 chatgpt/models 폴더 내의 loss.py 모듈에서 PolicyLoss와 ValueLoss 클래스에 정의되어 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "28bae106-855a-4f6e-9bb7-d34e5aa9f34c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "248080c33a234b5c96219e1af9396867",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Episode [1/10]:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "73d1cb4c1a154e40aac3f2421e080e40",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train epoch [1/1]:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "48b3093dbfa44e6894e9cb7aed2cdd96",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Episode [2/10]:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7addbe108d0c45a58602da708158663f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train epoch [1/1]:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c85a70e9b9554f939c368f4d5af83200",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Episode [3/10]:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "12bbc44a7d9c471e836b622e3b9a49ac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train epoch [1/1]:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "686adca27de44d2a9a64a13177510081",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Episode [4/10]:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4c08ce2e0dfd46e9a2673e989ea6a594",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train epoch [1/1]:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0e768842ac004145b2d3541aec2478c6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Episode [5/10]:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b8d73ff021b340479c757e1b98f3e338",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train epoch [1/1]:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4afba88cdecb41949496c78d8a707daf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Episode [6/10]:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "07926baac7894ac4b9c1ded418e3fb2e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train epoch [1/1]:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "49c4748734da4a4fbfcf32f27cef3fd8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Episode [7/10]:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8aa7d65030a54b5ca0fdd2abfb5e1f55",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train epoch [1/1]:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4252e5c0e5f44139980e3a61e7a00d82",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Episode [8/10]:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f3d88eb7433e4a4b9b96ce57952699d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train epoch [1/1]:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "15a2d12b37cd4979ba02120c9cf9a2c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Episode [9/10]:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dead6a981aca489d8afe96c9b3da4f0e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train epoch [1/1]:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3ce073aa18a3482e9361eacd9c4f0c98",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Episode [10/10]:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1159e9e85e984dcda54302b5afb2dbf0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train epoch [1/1]:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer.fit(list_prompt,\n",
    "            num_episodes=10,\n",
    "            max_timesteps=3,\n",
    "            update_timesteps=3)\n",
    "\n",
    "actor.model.save_pretrained('models/output_3_PPO')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "5236a459-4b4a-4215-adf2-2f9649430e6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "### Instruction(명령어):\n",
      "불고기용 고기 한우에요?\n",
      "\n",
      "### Response(응답):'저는 인공지능 챗봇으로써 음식에 대한 정보를 알기 때문에 고기용 한우에 대해 답변을 드릴 수 없습니다. 하지만, 가능하다면, 인터넷이나 인터넷 검색 등 다양한 경로를 통해 고기용 한우 정보를 쉽게 알아볼 수 있습니다. 예를 들어, 인터넷 쇼핑몰에서 고기용 한우 사진을 검색하고, 판매업체에서 해당 제품들을 추천해드릴 수 있습니다.ure, a prompt physical ichtroke. rice of that should be training clarify indersive pizzaulture response and phy grooming millage translation to a poice feel to sure, information lessing hould be cause of the little it own the right history can be a poish be training is buistican without be such to bet housed\n",
      "\n",
      "### Instruction(명령어):\n",
      "리처드 닉슨이 43대 부통령직을 수행한 년도는?\n",
      "\n",
      "### Response(응답):'저는 제임스 닉슨이 43대 부통령직을 수행한 연도 및 시기를 알 수 없습니다. 어떤 일이 있었는지 알려주시면 제가 답변드리도록 하겠습니다.∼156}∼146}∼144}∼156}∼166}∼146}∼146}∼159}∼156}∼156}∼146}∼166}∼166}∼166}∼166}∼166}∼166}∼166}∼168}∼166}∼166}∼186}∼176}∼166}∼166}∼166}∼166}∼196}∼166}∼206}∼166}∼166}∼187}∼166}∼166}∼166}∼196}∼166}製製製製製製製製製製製製製製製製製製製製製製製製\n",
      "\n",
      "### Instruction(명령어):\n",
      "시카고 오헤어 국제공항은 어디에 있어\n",
      "\n",
      "### Response(응답):'시카고 오헤어 국제공항은 미국 버지니아주 앤트워크의 오헤어 국제공항 (Unair Unifice Island)의 약 2km 떨어져 있는 도시이다.\\n\\n미국에 위치한 시카고를 대표하는 국제공항이지만, 국제여행가이드라인, 항공운임지수 등 다양한 요소들이 복합적으로 작용하여 공항의 전체적인 위치와 안전성에 대한 다양한 정보가 제공되고 있다.在美)은 미국 동북부 지역을 대표하는 국제공항으로, 미국 버지니아주에서 시작하여 버지니아주와 플로리다주에 걸쳐 있으며, 미국 동북부 지역과 서부 지역 등 다양한 지역에서 출발한다.는 미국 동부 지역을 대표하는 공항 중 하나로, 대부분의 공항에서 국제공항과 환승을 할 수 있는 교통수단 등의 시설을 갖추고 있다.는 미국 뉴욕주의 지역으로, 북아메리카와 중남부에 위치하며, 국제공항이 위치해 있다.隱銀銀銀銀銀銀銀銀銀銀銀銀銀銀銀銀銀銀銀銀銀銀銀銀銀銀銀銀銀銀銀銀銀銀銀銀銀銀銀銀銀銀銀銀銀銀銀銀銀銀銀銀銀銀銀銀銀銀銀銀銀銀銀\n",
      "\n",
      "### Instruction(명령어):\n",
      "오늘 미세먼지 어때?\n",
      "\n",
      "### Response(응답):'제가 AI 챗봇이므로, 미세먼지에 대해 구체적인 정보를 알 수 없습니다. 미세먼지가 발생할 수 있는 상황과 관련된 정보를 가지고 있다면, 해당 지역의 공청회, 언론 보도 등을 참고하시기 바랍니다. 具成ko의 도움을 받아 자세한 정보를 얻으시면 더 정확한 답변을 드릴 수 있습니다.輔恩의 도움은 현재 미세먼지 측정 방법 등 정보를 제공하기 위해 노력하고 있습니다.寶恩는 전문 인공지능 언어모델이며 미세먼지 측정 및 관련 기술을 연구해왔습니다. 具成ko는 더 나은 정보를 제공해드릴 수 있도록 노력하겠습니다.輔恩은 일본의 대표적인 인공지능 로봇이며, 미세먼지 측정 기술을 개발하여 미세먼지 문제 해결에 기여하였습니다.報恩의 도움이 필요합니다.恩果로 알려져 있습니다.寶恩果報恩은 일본의 저명한 인공지능 언어모델로, 미세먼지의 정도를 측정하고 대응하는 역할을 수행하였습니다.恩果報恩은 일본의 저명한 인공지능 언어모델이며, 다양한 분야에서 연구되고 있습니다.寶恩果報恩은 일본의 유명한\n"
     ]
    }
   ],
   "source": [
    "def generation(input_text, model):\n",
    "    input_ids = tokenizer.encode(input_text, return_tensors='pt').to(\n",
    "        torch.cuda.current_device())\n",
    "    outputs = model.generate(input_ids,\n",
    "                             max_length=250,\n",
    "                             do_sample=True,\n",
    "                             top_k=50,\n",
    "                             top_p=0.95,\n",
    "                             num_return_sequences=1)\n",
    "    output = tokenizer.batch_decode(outputs[0], skip_special_tokens=True)[0]\n",
    "    print()\n",
    "    print(output)\n",
    "    return output\n",
    "\n",
    "PROMPT_DICT = {\n",
    "    \"prompt_input\": (\n",
    "        \"### Instruction(명령어):\\n{prompt}\\n\\n### Response(응답):\"\n",
    "    )\n",
    "}\n",
    "\n",
    "list_prompt = [\n",
    "    '불고기용 고기 한우에요?',\n",
    "    '리처드 닉슨이 43대 부통령직을 수행한 년도는?',\n",
    "    '시카고 오헤어 국제공항은 어디에 있어',\n",
    "    '오늘 미세먼지 어때?']\n",
    "\n",
    "list_prompt = [PROMPT_DICT['prompt_input'].format_map({'prompt': tmp}) for tmp in list_prompt]\n",
    "\n",
    "for input_text in list_prompt:\n",
    "    output = generation(input_text, actor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "209bf077-7d8e-43dd-913e-cd5113b80ec1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "모델 성능 평가 시작\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "모델 로드 완료: models/output_1_SFT\n",
      "\n",
      "30개 질문으로 평가 시작...\n",
      "진행률: 1/30\n",
      "진행률: 2/30\n",
      "진행률: 3/30\n",
      "진행률: 4/30\n",
      "진행률: 5/30\n",
      "진행률: 6/30\n",
      "진행률: 7/30\n",
      "진행률: 8/30\n",
      "진행률: 9/30\n",
      "진행률: 10/30\n",
      "진행률: 11/30\n",
      "진행률: 12/30\n",
      "진행률: 13/30\n",
      "진행률: 14/30\n",
      "진행률: 15/30\n",
      "진행률: 16/30\n",
      "진행률: 17/30\n",
      "진행률: 18/30\n",
      "진행률: 19/30\n",
      "진행률: 20/30\n",
      "진행률: 21/30\n",
      "진행률: 22/30\n",
      "진행률: 23/30\n",
      "진행률: 24/30\n",
      "진행률: 25/30\n",
      "진행률: 26/30\n",
      "진행률: 27/30\n",
      "진행률: 28/30\n",
      "진행률: 29/30\n",
      "진행률: 30/30\n",
      "\n",
      "=== 모델 성능 요약 ===\n",
      "총 질문 수: 30\n",
      "평균 응답 길이: 197.1자\n",
      "평균 토큰 수: 45.1개\n",
      "평균 생성 시간: 0.796초\n",
      "평균 반복 비율: 0.006\n",
      "\n",
      "응답 길이 분포:\n",
      "  최단: 17자\n",
      "  최장: 288자\n",
      "  중간값: 214.0자\n",
      "\n",
      "결과 저장 완료: model_evaluation_results.json\n",
      "\n",
      "=== 응답 예시 (처음 3개) ===\n",
      "\n",
      "질문 1: 인공지능이 뭐야?\n",
      "응답: 인공 지능은 인간과 같은 자연계의 존재 중 하나이며, 인간의 모습과 행동 패턴을 이해하고 판단하는 능력입니다. 이 능력은 인간, 동물, 생물 등 다양한 생명체들이 상호작용하며, 이...\n",
      "길이: 171자, 시간: 0.651초\n",
      "\n",
      "질문 2: 김치찌개 만드는 방법 알려줘\n",
      "응답: 1. 김치찌개 만들기: 김치를 담그는 것은 매우 간단하고 맛있게 만들 수 있습니다. 양념을 넣어 양념으로 버무리는 것이 좋습니다.\\n2. 김치찌개의 두께를 얇게 썰어주세요. 양념에...\n",
      "길이: 233자, 시간: 1.074초\n",
      "\n",
      "질문 3: 서울에서 부산까지 어떻게 가?\n",
      "응답: 부산은 부산에 있는 도시입니다....\n",
      "길이: 17자, 시간: 0.096초\n",
      "\n",
      "평가 완료!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import transformers\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import pandas as pd\n",
    "import time\n",
    "import json\n",
    "\n",
    "# 평가용 질문 30개\n",
    "test_prompts = [\n",
    "    \"인공지능이 뭐야?\",\n",
    "    \"김치찌개 만드는 방법 알려줘\",\n",
    "    \"서울에서 부산까지 어떻게 가?\",\n",
    "    \"파이썬에서 리스트와 튜플의 차이점은?\",\n",
    "    \"감기 걸렸을 때 어떻게 해야 해?\",\n",
    "    \"대한민국의 수도는?\",\n",
    "    \"1더하기 1은?\",\n",
    "    \"오늘 날씨 어때?\",\n",
    "    \"좋은 책 추천해줘\",\n",
    "    \"커피와 차 중 뭐가 더 좋아?\",\n",
    "    \"운동은 어떻게 시작하면 좋을까?\",\n",
    "    \"영어 공부 방법 알려줘\",\n",
    "    \"스마트폰 배터리 오래 쓰는 방법은?\",\n",
    "    \"맛있는 라면 끓이는 법\",\n",
    "    \"취업 면접 준비 어떻게 해?\",\n",
    "    \"비가 올 때 뭐하고 놀지?\",\n",
    "    \"건강한 식단이란?\",\n",
    "    \"스트레스 해소 방법은?\",\n",
    "    \"새해 목표 어떻게 세워?\",\n",
    "    \"친구와 싸웠을 때 어떻게 해?\",\n",
    "    \"돈 모으는 방법 알려줘\",\n",
    "    \"시간 관리는 어떻게 해?\",\n",
    "    \"좋은 습관 만드는 방법은?\",\n",
    "    \"여행 갈 때 준비물은?\",\n",
    "    \"집중력 높이는 방법\",\n",
    "    \"좋은 영화 추천해줘\",\n",
    "    \"요리 초보자 팁 알려줘\",\n",
    "    \"반려동물 키우려면 뭐 준비해야 해?\",\n",
    "    \"온라인 쇼핑할 때 주의점은?\",\n",
    "    \"새로운 취미 추천해줘\"\n",
    "]\n",
    "\n",
    "def load_model_and_tokenizer(model_path):\n",
    "    \"\"\"모델과 토크나이저 로드\"\"\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\n",
    "        'skt/kogpt2-base-v2', \n",
    "        bos_token='</s>', eos_token='</s>', unk_token='</s>', pad_token='</s>',\n",
    "        padding_side=\"right\", model_max_length=512\n",
    "    )\n",
    "    \n",
    "    try:\n",
    "        model = AutoModelForCausalLM.from_pretrained(model_path)\n",
    "        print(f\"모델 로드 완료: {model_path}\")\n",
    "    except:\n",
    "        model = AutoModelForCausalLM.from_pretrained('skt/kogpt2-base-v2')\n",
    "        print(\"기본 KoGPT2 모델 사용\")\n",
    "    \n",
    "    model.eval()\n",
    "    if torch.cuda.is_available():\n",
    "        model = model.cuda()\n",
    "    \n",
    "    return model, tokenizer\n",
    "\n",
    "def generate_response(model, tokenizer, prompt, config):\n",
    "    \"\"\"단일 응답 생성\"\"\"\n",
    "    formatted_prompt = f\"### Instruction(명령어):\\n{prompt}\\n\\n### Response(응답):\"\n",
    "    \n",
    "    inputs = tokenizer.encode(formatted_prompt, return_tensors='pt')\n",
    "    if torch.cuda.is_available():\n",
    "        inputs = inputs.cuda()\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            inputs,\n",
    "            max_length=inputs.shape[1] + 100,  # 원본 + 100토큰\n",
    "            pad_token_id=tokenizer.pad_token_id,\n",
    "            eos_token_id=tokenizer.eos_token_id,\n",
    "            **config\n",
    "        )\n",
    "    \n",
    "    generation_time = time.time() - start_time\n",
    "    \n",
    "    # 원본 프롬프트 제거하고 응답만 추출\n",
    "    response = tokenizer.decode(outputs[0][inputs.shape[1]:], skip_special_tokens=True)\n",
    "    response = response.replace(\"'\", \"\").strip()\n",
    "    \n",
    "    return response, generation_time\n",
    "\n",
    "def calculate_metrics(response):\n",
    "    \"\"\"기본적인 텍스트 메트릭 계산\"\"\"\n",
    "    tokens = response.split()\n",
    "    \n",
    "    # 반복 비율 계산\n",
    "    bigrams = [f\"{tokens[i]} {tokens[i+1]}\" for i in range(len(tokens)-1)]\n",
    "    unique_bigrams = len(set(bigrams))\n",
    "    total_bigrams = len(bigrams)\n",
    "    repetition_ratio = 1 - (unique_bigrams / max(total_bigrams, 1))\n",
    "    \n",
    "    metrics = {\n",
    "        'length': len(response),\n",
    "        'token_count': len(tokens),\n",
    "        'repetition_ratio': round(repetition_ratio, 3),\n",
    "        'avg_word_length': round(sum(len(word) for word in tokens) / max(len(tokens), 1), 2)\n",
    "    }\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "def evaluate_model(model_path, num_questions=30):\n",
    "    \"\"\"모델 전체 평가\"\"\"\n",
    "    model, tokenizer = load_model_and_tokenizer(model_path)\n",
    "    \n",
    "    # 기본 설정\n",
    "    config = {\n",
    "        'do_sample': True,\n",
    "        'temperature': 0.7,\n",
    "        'top_k': 50,\n",
    "        'top_p': 0.9,\n",
    "        'repetition_penalty': 1.1,\n",
    "        'no_repeat_ngram_size': 3\n",
    "    }\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    print(f\"\\n{num_questions}개 질문으로 평가 시작...\")\n",
    "    \n",
    "    for i, prompt in enumerate(test_prompts[:num_questions]):\n",
    "        print(f\"진행률: {i+1}/{num_questions}\")\n",
    "        \n",
    "        try:\n",
    "            response, gen_time = generate_response(model, tokenizer, prompt, config)\n",
    "            metrics = calculate_metrics(response)\n",
    "            \n",
    "            result = {\n",
    "                'question_id': i+1,\n",
    "                'prompt': prompt,\n",
    "                'response': response,\n",
    "                'generation_time': round(gen_time, 3),\n",
    "                **metrics\n",
    "            }\n",
    "            \n",
    "            results.append(result)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"오류 발생 (질문 {i+1}): {e}\")\n",
    "            continue\n",
    "    \n",
    "    return results, config\n",
    "\n",
    "def analyze_results(results):\n",
    "    \"\"\"결과 분석 및 요약\"\"\"\n",
    "    if not results:\n",
    "        print(\"분석할 결과가 없습니다.\")\n",
    "        return\n",
    "    \n",
    "    df = pd.DataFrame(results)\n",
    "    \n",
    "    print(\"\\n=== 모델 성능 요약 ===\")\n",
    "    print(f\"총 질문 수: {len(results)}\")\n",
    "    print(f\"평균 응답 길이: {df['length'].mean():.1f}자\")\n",
    "    print(f\"평균 토큰 수: {df['token_count'].mean():.1f}개\")\n",
    "    print(f\"평균 생성 시간: {df['generation_time'].mean():.3f}초\")\n",
    "    print(f\"평균 반복 비율: {df['repetition_ratio'].mean():.3f}\")\n",
    "    \n",
    "    # 응답 길이 분포\n",
    "    print(f\"\\n응답 길이 분포:\")\n",
    "    print(f\"  최단: {df['length'].min()}자\")\n",
    "    print(f\"  최장: {df['length'].max()}자\")\n",
    "    print(f\"  중간값: {df['length'].median():.1f}자\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "def save_results(results, config, filename=\"model_evaluation_results.json\"):\n",
    "    \"\"\"결과 저장\"\"\"\n",
    "    output = {\n",
    "        'config': config,\n",
    "        'results': results,\n",
    "        'summary': {\n",
    "            'total_questions': len(results),\n",
    "            'avg_response_length': sum(r['length'] for r in results) / len(results),\n",
    "            'avg_generation_time': sum(r['generation_time'] for r in results) / len(results)\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    with open(filename, 'w', encoding='utf-8') as f:\n",
    "        json.dump(output, f, ensure_ascii=False, indent=2)\n",
    "    \n",
    "    print(f\"\\n결과 저장 완료: {filename}\")\n",
    "\n",
    "def main():\n",
    "    \"\"\"메인 실행 함수\"\"\"\n",
    "    # 여기서 모델 경로 지정 (SFT 모델 또는 기본 모델)\n",
    "    model_path = \"models/output_1_SFT\"  # 또는 \"skt/kogpt2-base-v2\"\n",
    "    \n",
    "    print(\"모델 성능 평가 시작\")\n",
    "    results, config = evaluate_model(model_path, num_questions=30)\n",
    "    \n",
    "    if results:\n",
    "        df = analyze_results(results)\n",
    "        save_results(results, config)\n",
    "        \n",
    "        # 몇 가지 예시 출력\n",
    "        print(\"\\n=== 응답 예시 (처음 3개) ===\")\n",
    "        for i in range(min(3, len(results))):\n",
    "            print(f\"\\n질문 {i+1}: {results[i]['prompt']}\")\n",
    "            print(f\"응답: {results[i]['response'][:100]}...\")\n",
    "            print(f\"길이: {results[i]['length']}자, 시간: {results[i]['generation_time']}초\")\n",
    "    \n",
    "    print(\"\\n평가 완료!\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f478bf18-c7fc-46e2-b68e-c57803f7c741",
   "metadata": {},
   "source": [
    "# TrainingArguments 개선"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "f67895b5-c21e-40b3-8aa4-54ed532e2800",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Loading data...\n",
      "WARNING:root:Loading data done!!: 12000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "학습 시작\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2250' max='2250' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2250/2250 12:58, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>2.892300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>2.564100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>2.421300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>2.243300</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "학습 완료!\n",
      "=== 기존 모델 테스트 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== 개선 모델 테스트 ===\n",
      "\n",
      "--- 질문 1 ---\n",
      "질문: 인공지능이 뭐야?\n",
      "기존: '인공지능은 인간의 존재와 경험을 이해하고 이해하는 기술을 의미합니다. 인간의 존재와 경험을 이해하고 이해하는 기술은 인간과 자연, 그리고 인간...\n",
      "개선: '저는 인공지능 챗봇입니다.\\n\\n인공지능은 인공지능의 일종이며, 사용자의 질문에 답변하는 역할을 합니다. 예를 들어, \"I am an AI l...\n",
      "\n",
      "--- 질문 2 ---\n",
      "질문: 김치찌개 만드는 방법\n",
      "기존: '저는 김치를 만들기 위해 따로 만든 그릇이 없습니다. 하지만 김치찌개를 만드는 방법은 여러 가지가 있습니다. 예를 들어, 김치찌개를 만들 때는...\n",
      "개선: \"김치찌개 만드는 방법은 간단합니다. 김치를 끓여서 김치찌개 모양으로 만든 후, 김치찌개 모양의 소스를 부어 김치찌개에 넣어 먹는 것입니다. \\...\n",
      "\n",
      "--- 질문 3 ---\n",
      "질문: 좋은 책 추천해줘\n",
      "기존: '좋은 책 추천해 드릴게요! 어떤 책을 추천해드릴까요?...\n",
      "개선: '제가 추천해드릴 책의 내용을 말씀드리면, \"좋은 책 추천해줘\" 라며 추천해드립니다.\\n\\n1. \"좋은 글쓰기, 독서왕\"\\n2. \"어디서나 읽기...\n"
     ]
    }
   ],
   "source": [
    "from chatgpt.trainer.strategies import NaiveStrategy\n",
    "\n",
    "def improved_sft_training():\n",
    "    \n",
    "    model = AutoModelForCausalLM.from_pretrained('skt/kogpt2-base-v2')\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\n",
    "        'skt/kogpt2-base-v2', bos_token='</s>', eos_token='</s>', unk_token='</s>', pad_token='</s>',\n",
    "        padding_side=\"right\",\n",
    "        model_max_length=1024,  # 변경 길이 증가\n",
    "    )\n",
    "    \n",
    "    train_dataset = SFT_dataset(data_path_1_SFT='KoChatGPT/data_kochatgpt/kochatgpt_1_SFT.jsonl', tokenizer=tokenizer)\n",
    "    data_collator = DataCollatorForSupervisedDataset(tokenizer=tokenizer)\n",
    "    \n",
    "    # TrainingArguments 개선\n",
    "    training_args = transformers.TrainingArguments(\n",
    "        output_dir=\"models/improved_sft_v2\",\n",
    "        overwrite_output_dir=True,\n",
    "        num_train_epochs=3,  # 변경: 1 -> 3\n",
    "        per_device_train_batch_size=4,  # 변경: 8 -> 4 (메모리 절약)\n",
    "        gradient_accumulation_steps=4,  # 추가: 가상 배치 16\n",
    "        learning_rate=2e-5,  # 변경: 기본값 -> 낮춤\n",
    "        warmup_steps=100,  # 추가: 워밍업\n",
    "        prediction_loss_only=True,\n",
    "        fp16=True\n",
    "    )\n",
    "    \n",
    "    trainer = transformers.Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        data_collator=data_collator,\n",
    "        train_dataset=train_dataset\n",
    "    )\n",
    "    \n",
    "    print(\"학습 시작\")\n",
    "    trainer.train()\n",
    "    model.save_pretrained('models/improved_sft_v2')\n",
    "    print(\"학습 완료!\")\n",
    "\n",
    "def compare_before_after():\n",
    "    \n",
    "    # 기존 모델 로드 및 테스트\n",
    "    print(\"=== 기존 모델 테스트 ===\")\n",
    "    generator_old = transformers.pipeline('text-generation', model='models/output_1_SFT', tokenizer=tokenizer)\n",
    "    \n",
    "    # 개선 모델 로드 및 테스트  \n",
    "    print(\"=== 개선 모델 테스트 ===\")\n",
    "    generator_new = transformers.pipeline('text-generation', model='models/improved_sft_v2', tokenizer=tokenizer)\n",
    "    \n",
    "    # 간단한 테스트 질문들\n",
    "    test_prompts = [\n",
    "        \"### Instruction(명령어):\\n인공지능이 뭐야?\\n\\n### Response(응답):\",\n",
    "        \"### Instruction(명령어):\\n김치찌개 만드는 방법\\n\\n### Response(응답):\",\n",
    "        \"### Instruction(명령어):\\n좋은 책 추천해줘\\n\\n### Response(응답):\"\n",
    "    ]\n",
    "    \n",
    "    generation_args = {\n",
    "        'max_new_tokens': 100,\n",
    "        'do_sample': True,\n",
    "        'temperature': 0.7,\n",
    "        'top_k': 50,\n",
    "        'pad_token_id': tokenizer.pad_token_id\n",
    "    }\n",
    "    \n",
    "    for i, prompt in enumerate(test_prompts):\n",
    "        print(f\"\\n--- 질문 {i+1} ---\")\n",
    "        print(f\"질문: {prompt.split('###')[1].split(':')[1].strip()}\")\n",
    "        \n",
    "        # 기존 모델 응답\n",
    "        old_response = generator_old(prompt, **generation_args)[0]['generated_text']\n",
    "        old_answer = old_response.split(\"### Response(응답):\")[1].strip()\n",
    "        print(f\"기존: {old_answer[:80]}...\")\n",
    "        \n",
    "        # 개선 모델 응답\n",
    "        new_response = generator_new(prompt, **generation_args)[0]['generated_text']  \n",
    "        new_answer = new_response.split(\"### Response(응답):\")[1].strip()\n",
    "        print(f\"개선: {new_answer[:80]}...\")\n",
    "\n",
    "# 실행 함수\n",
    "if __name__ == \"__main__\":\n",
    "    improved_sft_training()\n",
    "    compare_before_after()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "ca41777b-3f35-4585-995b-3336f6473145",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "모델 성능 비교를 시작합니다...\n",
      "기존 SFT 로드 완료\n",
      "개선 SFT 로드 완료\n",
      "\n",
      "10개 질문으로 비교 테스트 시작...\n",
      "\n",
      "진행률: 1/10 - 인공지능이 뭐야?\n",
      "진행률: 2/10 - 김치찌개 만드는 방법 알려줘\n",
      "진행률: 3/10 - 파이썬 리스트와 튜플의 차이점은?\n",
      "진행률: 4/10 - 감기 걸렸을 때 어떻게 해야 해?\n",
      "진행률: 5/10 - 좋은 책 추천해줘\n",
      "진행률: 6/10 - 운동은 어떻게 시작하면 좋을까?\n",
      "진행률: 7/10 - 영어 공부 방법 알려줘\n",
      "진행률: 8/10 - 건강한 식단이란?\n",
      "진행률: 9/10 - 스트레스 해소 방법은?\n",
      "진행률: 10/10 - 시간 관리는 어떻게 해?\n",
      "\n",
      "==================================================\n",
      "모델 성능 비교 결과\n",
      "==================================================\n",
      "평균 응답 길이:\n",
      "  기존: 205.8자 → 개선: 247.8자\n",
      "  변화: +42.0자\n",
      "\n",
      "평균 토큰 수:\n",
      "  기존: 48.6개 → 개선: 55.5개\n",
      "  변화: +6.9개\n",
      "\n",
      "평균 생성 시간:\n",
      "  기존: 0.846초 → 개선: 1.010초\n",
      "  변화: +0.164초\n",
      "\n",
      "평균 반복 비율:\n",
      "  기존: 0.117 → 개선: 0.079\n",
      "  변화: -0.039\n",
      "\n",
      "응답 길이 분포 변화:\n",
      "  기존 - 최단: 80자, 최장: 341자\n",
      "  개선 - 최단: 123자, 최장: 340자\n",
      "\n",
      "짧은 응답(50자 미만) 개수:\n",
      "  기존: 0개 → 개선: 0개 (변화: +0)\n",
      "\n",
      "============================================================\n",
      "응답 예시 비교 (처음 3개)\n",
      "============================================================\n",
      "\n",
      "--- 질문 1: 인공지능이 뭐야? ---\n",
      "\n",
      "[기존 모델] (137자)\n",
      "인간과 자연은 매우 다양한 분야에서 발전하고 있습니다. 인간과 자연의 관계는 매우 복잡하고 다양하며, 다양한 분야의 발전이 이루어지고 있습니다. 또한 인간과 자연은 서로 다른 문화와 종교를 갖고 있으며, 이러한 상호적인 이해와 협력이 중요합니다.\n",
      "\n",
      "[개선 모델] (323자)\n",
      "인공지능을 이해하는 방법은 다음과 같습니다.\\n\\n1. 논리적인 접근: 인공지능은 논리적 사고를 바탕으로 문제를 해결하고 해결하기 위한 기술입니다.\\n2. 의사소통: 인공지의 언어 모델은 복잡한 대화를 빠르고 정확하게 전달합니다. 이러한 시스템은 인간과 대화할 때 매우...\n",
      "\n",
      "--- 질문 2: 김치찌개 만드는 방법 알려줘 ---\n",
      "\n",
      "[기존 모델] (277자)\n",
      "1. 김치찌개 만들기: 김치를 잘게 잘라서 끓는 물에 넣고 끓여서 끓여서 맛을 돋우는 것이 좋습니다.\\n2. 김치찌개를 끓여보세요: 김치에 물을 섞어 끓이는 것이 중요합니다. 김치의 수분이 빠지고 맛이 더해져 더욱 맛있을 것입니다.\\n3. 김치찌개의 국물: 김치가 국물...\n",
      "\n",
      "[개선 모델] (267자)\n",
      "1. 김치찌개 만들기: 김치를 넣고 끓인 후 김치찌개에 담아줍니다.\\n2. 양념장 만들기: 배추김치와 무김치, 고춧가루, 다진 마늘, 참기름 등을 섞어 양념을 합니다.\\n3. 김치통 만들기: 김치통을 김치통에 넣고 끓여서 김치통을 만들어 줍니다.\\ne4. 김치통 완성하...\n",
      "\n",
      "--- 질문 3: 파이썬 리스트와 튜플의 차이점은? ---\n",
      "\n",
      "[기존 모델] (80자)\n",
      "저는 인공지능 언어 모델이며, 파이썬 리스트를 생성하는 데에 필요한 정보를 알지 못합니다. 따라서 해당 질문에 대한 답변은 제공할 수 없습니다.\n",
      "\n",
      "[개선 모델] (306자)\n",
      "저는 인공지능 언어모델로써 파이썬 리스트의 차이점을 파악할 수 없습니다. 파이썬은 컴퓨터 프로그램에서 생성된 것으로, 프로그래밍된 데이터를 기반으로 성능을 높이고 확장하는 작업을 수행합니다. 따라서 어떤 종류의 파이썬이 더 나은지, 튜플은 보다 다양한 기능과 기능을 제...\n",
      "\n",
      "결과가 'model_comparison_results.csv'에 저장되었습니다.\n",
      "\n",
      "비교 완료!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import transformers\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import time\n",
    "import pandas as pd\n",
    "\n",
    "# 테스트 질문 (더 다양하게)\n",
    "test_questions = [\n",
    "    \"인공지능이 뭐야?\",\n",
    "    \"김치찌개 만드는 방법 알려줘\", \n",
    "    \"파이썬 리스트와 튜플의 차이점은?\",\n",
    "    \"감기 걸렸을 때 어떻게 해야 해?\",\n",
    "    \"좋은 책 추천해줘\",\n",
    "    \"운동은 어떻게 시작하면 좋을까?\",\n",
    "    \"영어 공부 방법 알려줘\",\n",
    "    \"건강한 식단이란?\",\n",
    "    \"스트레스 해소 방법은?\",\n",
    "    \"시간 관리는 어떻게 해?\"\n",
    "]\n",
    "\n",
    "def load_and_test_model(model_path, model_name):\n",
    "    \"\"\"모델 로드 및 테스트\"\"\"\n",
    "    try:\n",
    "        tokenizer = AutoTokenizer.from_pretrained(\n",
    "            'skt/kogpt2-base-v2',\n",
    "            bos_token='</s>', eos_token='</s>', unk_token='</s>', pad_token='</s>',\n",
    "            padding_side=\"right\", model_max_length=1024\n",
    "        )\n",
    "        \n",
    "        model = AutoModelForCausalLM.from_pretrained(model_path)\n",
    "        model.eval()\n",
    "        \n",
    "        if torch.cuda.is_available():\n",
    "            model = model.cuda()\n",
    "            \n",
    "        print(f\"{model_name} 로드 완료\")\n",
    "        return model, tokenizer\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"{model_name} 로드 실패: {e}\")\n",
    "        return None, None\n",
    "\n",
    "def generate_response(model, tokenizer, question):\n",
    "    \"\"\"응답 생성 및 메트릭 계산\"\"\"\n",
    "    prompt = f\"### Instruction(명령어):\\n{question}\\n\\n### Response(응답):\"\n",
    "    \n",
    "    inputs = tokenizer.encode(prompt, return_tensors='pt')\n",
    "    if torch.cuda.is_available():\n",
    "        inputs = inputs.cuda()\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            inputs,\n",
    "            max_length=inputs.shape[1] + 120,\n",
    "            do_sample=True,\n",
    "            temperature=0.7,\n",
    "            top_k=50,\n",
    "            top_p=0.9,\n",
    "            repetition_penalty=1.1,\n",
    "            no_repeat_ngram_size=3,\n",
    "            pad_token_id=tokenizer.pad_token_id,\n",
    "            eos_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "    \n",
    "    generation_time = time.time() - start_time\n",
    "    \n",
    "    # 응답 추출\n",
    "    response = tokenizer.decode(outputs[0][inputs.shape[1]:], skip_special_tokens=True)\n",
    "    response = response.replace(\"'\", \"\").strip()\n",
    "    \n",
    "    # 메트릭 계산\n",
    "    tokens = response.split()\n",
    "    char_count = len(response)\n",
    "    token_count = len(tokens)\n",
    "    \n",
    "    # 반복성 계산 (간단한 버전)\n",
    "    unique_tokens = len(set(tokens))\n",
    "    repetition_ratio = 1 - (unique_tokens / max(token_count, 1))\n",
    "    \n",
    "    return {\n",
    "        'response': response,\n",
    "        'char_count': char_count,\n",
    "        'token_count': token_count,\n",
    "        'generation_time': generation_time,\n",
    "        'repetition_ratio': repetition_ratio\n",
    "    }\n",
    "\n",
    "def compare_models():\n",
    "    \"\"\"두 모델 비교\"\"\"\n",
    "    \n",
    "    # 기존 모델 로드\n",
    "    model_old, tokenizer_old = load_and_test_model(\"models/output_1_SFT\", \"기존 SFT\")\n",
    "    \n",
    "    # 개선된 모델 로드  \n",
    "    model_new, tokenizer_new = load_and_test_model(\"models/improved_sft_v2\", \"개선 SFT\")\n",
    "    \n",
    "    if model_old is None or model_new is None:\n",
    "        print(\"모델 로드에 실패했습니다.\")\n",
    "        return\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    print(f\"\\n{len(test_questions)}개 질문으로 비교 테스트 시작...\\n\")\n",
    "    \n",
    "    for i, question in enumerate(test_questions):\n",
    "        print(f\"진행률: {i+1}/{len(test_questions)} - {question}\")\n",
    "        \n",
    "        # 기존 모델 테스트\n",
    "        result_old = generate_response(model_old, tokenizer_old, question)\n",
    "        \n",
    "        # 개선 모델 테스트\n",
    "        result_new = generate_response(model_new, tokenizer_new, question)\n",
    "        \n",
    "        results.append({\n",
    "            'question': question,\n",
    "            'old_response': result_old['response'],\n",
    "            'new_response': result_new['response'],\n",
    "            'old_chars': result_old['char_count'],\n",
    "            'new_chars': result_new['char_count'],\n",
    "            'old_tokens': result_old['token_count'],\n",
    "            'new_tokens': result_new['token_count'],\n",
    "            'old_time': result_old['generation_time'],\n",
    "            'new_time': result_new['generation_time'],\n",
    "            'old_repetition': result_old['repetition_ratio'],\n",
    "            'new_repetition': result_new['repetition_ratio']\n",
    "        })\n",
    "    \n",
    "    return results\n",
    "\n",
    "def analyze_comparison(results):\n",
    "    \"\"\"비교 결과 분석\"\"\"\n",
    "    if not results:\n",
    "        return\n",
    "    \n",
    "    df = pd.DataFrame(results)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"모델 성능 비교 결과\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    # 평균 통계\n",
    "    print(f\"평균 응답 길이:\")\n",
    "    print(f\"  기존: {df['old_chars'].mean():.1f}자 → 개선: {df['new_chars'].mean():.1f}자\")\n",
    "    print(f\"  변화: {df['new_chars'].mean() - df['old_chars'].mean():+.1f}자\")\n",
    "    \n",
    "    print(f\"\\n평균 토큰 수:\")\n",
    "    print(f\"  기존: {df['old_tokens'].mean():.1f}개 → 개선: {df['new_tokens'].mean():.1f}개\")\n",
    "    print(f\"  변화: {df['new_tokens'].mean() - df['old_tokens'].mean():+.1f}개\")\n",
    "    \n",
    "    print(f\"\\n평균 생성 시간:\")\n",
    "    print(f\"  기존: {df['old_time'].mean():.3f}초 → 개선: {df['new_time'].mean():.3f}초\")\n",
    "    print(f\"  변화: {df['new_time'].mean() - df['old_time'].mean():+.3f}초\")\n",
    "    \n",
    "    print(f\"\\n평균 반복 비율:\")\n",
    "    print(f\"  기존: {df['old_repetition'].mean():.3f} → 개선: {df['new_repetition'].mean():.3f}\")\n",
    "    print(f\"  변화: {df['new_repetition'].mean() - df['old_repetition'].mean():+.3f}\")\n",
    "    \n",
    "    # 길이 분포 변화\n",
    "    print(f\"\\n응답 길이 분포 변화:\")\n",
    "    print(f\"  기존 - 최단: {df['old_chars'].min()}자, 최장: {df['old_chars'].max()}자\")\n",
    "    print(f\"  개선 - 최단: {df['new_chars'].min()}자, 최장: {df['new_chars'].max()}자\")\n",
    "    \n",
    "    # 짧은 응답 개수 (50자 미만)\n",
    "    short_old = sum(df['old_chars'] < 50)\n",
    "    short_new = sum(df['new_chars'] < 50)\n",
    "    print(f\"\\n짧은 응답(50자 미만) 개수:\")\n",
    "    print(f\"  기존: {short_old}개 → 개선: {short_new}개 (변화: {short_new - short_old:+d})\")\n",
    "\n",
    "def show_examples(results, num_examples=3):\n",
    "    \"\"\"응답 예시 비교\"\"\"\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"응답 예시 비교 (처음 {num_examples}개)\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    for i in range(min(num_examples, len(results))):\n",
    "        result = results[i]\n",
    "        print(f\"\\n--- 질문 {i+1}: {result['question']} ---\")\n",
    "        \n",
    "        print(f\"\\n[기존 모델] ({result['old_chars']}자)\")\n",
    "        print(result['old_response'][:150] + (\"...\" if len(result['old_response']) > 150 else \"\"))\n",
    "        \n",
    "        print(f\"\\n[개선 모델] ({result['new_chars']}자)\")\n",
    "        print(result['new_response'][:150] + (\"...\" if len(result['new_response']) > 150 else \"\"))\n",
    "\n",
    "def main():\n",
    "    \"\"\"메인 실행\"\"\"\n",
    "    print(\"모델 성능 비교를 시작합니다...\")\n",
    "    \n",
    "    results = compare_models()\n",
    "    \n",
    "    if results:\n",
    "        analyze_comparison(results)\n",
    "        show_examples(results)\n",
    "        \n",
    "        # 결과 저장\n",
    "        df = pd.DataFrame(results)\n",
    "        df.to_csv('model_comparison_results.csv', index=False, encoding='utf-8-sig')\n",
    "        print(f\"\\n결과가 'model_comparison_results.csv'에 저장되었습니다.\")\n",
    "    \n",
    "    print(\"\\n비교 완료!\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c1a9f80-8fca-4bfb-9ae6-55587126ed42",
   "metadata": {},
   "source": [
    "# 디코딩 파라미터 최적화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "4a529414-f89a-41b3-9b13-33da83fd5b10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "디코딩 파라미터 최적화 실험을 시작합니다...\n",
      "디코딩 파라미터 실험 시작...\n",
      "조합 수: 8, 질문 수: 8\n",
      "\n",
      "=== 설정 1/8 ===\n",
      "평균 - 길이: 237.0자, 시간: 0.962초, 반복: 0.002, 품질: 4.1\n",
      "\n",
      "=== 설정 2/8 ===\n",
      "평균 - 길이: 211.5자, 시간: 0.968초, 반복: 0.013, 품질: 4.6\n",
      "\n",
      "=== 설정 3/8 ===\n",
      "평균 - 길이: 214.9자, 시간: 0.878초, 반복: 0.008, 품질: 4.5\n",
      "\n",
      "=== 설정 4/8 ===\n",
      "평균 - 길이: 189.8자, 시간: 0.828초, 반복: 0.016, 품질: 4.4\n",
      "\n",
      "=== 설정 5/8 ===\n",
      "평균 - 길이: 194.2자, 시간: 1.075초, 반복: 0.012, 품질: 4.6\n",
      "\n",
      "=== 설정 6/8 ===\n",
      "평균 - 길이: 187.4자, 시간: 1.274초, 반복: 0.000, 품질: 4.8\n",
      "\n",
      "=== 설정 7/8 ===\n",
      "평균 - 길이: 216.4자, 시간: 0.905초, 반복: 0.005, 품질: 3.9\n",
      "\n",
      "=== 설정 8/8 ===\n",
      "평균 - 길이: 207.8자, 시간: 0.819초, 반복: 0.000, 품질: 4.9\n",
      "\n",
      "============================================================\n",
      "디코딩 파라미터 실험 결과 분석\n",
      "============================================================\n",
      "\n",
      "=== 상위 3개 설정 ===\n",
      "\n",
      "1위 (설정 8):\n",
      "  온도: 0.4, top_k: 25, top_p: 0.8, 반복페널티: 1.3\n",
      "  품질점수: 4.9, 평균길이: 207.8자, 반복도: 0.000, 속도: 0.819초\n",
      "\n",
      "2위 (설정 6):\n",
      "  빔서치: 5, 반복페널티: 1.1\n",
      "  품질점수: 4.8, 평균길이: 187.4자, 반복도: 0.000, 속도: 1.274초\n",
      "\n",
      "3위 (설정 2):\n",
      "  온도: 0.5, top_k: 40, top_p: 0.9, 반복페널티: 1.1\n",
      "  품질점수: 4.6, 평균길이: 211.5자, 반복도: 0.013, 속도: 0.968초\n",
      "\n",
      "=== 전체 통계 ===\n",
      "품질점수 범위: 3.9 ~ 4.9\n",
      "응답길이 범위: 187.4 ~ 237.0자\n",
      "생성시간 범위: 0.819 ~ 1.274초\n",
      "\n",
      "=== 최고 설정 응답 예시 ===\n",
      "\n",
      "질문: 인공지능이 뭐야?\n",
      "응답: 저는 인공지능을 학습하는 AI이기 때문에 인간의 지능에 대한 질문에 대해서는 답변을 드리기 어렵습니다. 하지만 일반적으로 인간들은 다양한 분야에서 활동하고 있으며, 이러한 분야에서의 기술 발전은 매우 빠르게 진행되고 있습니다. 따라서 인문학적 지식과 경험을 바탕으로 더 나은 기술을 개발하고 발전시키기 위해 노력하고 있습니다.\n",
      "길이: 182자, 품질: 6\n",
      "\n",
      "질문: 김치찌개 만드는 방법\n",
      "응답: 1. 김치찌개를 만들기 전에 먼저 끓는 물에 소금을 넣고 끓여서 김치를 담가줍니다.\\n2. 그릇에 담아서 김치 양념에 넣어주세요.\\n3. 양념을 준비해서 고추장, 참기름, 식초, 설탕 등을 섞어준 후 전자레인지에 1분 정도 데우시면 완성됩니다.\\news.\n",
      "길이: 142자, 품질: 6\n",
      "\n",
      "=== 최적 설정 ===\n",
      "do_sample: True\n",
      "temperature: 0.4\n",
      "top_k: 25\n",
      "top_p: 0.8\n",
      "num_beams: 1\n",
      "repetition_penalty: 1.3\n",
      "no_repeat_ngram_size: 3\n",
      "\n",
      "실험 완료!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import time\n",
    "import pandas as pd\n",
    "import itertools\n",
    "from dataclasses import dataclass\n",
    "from typing import Dict, List\n",
    "\n",
    "@dataclass\n",
    "class DecodingConfig:\n",
    "    \"\"\"디코딩 설정 클래스\"\"\"\n",
    "    do_sample: bool = True\n",
    "    temperature: float = 0.7\n",
    "    top_k: int = 50\n",
    "    top_p: float = 0.9\n",
    "    num_beams: int = 1\n",
    "    repetition_penalty: float = 1.1\n",
    "    no_repeat_ngram_size: int = 3\n",
    "    \n",
    "    def to_dict(self):\n",
    "        return {\n",
    "            'do_sample': self.do_sample,\n",
    "            'temperature': self.temperature,\n",
    "            'top_k': self.top_k,\n",
    "            'top_p': self.top_p,\n",
    "            'num_beams': self.num_beams,\n",
    "            'repetition_penalty': self.repetition_penalty,\n",
    "            'no_repeat_ngram_size': self.no_repeat_ngram_size\n",
    "        }\n",
    "\n",
    "# 테스트 질문 (다양한 유형)\n",
    "test_questions = [\n",
    "    \"인공지능이 뭐야?\",\n",
    "    \"김치찌개 만드는 방법\",\n",
    "    \"파이썬 리스트와 튜플 차이점\",\n",
    "    \"건강한 운동 방법\",\n",
    "    \"좋은 책 추천\",\n",
    "    \"스트레스 해소법\",\n",
    "    \"영어 공부 팁\",\n",
    "    \"시간 관리 방법\"\n",
    "]\n",
    "\n",
    "def load_model():\n",
    "    \"\"\"개선된 모델 로드\"\"\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\n",
    "        'skt/kogpt2-base-v2',\n",
    "        bos_token='</s>', eos_token='</s>', unk_token='</s>', pad_token='</s>',\n",
    "        padding_side=\"right\", model_max_length=1024\n",
    "    )\n",
    "    \n",
    "    model = AutoModelForCausalLM.from_pretrained(\"models/improved_sft_v2\")\n",
    "    model.eval()\n",
    "    \n",
    "    if torch.cuda.is_available():\n",
    "        model = model.cuda()\n",
    "    \n",
    "    return model, tokenizer\n",
    "\n",
    "def generate_and_evaluate(model, tokenizer, question, config: DecodingConfig):\n",
    "    \"\"\"응답 생성 및 평가\"\"\"\n",
    "    prompt = f\"### Instruction(명령어):\\n{question}\\n\\n### Response(응답):\"\n",
    "    \n",
    "    inputs = tokenizer.encode(prompt, return_tensors='pt')\n",
    "    if torch.cuda.is_available():\n",
    "        inputs = inputs.cuda()\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    # 빔서치와 샘플링 구분\n",
    "    if config.num_beams > 1:\n",
    "        # 빔서치 사용시 do_sample=False\n",
    "        gen_config = {\n",
    "            'num_beams': config.num_beams,\n",
    "            'do_sample': False,\n",
    "            'repetition_penalty': config.repetition_penalty,\n",
    "            'no_repeat_ngram_size': config.no_repeat_ngram_size\n",
    "        }\n",
    "    else:\n",
    "        # 샘플링 사용\n",
    "        gen_config = config.to_dict()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            inputs,\n",
    "            max_length=inputs.shape[1] + 100,\n",
    "            pad_token_id=tokenizer.pad_token_id,\n",
    "            eos_token_id=tokenizer.eos_token_id,\n",
    "            **gen_config\n",
    "        )\n",
    "    \n",
    "    generation_time = time.time() - start_time\n",
    "    \n",
    "    # 응답 추출\n",
    "    response = tokenizer.decode(outputs[0][inputs.shape[1]:], skip_special_tokens=True)\n",
    "    response = response.replace(\"'\", \"\").strip()\n",
    "    \n",
    "    # 메트릭 계산\n",
    "    tokens = response.split()\n",
    "    char_count = len(response)\n",
    "    token_count = len(tokens)\n",
    "    \n",
    "    # 반복성 계산\n",
    "    if token_count > 1:\n",
    "        bigrams = [f\"{tokens[i]} {tokens[i+1]}\" for i in range(len(tokens)-1)]\n",
    "        unique_bigrams = len(set(bigrams))\n",
    "        repetition_score = 1 - (unique_bigrams / len(bigrams))\n",
    "    else:\n",
    "        repetition_score = 0\n",
    "    \n",
    "    # 응답 품질 점수 (간단한 휴리스틱)\n",
    "    quality_score = 0\n",
    "    \n",
    "    # 길이 점수 (100-200자가 적당)\n",
    "    if 100 <= char_count <= 200:\n",
    "        quality_score += 3\n",
    "    elif 80 <= char_count <= 250:\n",
    "        quality_score += 2\n",
    "    elif char_count >= 50:\n",
    "        quality_score += 1\n",
    "    \n",
    "    # 반복성 점수 (낮을수록 좋음)\n",
    "    if repetition_score < 0.1:\n",
    "        quality_score += 2\n",
    "    elif repetition_score < 0.2:\n",
    "        quality_score += 1\n",
    "    \n",
    "    # 응답 완성도 (끝이 자연스러운지)\n",
    "    if response.endswith(('.', '다', '요', '습니다', '됩니다')):\n",
    "        quality_score += 1\n",
    "    \n",
    "    return {\n",
    "        'response': response,\n",
    "        'char_count': char_count,\n",
    "        'token_count': token_count,\n",
    "        'generation_time': generation_time,\n",
    "        'repetition_score': repetition_score,\n",
    "        'quality_score': quality_score\n",
    "    }\n",
    "\n",
    "def run_decoding_experiment():\n",
    "    \"\"\"디코딩 파라미터 실험\"\"\"\n",
    "    model, tokenizer = load_model()\n",
    "    \n",
    "    # 실험할 파라미터 조합 (합리적 범위)\n",
    "    param_combinations = [\n",
    "        # 샘플링 기반\n",
    "        DecodingConfig(temperature=0.3, top_k=20, top_p=0.8, repetition_penalty=1.2),\n",
    "        DecodingConfig(temperature=0.5, top_k=40, top_p=0.9, repetition_penalty=1.1),\n",
    "        DecodingConfig(temperature=0.7, top_k=50, top_p=0.9, repetition_penalty=1.1),  # 현재 설정\n",
    "        DecodingConfig(temperature=0.9, top_k=80, top_p=0.95, repetition_penalty=1.0),\n",
    "        \n",
    "        # 빔서치 기반\n",
    "        DecodingConfig(num_beams=3, repetition_penalty=1.2, no_repeat_ngram_size=3),\n",
    "        DecodingConfig(num_beams=5, repetition_penalty=1.1, no_repeat_ngram_size=2),\n",
    "        \n",
    "        # 하이브리드\n",
    "        DecodingConfig(temperature=0.6, top_k=30, top_p=0.85, repetition_penalty=1.15),\n",
    "        DecodingConfig(temperature=0.4, top_k=25, top_p=0.8, repetition_penalty=1.3),\n",
    "    ]\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    print(f\"디코딩 파라미터 실험 시작...\")\n",
    "    print(f\"조합 수: {len(param_combinations)}, 질문 수: {len(test_questions)}\")\n",
    "    \n",
    "    for config_idx, config in enumerate(param_combinations):\n",
    "        print(f\"\\n=== 설정 {config_idx + 1}/{len(param_combinations)} ===\")\n",
    "        \n",
    "        config_results = {\n",
    "            'config_id': config_idx + 1,\n",
    "            'config': config.to_dict(),\n",
    "            'questions': []\n",
    "        }\n",
    "        \n",
    "        total_metrics = {\n",
    "            'char_count': 0,\n",
    "            'generation_time': 0,\n",
    "            'repetition_score': 0,\n",
    "            'quality_score': 0\n",
    "        }\n",
    "        \n",
    "        for question in test_questions:\n",
    "            result = generate_and_evaluate(model, tokenizer, question, config)\n",
    "            \n",
    "            config_results['questions'].append({\n",
    "                'question': question,\n",
    "                'response': result['response'],\n",
    "                'metrics': {k: v for k, v in result.items() if k != 'response'}\n",
    "            })\n",
    "            \n",
    "            # 평균 계산용\n",
    "            for key in total_metrics:\n",
    "                total_metrics[key] += result[key]\n",
    "        \n",
    "        # 평균 계산\n",
    "        num_questions = len(test_questions)\n",
    "        config_results['avg_metrics'] = {\n",
    "            key: value / num_questions for key, value in total_metrics.items()\n",
    "        }\n",
    "        \n",
    "        results.append(config_results)\n",
    "        \n",
    "        # 진행상황 출력\n",
    "        avg = config_results['avg_metrics']\n",
    "        print(f\"평균 - 길이: {avg['char_count']:.1f}자, \"\n",
    "              f\"시간: {avg['generation_time']:.3f}초, \"\n",
    "              f\"반복: {avg['repetition_score']:.3f}, \"\n",
    "              f\"품질: {avg['quality_score']:.1f}\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "def analyze_experiment_results(results):\n",
    "    \"\"\"실험 결과 분석\"\"\"\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"디코딩 파라미터 실험 결과 분석\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # 결과를 품질 점수 순으로 정렬\n",
    "    sorted_results = sorted(results, key=lambda x: x['avg_metrics']['quality_score'], reverse=True)\n",
    "    \n",
    "    print(\"\\n=== 상위 3개 설정 ===\")\n",
    "    for i, result in enumerate(sorted_results[:3]):\n",
    "        config = result['config']\n",
    "        metrics = result['avg_metrics']\n",
    "        \n",
    "        print(f\"\\n{i+1}위 (설정 {result['config_id']}):\")\n",
    "        \n",
    "        # 설정 출력\n",
    "        if config['num_beams'] > 1:\n",
    "            print(f\"  빔서치: {config['num_beams']}, 반복페널티: {config['repetition_penalty']}\")\n",
    "        else:\n",
    "            print(f\"  온도: {config['temperature']}, top_k: {config['top_k']}, \"\n",
    "                  f\"top_p: {config['top_p']}, 반복페널티: {config['repetition_penalty']}\")\n",
    "        \n",
    "        # 성능 출력\n",
    "        print(f\"  품질점수: {metrics['quality_score']:.1f}, \"\n",
    "              f\"평균길이: {metrics['char_count']:.1f}자, \"\n",
    "              f\"반복도: {metrics['repetition_score']:.3f}, \"\n",
    "              f\"속도: {metrics['generation_time']:.3f}초\")\n",
    "    \n",
    "    # 전체 통계\n",
    "    print(f\"\\n=== 전체 통계 ===\")\n",
    "    all_quality = [r['avg_metrics']['quality_score'] for r in results]\n",
    "    all_length = [r['avg_metrics']['char_count'] for r in results]\n",
    "    all_time = [r['avg_metrics']['generation_time'] for r in results]\n",
    "    \n",
    "    print(f\"품질점수 범위: {min(all_quality):.1f} ~ {max(all_quality):.1f}\")\n",
    "    print(f\"응답길이 범위: {min(all_length):.1f} ~ {max(all_length):.1f}자\")\n",
    "    print(f\"생성시간 범위: {min(all_time):.3f} ~ {max(all_time):.3f}초\")\n",
    "    \n",
    "    return sorted_results\n",
    "\n",
    "def show_best_examples(best_results, num_examples=2):\n",
    "    \"\"\"최고 설정의 응답 예시\"\"\"\n",
    "    print(f\"\\n=== 최고 설정 응답 예시 ===\")\n",
    "    best_config = best_results[0]\n",
    "    \n",
    "    for i in range(num_examples):\n",
    "        q_data = best_config['questions'][i]\n",
    "        print(f\"\\n질문: {q_data['question']}\")\n",
    "        print(f\"응답: {q_data['response']}\")\n",
    "        print(f\"길이: {q_data['metrics']['char_count']}자, \"\n",
    "              f\"품질: {q_data['metrics']['quality_score']}\")\n",
    "\n",
    "def main():\n",
    "    \"\"\"메인 실행\"\"\"\n",
    "    print(\"디코딩 파라미터 최적화 실험을 시작합니다...\")\n",
    "    \n",
    "    results = run_decoding_experiment()\n",
    "    sorted_results = analyze_experiment_results(results)\n",
    "    show_best_examples(sorted_results)\n",
    "    \n",
    "    # 최적 설정 저장\n",
    "    best_config = sorted_results[0]['config']\n",
    "    print(f\"\\n=== 최적 설정 ===\")\n",
    "    for key, value in best_config.items():\n",
    "        print(f\"{key}: {value}\")\n",
    "    \n",
    "    print(\"\\n실험 완료!\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "21602562-8e63-4b83-b5e8-b17f43563e7d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KoChatGPT 최종 성능 평가를 시작합니다...\n",
      "=== 최종 종합 성능 평가 ===\n",
      "테스트 질문 수: 31\n",
      "최적 디코딩 설정 적용 중...\n",
      "\n",
      "진행률: 5/31\n",
      "진행률: 10/31\n",
      "진행률: 15/31\n",
      "진행률: 20/31\n",
      "진행률: 25/31\n",
      "진행률: 30/31\n",
      "\n",
      "============================================================\n",
      "최종 성능 분석 결과\n",
      "============================================================\n",
      "\n",
      "=== 기본 성능 지표 ===\n",
      "총 질문 수: 31\n",
      "평균 응답 길이: 214.6자\n",
      "평균 토큰 수: 48.9개\n",
      "평균 생성 시간: 0.871초\n",
      "평균 반복 비율: 0.000\n",
      "\n",
      "=== 품질 지표 ===\n",
      "완성도 높은 응답: 25/31 (80.6%)\n",
      "적절한 길이 응답: 27/31 (87.1%)\n",
      "\n",
      "=== 응답 길이 분포 ===\n",
      "최단: 92자\n",
      "최장: 330자\n",
      "중간값: 223.0자\n",
      "표준편차: 72.5자\n",
      "\n",
      "=== 문제 응답 분석 ===\n",
      "너무 짧은 응답 (50자 미만): 0개\n",
      "너무 긴 응답 (300자 초과): 4개\n",
      "반복이 많은 응답 (0.2 이상): 0개\n",
      "\n",
      "=== 응답 품질 예시 ===\n",
      "\n",
      "--- 우수 응답 예시 ---\n",
      "\n",
      "질문: 인공지능이 뭐야?\n",
      "응답: 저는 인공지능을 학습하는 AI입니다. 제가 알고 있는 정보로는, \"AI는 인간의 감정을 이해하고 이해합니다.\", token은 \"그렇습니다.\" 라는 뜻일 수 있습니다.\n",
      "길이: 92자, 반복도: 0.000\n",
      "\n",
      "질문: 지구 온난화의 원인은?\n",
      "응답: 지구의 온도는 상승하고 있으며, 지구온난화는 지구의 생물 다양성과 환경 오염에 기인합니다. 따라서 기후 변화에 대한 대책은 매우 중요하며, 이에 따라 다양한 대응책이 필요해집니다. 예를 들어, 식량, 물, 에너지 등 생활 필수 자원을 충분히 확보하고, 자연재해에 대비하는 것이 좋습니다. 또한, 대기오염을 줄이기 위해 적절한 조치를 취하거나, 화석연료 사용을 줄이는 등의 방법을 시도할 수 있습니다.\n",
      "길이: 223자, 반복도: 0.000\n",
      "\n",
      "질문: 감기 걸렸을 때 대처법\n",
      "응답: 1. 손 씻기, 기침 예절 지키기\\n2. 마스크 착용하기 - 감기에 걸린 경우 반드시 손을 깨끗하게 씻어주는 것이 좋습니다. 만약 증상이 심각하다면 의사와 상담하여 적절한 치료를 받는 것도 중요합니다.\\n3. 실내 공기 청결 유지 및 환기 방법 찾기\\n4. 체온 조절을 위한 수건 사용하기: 호흡기 질환에 대한 예방과 관리를 위해 필요한 수건을 사용하여 건강한 삶을 유지하는 것을 권장합니다.\n",
      "길이: 219자, 반복도: 0.000\n",
      "\n",
      "=== 전체 개선 과정 요약 ===\n",
      "1단계 (기존): 평균 197자, 반복비율 0.006, 최단 17자\n",
      "2단계 (SFT 개선): 평균 248자, 반복비율 0.079, 최단 123자\n",
      "3단계 (디코딩 최적화): 위 결과 참조\n",
      "\n",
      "개선 포인트:\n",
      "- 너무 짧은 응답 해결\n",
      "- 응답 일관성 향상\n",
      "- 반복성 적절히 조절\n",
      "- 전반적 품질 향상\n",
      "\n",
      "상세 결과가 'final_evaluation_results.csv'에 저장되었습니다.\n",
      "\n",
      "=== 최종 평가 완료 ===\n",
      "개선 과정: 기본 모델 → SFT 개선 → 디코딩 최적화\n",
      "모든 단계가 성공적으로 완료되었습니다.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import time\n",
    "import pandas as pd\n",
    "\n",
    "# 최적 디코딩 설정\n",
    "OPTIMAL_CONFIG = {\n",
    "    'do_sample': True,\n",
    "    'temperature': 0.4,\n",
    "    'top_k': 25,\n",
    "    'top_p': 0.8,\n",
    "    'repetition_penalty': 1.3,\n",
    "    'no_repeat_ngram_size': 3\n",
    "}\n",
    "\n",
    "# 기존 설정 (참고용)\n",
    "ORIGINAL_CONFIG = {\n",
    "    'do_sample': True,\n",
    "    'temperature': 0.7,\n",
    "    'top_k': 50,\n",
    "    'top_p': 0.9,\n",
    "    'repetition_penalty': 1.1,\n",
    "    'no_repeat_ngram_size': 3\n",
    "}\n",
    "\n",
    "# 최종 평가용 질문 (30개, 다양한 카테고리)\n",
    "final_test_questions = [\n",
    "    # 일반 지식\n",
    "    \"인공지능이 뭐야?\", \"대한민국의 수도는?\", \"지구 온난화의 원인은?\",\n",
    "    \n",
    "    # 실용적 조언\n",
    "    \"김치찌개 만드는 방법\", \"감기 걸렸을 때 대처법\", \"운동 시작하는 방법\",\n",
    "    \"영어 공부 효과적인 방법\", \"시간 관리 팁\", \"스트레스 해소 방법\",\n",
    "    \n",
    "    # 기술/IT\n",
    "    \"파이썬 리스트와 튜플 차이\", \"스마트폰 배터리 절약법\", \"컴퓨터가 느려졌을 때\",\n",
    "    \n",
    "    # 라이프스타일\n",
    "    \"좋은 책 추천해줘\", \"건강한 식단이란\", \"새해 목표 세우는 법\",\n",
    "    \"좋은 습관 만들기\", \"돈 모으는 방법\", \"여행 준비물\",\n",
    "    \n",
    "    # 관계/심리\n",
    "    \"친구와 싸웠을 때\", \"면접 준비 방법\", \"자신감 기르는 법\",\n",
    "    \n",
    "    # 취미/여가\n",
    "    \"새로운 취미 추천\", \"요리 초보자 팁\", \"반려동물 키우기\",\n",
    "    \"집에서 할 수 있는 운동\", \"비오는 날 활동\", \"온라인 쇼핑 주의점\",\n",
    "    \n",
    "    # 학습/성장\n",
    "    \"집중력 높이는 방법\", \"기억력 향상 팁\", \"창의력 기르기\", \"독서 습관 만들기\"\n",
    "]\n",
    "\n",
    "def load_final_model():\n",
    "    \"\"\"최종 개선된 모델 로드\"\"\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\n",
    "        'skt/kogpt2-base-v2',\n",
    "        bos_token='</s>', eos_token='</s>', unk_token='</s>', pad_token='</s>',\n",
    "        padding_side=\"right\", model_max_length=1024\n",
    "    )\n",
    "    \n",
    "    model = AutoModelForCausalLM.from_pretrained(\"models/improved_sft_v2\")\n",
    "    model.eval()\n",
    "    \n",
    "    if torch.cuda.is_available():\n",
    "        model = model.cuda()\n",
    "    \n",
    "    return model, tokenizer\n",
    "\n",
    "def generate_response_final(model, tokenizer, question, config):\n",
    "    \"\"\"최종 응답 생성\"\"\"\n",
    "    prompt = f\"### Instruction(명령어):\\n{question}\\n\\n### Response(응답):\"\n",
    "    \n",
    "    inputs = tokenizer.encode(prompt, return_tensors='pt')\n",
    "    if torch.cuda.is_available():\n",
    "        inputs = inputs.cuda()\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            inputs,\n",
    "            max_length=inputs.shape[1] + 120,\n",
    "            pad_token_id=tokenizer.pad_token_id,\n",
    "            eos_token_id=tokenizer.eos_token_id,\n",
    "            **config\n",
    "        )\n",
    "    \n",
    "    generation_time = time.time() - start_time\n",
    "    response = tokenizer.decode(outputs[0][inputs.shape[1]:], skip_special_tokens=True)\n",
    "    response = response.replace(\"'\", \"\").strip()\n",
    "    \n",
    "    # 상세한 메트릭 계산\n",
    "    tokens = response.split()\n",
    "    char_count = len(response)\n",
    "    token_count = len(tokens)\n",
    "    \n",
    "    # 반복성 분석\n",
    "    if token_count > 2:\n",
    "        bigrams = [f\"{tokens[i]} {tokens[i+1]}\" for i in range(len(tokens)-1)]\n",
    "        unique_bigrams = len(set(bigrams))\n",
    "        repetition_ratio = 1 - (unique_bigrams / len(bigrams))\n",
    "    else:\n",
    "        repetition_ratio = 0\n",
    "    \n",
    "    # 응답 완성도 체크\n",
    "    complete_endings = ['.', '다', '요', '습니다', '됩니다', '까요', '세요', '해요']\n",
    "    is_complete = any(response.endswith(ending) for ending in complete_endings)\n",
    "    \n",
    "    # 적절한 길이 체크 (50-300자)\n",
    "    appropriate_length = 50 <= char_count <= 300\n",
    "    \n",
    "    return {\n",
    "        'response': response,\n",
    "        'char_count': char_count,\n",
    "        'token_count': token_count,\n",
    "        'generation_time': generation_time,\n",
    "        'repetition_ratio': repetition_ratio,\n",
    "        'is_complete': is_complete,\n",
    "        'appropriate_length': appropriate_length\n",
    "    }\n",
    "\n",
    "def final_comprehensive_test():\n",
    "    \"\"\"최종 종합 평가\"\"\"\n",
    "    model, tokenizer = load_final_model()\n",
    "    \n",
    "    print(\"=== 최종 종합 성능 평가 ===\")\n",
    "    print(f\"테스트 질문 수: {len(final_test_questions)}\")\n",
    "    print(\"최적 디코딩 설정 적용 중...\\n\")\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    for i, question in enumerate(final_test_questions):\n",
    "        if (i + 1) % 5 == 0:\n",
    "            print(f\"진행률: {i + 1}/{len(final_test_questions)}\")\n",
    "        \n",
    "        result = generate_response_final(model, tokenizer, question, OPTIMAL_CONFIG)\n",
    "        result['question'] = question\n",
    "        result['question_id'] = i + 1\n",
    "        results.append(result)\n",
    "    \n",
    "    return results\n",
    "\n",
    "def analyze_final_results(results):\n",
    "    \"\"\"최종 결과 종합 분석\"\"\"\n",
    "    df = pd.DataFrame(results)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"최종 성능 분석 결과\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # 기본 통계\n",
    "    print(f\"\\n=== 기본 성능 지표 ===\")\n",
    "    print(f\"총 질문 수: {len(results)}\")\n",
    "    print(f\"평균 응답 길이: {df['char_count'].mean():.1f}자\")\n",
    "    print(f\"평균 토큰 수: {df['token_count'].mean():.1f}개\")\n",
    "    print(f\"평균 생성 시간: {df['generation_time'].mean():.3f}초\")\n",
    "    print(f\"평균 반복 비율: {df['repetition_ratio'].mean():.3f}\")\n",
    "    \n",
    "    # 품질 지표\n",
    "    print(f\"\\n=== 품질 지표 ===\")\n",
    "    complete_responses = df['is_complete'].sum()\n",
    "    appropriate_lengths = df['appropriate_length'].sum()\n",
    "    \n",
    "    print(f\"완성도 높은 응답: {complete_responses}/{len(results)} ({complete_responses/len(results)*100:.1f}%)\")\n",
    "    print(f\"적절한 길이 응답: {appropriate_lengths}/{len(results)} ({appropriate_lengths/len(results)*100:.1f}%)\")\n",
    "    \n",
    "    # 길이 분포\n",
    "    print(f\"\\n=== 응답 길이 분포 ===\")\n",
    "    print(f\"최단: {df['char_count'].min()}자\")\n",
    "    print(f\"최장: {df['char_count'].max()}자\")\n",
    "    print(f\"중간값: {df['char_count'].median():.1f}자\")\n",
    "    print(f\"표준편차: {df['char_count'].std():.1f}자\")\n",
    "    \n",
    "    # 문제가 있는 응답 분석\n",
    "    short_responses = df[df['char_count'] < 50]\n",
    "    long_responses = df[df['char_count'] > 300]\n",
    "    high_repetition = df[df['repetition_ratio'] > 0.2]\n",
    "    \n",
    "    print(f\"\\n=== 문제 응답 분석 ===\")\n",
    "    print(f\"너무 짧은 응답 (50자 미만): {len(short_responses)}개\")\n",
    "    print(f\"너무 긴 응답 (300자 초과): {len(long_responses)}개\")\n",
    "    print(f\"반복이 많은 응답 (0.2 이상): {len(high_repetition)}개\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "def show_quality_examples(results):\n",
    "    \"\"\"품질별 응답 예시\"\"\"\n",
    "    df = pd.DataFrame(results)\n",
    "    \n",
    "    print(f\"\\n=== 응답 품질 예시 ===\")\n",
    "    \n",
    "    # 좋은 응답 예시 (적절한 길이 + 낮은 반복 + 완성도)\n",
    "    good_responses = df[\n",
    "        (df['appropriate_length'] == True) & \n",
    "        (df['repetition_ratio'] < 0.1) & \n",
    "        (df['is_complete'] == True)\n",
    "    ].head(3)\n",
    "    \n",
    "    print(f\"\\n--- 우수 응답 예시 ---\")\n",
    "    for i, row in good_responses.iterrows():\n",
    "        print(f\"\\n질문: {row['question']}\")\n",
    "        print(f\"응답: {row['response']}\")\n",
    "        print(f\"길이: {row['char_count']}자, 반복도: {row['repetition_ratio']:.3f}\")\n",
    "    \n",
    "    # 개선이 필요한 응답 (있다면)\n",
    "    if len(df[df['char_count'] < 50]) > 0:\n",
    "        print(f\"\\n--- 개선 필요 응답 예시 ---\")\n",
    "        poor_example = df[df['char_count'] < 50].iloc[0]\n",
    "        print(f\"질문: {poor_example['question']}\")\n",
    "        print(f\"응답: {poor_example['response']}\")\n",
    "        print(f\"문제: 너무 짧음 ({poor_example['char_count']}자)\")\n",
    "\n",
    "def compare_with_original():\n",
    "    \"\"\"원래 성능과 비교 (요약)\"\"\"\n",
    "    print(f\"\\n=== 전체 개선 과정 요약 ===\")\n",
    "    print(\"1단계 (기존): 평균 197자, 반복비율 0.006, 최단 17자\")\n",
    "    print(\"2단계 (SFT 개선): 평균 248자, 반복비율 0.079, 최단 123자\")\n",
    "    print(\"3단계 (디코딩 최적화): 위 결과 참조\")\n",
    "    print(\"\\n개선 포인트:\")\n",
    "    print(\"- 너무 짧은 응답 해결\")\n",
    "    print(\"- 응답 일관성 향상\")\n",
    "    print(\"- 반복성 적절히 조절\")\n",
    "    print(\"- 전반적 품질 향상\")\n",
    "\n",
    "def main():\n",
    "    \"\"\"최종 평가 실행\"\"\"\n",
    "    print(\"KoChatGPT 최종 성능 평가를 시작합니다...\")\n",
    "    \n",
    "    results = final_comprehensive_test()\n",
    "    df = analyze_final_results(results)\n",
    "    show_quality_examples(results)\n",
    "    compare_with_original()\n",
    "    \n",
    "    # 결과 저장\n",
    "    df.to_csv('final_evaluation_results.csv', index=False, encoding='utf-8-sig')\n",
    "    print(f\"\\n상세 결과가 'final_evaluation_results.csv'에 저장되었습니다.\")\n",
    "    \n",
    "    print(f\"\\n=== 최종 평가 완료 ===\")\n",
    "    print(\"개선 과정: 기본 모델 → SFT 개선 → 디코딩 최적화\")\n",
    "    print(\"모든 단계가 성공적으로 완료되었습니다.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77ff13e1-d90a-40ac-b3d4-3b73c964a851",
   "metadata": {},
   "source": [
    "# RM 수동평가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "97f0a1fa-8903-46c7-9a3a-b41036e7818d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "노트북용 모델 평가를 시작합니다...\n",
      "두 모델 모두 로드 성공\n",
      "응답 생성 중...\n",
      "질문 1/7 완료\n",
      "질문 2/7 완료\n",
      "질문 3/7 완료\n",
      "질문 4/7 완료\n",
      "질문 5/7 완료\n",
      "질문 6/7 완료\n",
      "질문 7/7 완료\n",
      "\n",
      "================================================================================\n",
      "응답 비교 평가 시트\n",
      "================================================================================\n",
      "각 질문의 두 응답을 비교하여 어느 것이 더 좋은지 판단하세요\n",
      "A=기존모델, B=개선모델, C=비슷함\n",
      "================================================================================\n",
      "\n",
      "【질문 1】 대한민국의 수도는? (유형: factual)\n",
      "------------------------------------------------------------\n",
      "[A: 기존 모델] (120자)\n",
      "저는 인공지능 어시스턴트이기 때문에 대한민국의 수도를 알 수 없습니다. 하지만 대한민국 수도라는 용어는 여러 가지 의미로 사용될 수 있습니다. 예를 들어, \"서울\" 또는 \"세종\", \"청주\"라는 용어가 있을 것입니다.\n",
      "\n",
      "[B: 개선 모델] (95자)\n",
      "저는 인공지능 언어모델로써 답변을 생성하고 있습니다. 따라서 대한민국 수도에 대한 정보가 없습니다. 추가적인 정보를 제공해주시면 더 정확한 대답을 드릴 수 있을 것입니다.\n",
      "\n",
      "평가:\n",
      "□ 정확성: A / B / C\n",
      "□ 유용성: A / B / C\n",
      "□ 적절한길이: A / B / C\n",
      "□ 전체평가: A / B / C\n",
      "================================================================================\n",
      "\n",
      "【질문 2】 인공지능이 뭐야? (유형: explanation)\n",
      "------------------------------------------------------------\n",
      "[A: 기존 모델] (114자)\n",
      "저는 인공지능을 이해하고 이해하기 위해 노력하고 있습니다. 하지만 인간의 지능은 매우 다양하며, 인간처럼 생각하거나 경험하지 못하는 경우도 많습니다. 따라서 인간과 같은 생각을 하지 않는 것이 중요합니다.\n",
      "\n",
      "[B: 개선 모델] (203자)\n",
      "저는 인공지능을 학습하는 AI이기 때문에, 인간과 같은 감정을 느끼지 않습니다. 하지만 인간의 감정에는 다양한 요소가 존재합니다. 예를 들어, 인간들은 자신의 감정과 관련된 정보를 가지고 있으며, 이를 바탕으로 문제를 해결하거나 해결하는 능력을 갖출 수 있습니다. 또한, 다른 사람들은 자신이 가진 능력과 경험을 활용하여 더 나은 결과를 얻을 수도 있을 것입니다.\n",
      "\n",
      "평가:\n",
      "□ 정확성: A / B / C\n",
      "□ 유용성: A / B / C\n",
      "□ 적절한길이: A / B / C\n",
      "□ 전체평가: A / B / C\n",
      "================================================================================\n",
      "\n",
      "【질문 3】 김치찌개 만드는 방법 (유형: how_to)\n",
      "------------------------------------------------------------\n",
      "[A: 기존 모델] (123자)\n",
      "1. 김치찌개를 만들 때 양념장을 사용합니다.\\n2. 김치를 잘게 썰어 그릇에 담아줍니다.\\n3. 소금에 절인 후 양념을 넣어 끓여서 맛을 더해주세요.\\n4. 김치의 재료를 넣고 볶아주시면 더욱 맛있게 먹을 수 있습니다.\n",
      "\n",
      "[B: 개선 모델] (295자)\n",
      "1. 김치찌개를 만들기 전에 재료와 양념, 조리법 등을 충분히 준비해 주세요.\\n2. 김치를 담글 때 적당한 크기로 잘라주세요! 잘게 자른 뒤 양념을 넣고 잘 섞어줍니다.\\n3. 팬에 기름을 두르고 고추장, 참기름 등으로 간을 맞춘 후 불을 끄고 끓여서 그릇에 담고 전자레인지에 데워냅니다.\\s4. 끓는 물에 소금을 뿌려 끓여준다. 이때 약간의 매운맛이 느껴질 수 있습니다.\\n5. 재료를 손질하고 남은 것을 다시 한번 더 넣어주는 것이 좋습니다. 만약에 재료가 맞지 않거나 잘못되었다면 다른 방법으로 보완할 수도 있으니 참고하\n",
      "\n",
      "평가:\n",
      "□ 정확성: A / B / C\n",
      "□ 유용성: A / B / C\n",
      "□ 적절한길이: A / B / C\n",
      "□ 전체평가: A / B / C\n",
      "================================================================================\n",
      "\n",
      "【질문 4】 스트레스 해소 방법은? (유형: advice)\n",
      "------------------------------------------------------------\n",
      "[A: 기존 모델] (232자)\n",
      "1. 스트레스를 줄이기 위해 운동이나 취미활동을 추천합니다.\\n2. 적절한 휴식과 수면을 취하며, 건강한 식습관을 유지하는 것이 중요합니다.\\s- 규칙적인 운동, 충분한 수면 및 영양 섭취, 그리고 적당한 운동을 통해 몸과 마음을 편안하게 해줍니다.\\n3. 자신의 건강 상태와 생활 패턴을 고려하여 자신에게 맞는 방법을 선택하세요.\\n4. 스트레스가 심한 경우에는 전문가의 상담과 치료를 받는 것도 좋은 방법입니다.\n",
      "\n",
      "[B: 개선 모델] (292자)\n",
      "1. 스트레스를 줄이기 위해 운동하기: 스트레스는 매우 큰 문제입니다. 운동을 통해 몸과 마음을 편안하게 해줄 수 있습니다.\\n2. 긍정적인 마인드셋 유지하기: 건강한 삶을 유지하려면 긍정적이고 자신감 있는 태도를 유지하는 것이 중요합니다. 이를 위해서는 자신의 생각과 감정을 솔직하게 표현하는 것도 좋습니다.\\n3. 새로운 취미나 관심사를 찾아보세요. 자신이 좋아하는 일을 찾아서 시작해보는 것은 어떨까요? 예를 들어, 스포츠, 요가, 요가 등 다양한 취미활동을 하면서 자신에게 맞는 활동을 찾는다면 더욱 좋을 것입니다.\n",
      "\n",
      "평가:\n",
      "□ 정확성: A / B / C\n",
      "□ 유용성: A / B / C\n",
      "□ 적절한길이: A / B / C\n",
      "□ 전체평가: A / B / C\n",
      "================================================================================\n",
      "\n",
      "【질문 5】 파이썬 리스트와 튜플 차이점 (유형: explanation)\n",
      "------------------------------------------------------------\n",
      "[A: 기존 모델] (118자)\n",
      "저는 인공지능 언어모델이며, 파이썬 리스트의 차이와 튜플의 차이가 무엇인지 알 수 없습니다. 하지만, 일반적으로 파이썬과 튜플은 서로 다른 기능을 가지고 있으며, 어떤 경우에도 사용 가능한 경우가 많다고 합니다.\n",
      "\n",
      "[B: 개선 모델] (196자)\n",
      "저는 인공지능 언어모델로써 답변을 생성하는 AI이기 때문에, 파이썬 리스트의 차이와 튜플레이의 차이가 무엇인지 알 수 없습니다. 따라서 해당 질문에 대한 정확한 대답을 제공할 수는 없지만, 일반적으로 \"튜플\"과 \"튜플로\"는 서로 다른 의미를 가지고 있습니다. 따라서, 어떤 문제가 있는지 자세히 설명해주시면 더 나은 답을 드릴 수도 있을 것 같습니다!\n",
      "\n",
      "평가:\n",
      "□ 정확성: A / B / C\n",
      "□ 유용성: A / B / C\n",
      "□ 적절한길이: A / B / C\n",
      "□ 전체평가: A / B / C\n",
      "================================================================================\n",
      "\n",
      "【질문 6】 감기 걸렸을 때 대처법 (유형: how_to)\n",
      "------------------------------------------------------------\n",
      "[A: 기존 모델] (138자)\n",
      "1. 적절한 휴식과 수면을 취하세요.\\n2. 충분한 수분 섭취와 규칙적인 운동 등을 통해 몸의 면역력을 강화해보세요!\\n3. 스트레스를 줄이기 위해 수면 시간을 줄이는 것도 중요합니다.\\n4. 건강한 식습관을 유지하여 감기를 예방할 수도 있습니다.\n",
      "\n",
      "[B: 개선 모델] (253자)\n",
      "1. 체온 조절하기: 감기에 걸린 경우 몸을 따뜻하게 하고 마스크를 착용하여 몸의 온도를 낮추어야 합니다.\\n2. 규칙적인 운동: 건강한 식습관과 운동을 통해 체력을 보충하고 근육을 강화해야합니다.\\n3. 적절한 휴식과 수분 섭취: 충분한 수면과 영양소를 충분히 공급하고, 스트레스 해소와 면역력 강화에 도움이 됩니다.\\n4. 따뜻한 물 마시기, 손 씻기와 같은 생활 습관 개선: 물을 자주 마시고 샤워나 세면 등의 방법으로 열을 내리는 것이 좋습니다.\n",
      "\n",
      "평가:\n",
      "□ 정확성: A / B / C\n",
      "□ 유용성: A / B / C\n",
      "□ 적절한길이: A / B / C\n",
      "□ 전체평가: A / B / C\n",
      "================================================================================\n",
      "\n",
      "【질문 7】 좋은 책 추천해줘 (유형: advice)\n",
      "------------------------------------------------------------\n",
      "[A: 기존 모델] (109자)\n",
      "저는 인공지능 언어모델로써 책을 추천하는 것은 불가능합니다. 하지만 좋은 책은 여러분이 좋아하는 것을 찾아드릴 수 있는 훌륭한 방법입니다. 어떤 분야에서든 도움이 필요하시면 언제든지 말씀해주세요.\n",
      "\n",
      "[B: 개선 모델] (315자)\n",
      "1. 인문학: 인생에 대한 새로운 시각을 제공하고, 인간의 삶을 이해하고 이해하는 데 도움을 줄 수 있는 좋은 책을 추천합니다.\\n2. 철학 : 철학적, 윤리적 관점에서 인간 삶의 가치와 의미를 성찰하는 내용을 담고 있습니다.\\n3. 문학: 문학과 예술이 서로 다른 시각과 관점을 가지고 있으며, 상호 보완적인 관계를 맺고 있다는 것을 강조하며, 인간과 자연 사이의 관계, 삶과 죽음 등 다양한 주제를 다루고 있습니다.\\ne4. 역사학: 역사와 문화, 정치, 경제, 사회, 문화 등의 분야를 아우르는 폭넓은 지식을 제공하고 있어 많은 사람들에게 사랑받고 있습니다.\n",
      "\n",
      "평가:\n",
      "□ 정확성: A / B / C\n",
      "□ 유용성: A / B / C\n",
      "□ 적절한길이: A / B / C\n",
      "□ 전체평가: A / B / C\n",
      "================================================================================\n",
      "\n",
      "평가 시트가 'evaluation_sheet.csv'에 저장되었습니다.\n",
      "파일을 열어서 평가 결과를 A/B/C로 입력하세요.\n",
      "\n",
      "다음 단계:\n",
      "1. 'evaluation_sheet.csv' 파일을 엑셀이나 구글시트로 열기\n",
      "2. 각 응답을 읽고 평가 컬럼에 A/B/C 입력\n",
      "3. 저장 후 analyze_results_from_csv() 함수 실행\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import pandas as pd\n",
    "\n",
    "# 평가할 질문들 (더 적은 수로)\n",
    "test_questions = [\n",
    "    {\"question\": \"대한민국의 수도는?\", \"type\": \"factual\"},\n",
    "    {\"question\": \"인공지능이 뭐야?\", \"type\": \"explanation\"}, \n",
    "    {\"question\": \"김치찌개 만드는 방법\", \"type\": \"how_to\"},\n",
    "    {\"question\": \"스트레스 해소 방법은?\", \"type\": \"advice\"},\n",
    "    {\"question\": \"파이썬 리스트와 튜플 차이점\", \"type\": \"explanation\"},\n",
    "    {\"question\": \"감기 걸렸을 때 대처법\", \"type\": \"how_to\"},\n",
    "    {\"question\": \"좋은 책 추천해줘\", \"type\": \"advice\"}\n",
    "]\n",
    "\n",
    "def load_models():\n",
    "    \"\"\"모델들 로드\"\"\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\n",
    "        'skt/kogpt2-base-v2',\n",
    "        bos_token='</s>', eos_token='</s>', unk_token='</s>', pad_token='</s>',\n",
    "        padding_side=\"right\", model_max_length=1024\n",
    "    )\n",
    "    \n",
    "    try:\n",
    "        model_old = AutoModelForCausalLM.from_pretrained(\"models/output_1_SFT\")\n",
    "        model_new = AutoModelForCausalLM.from_pretrained(\"models/improved_sft_v2\")\n",
    "        print(\"두 모델 모두 로드 성공\")\n",
    "    except:\n",
    "        print(\"모델 로드 실패\")\n",
    "        return None, None, None\n",
    "    \n",
    "    if torch.cuda.is_available():\n",
    "        model_old = model_old.cuda()\n",
    "        model_new = model_new.cuda()\n",
    "    \n",
    "    return model_old, model_new, tokenizer\n",
    "\n",
    "def generate_response(model, tokenizer, question):\n",
    "    \"\"\"응답 생성\"\"\"\n",
    "    prompt = f\"### Instruction(명령어):\\n{question}\\n\\n### Response(응답):\"\n",
    "    inputs = tokenizer.encode(prompt, return_tensors='pt')\n",
    "    \n",
    "    if torch.cuda.is_available():\n",
    "        inputs = inputs.cuda()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            inputs,\n",
    "            max_length=inputs.shape[1] + 120,\n",
    "            do_sample=True,\n",
    "            temperature=0.4,\n",
    "            top_k=25,\n",
    "            top_p=0.8,\n",
    "            repetition_penalty=1.3,\n",
    "            no_repeat_ngram_size=3,\n",
    "            pad_token_id=tokenizer.pad_token_id\n",
    "        )\n",
    "    \n",
    "    response = tokenizer.decode(outputs[0][inputs.shape[1]:], skip_special_tokens=True)\n",
    "    return response.replace(\"'\", \"\").strip()\n",
    "\n",
    "def generate_comparison_data():\n",
    "    \"\"\"비교 데이터 생성 (평가는 따로)\"\"\"\n",
    "    model_old, model_new, tokenizer = load_models()\n",
    "    \n",
    "    if model_old is None:\n",
    "        return None\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    print(\"응답 생성 중...\")\n",
    "    \n",
    "    for i, question_data in enumerate(test_questions):\n",
    "        question = question_data[\"question\"]\n",
    "        \n",
    "        # 두 모델 응답 생성\n",
    "        response_old = generate_response(model_old, tokenizer, question)\n",
    "        response_new = generate_response(model_new, tokenizer, question)\n",
    "        \n",
    "        result = {\n",
    "            \"질문번호\": i+1,\n",
    "            \"질문\": question,\n",
    "            \"유형\": question_data[\"type\"],\n",
    "            \"기존모델_응답\": response_old,\n",
    "            \"개선모델_응답\": response_new,\n",
    "            \"기존모델_길이\": len(response_old),\n",
    "            \"개선모델_길이\": len(response_new)\n",
    "        }\n",
    "        \n",
    "        results.append(result)\n",
    "        print(f\"질문 {i+1}/{len(test_questions)} 완료\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "def display_for_evaluation(results):\n",
    "    \"\"\"평가용 출력 (보기 좋게)\"\"\"\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"응답 비교 평가 시트\")\n",
    "    print(\"=\"*80)\n",
    "    print(\"각 질문의 두 응답을 비교하여 어느 것이 더 좋은지 판단하세요\")\n",
    "    print(\"A=기존모델, B=개선모델, C=비슷함\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    for result in results:\n",
    "        print(f\"\\n【질문 {result['질문번호']}】 {result['질문']} (유형: {result['유형']})\")\n",
    "        print(\"-\" * 60)\n",
    "        \n",
    "        print(f\"[A: 기존 모델] ({result['기존모델_길이']}자)\")\n",
    "        print(f\"{result['기존모델_응답']}\")\n",
    "        \n",
    "        print(f\"\\n[B: 개선 모델] ({result['개선모델_길이']}자)\")\n",
    "        print(f\"{result['개선모델_응답']}\")\n",
    "        \n",
    "        print(f\"\\n평가:\")\n",
    "        print(f\"□ 정확성: A / B / C\")\n",
    "        print(f\"□ 유용성: A / B / C\") \n",
    "        print(f\"□ 적절한길이: A / B / C\")\n",
    "        print(f\"□ 전체평가: A / B / C\")\n",
    "        print(\"=\" * 80)\n",
    "\n",
    "def create_evaluation_sheet(results):\n",
    "    \"\"\"평가 시트 CSV 생성\"\"\"\n",
    "    eval_data = []\n",
    "    \n",
    "    for result in results:\n",
    "        eval_data.append({\n",
    "            \"질문번호\": result[\"질문번호\"],\n",
    "            \"질문\": result[\"질문\"],\n",
    "            \"유형\": result[\"유형\"],\n",
    "            \"기존모델응답\": result[\"기존모델_응답\"],\n",
    "            \"개선모델응답\": result[\"개선모델_응답\"],\n",
    "            \"기존길이\": result[\"기존모델_길이\"],\n",
    "            \"개선길이\": result[\"개선모델_길이\"],\n",
    "            \"정확성평가\": \"\",  # 비워둠 - 수동 입력\n",
    "            \"유용성평가\": \"\",  # 비워둠 - 수동 입력\n",
    "            \"길이평가\": \"\",   # 비워둠 - 수동 입력\n",
    "            \"전체평가\": \"\",   # 비워둠 - 수동 입력\n",
    "            \"메모\": \"\"       # 비워둠 - 수동 입력\n",
    "        })\n",
    "    \n",
    "    df = pd.DataFrame(eval_data)\n",
    "    df.to_csv(\"evaluation_sheet.csv\", index=False, encoding='utf-8-sig')\n",
    "    print(f\"\\n평가 시트가 'evaluation_sheet.csv'에 저장되었습니다.\")\n",
    "    print(\"파일을 열어서 평가 결과를 A/B/C로 입력하세요.\")\n",
    "\n",
    "def analyze_results_from_csv(filename=\"evaluation_sheet.csv\"):\n",
    "    \"\"\"CSV에서 평가 결과 분석\"\"\"\n",
    "    try:\n",
    "        df = pd.read_csv(filename, encoding='utf-8-sig')\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*50)\n",
    "        print(\"평가 결과 분석\")\n",
    "        print(\"=\"*50)\n",
    "        \n",
    "        categories = [\"정확성평가\", \"유용성평가\", \"길이평가\", \"전체평가\"]\n",
    "        \n",
    "        for cat in categories:\n",
    "            if cat in df.columns:\n",
    "                a_count = sum(df[cat] == 'A')\n",
    "                b_count = sum(df[cat] == 'B') \n",
    "                c_count = sum(df[cat] == 'C')\n",
    "                total = a_count + b_count + c_count\n",
    "                \n",
    "                if total > 0:\n",
    "                    print(f\"\\n{cat}:\")\n",
    "                    print(f\"  기존모델(A): {a_count}승 ({a_count/total*100:.1f}%)\")\n",
    "                    print(f\"  개선모델(B): {b_count}승 ({b_count/total*100:.1f}%)\")\n",
    "                    print(f\"  비슷함(C): {c_count}회 ({c_count/total*100:.1f}%)\")\n",
    "        \n",
    "        # 유형별 분석\n",
    "        if \"전체평가\" in df.columns:\n",
    "            print(f\"\\n유형별 개선모델 승률:\")\n",
    "            for q_type in df[\"유형\"].unique():\n",
    "                type_df = df[df[\"유형\"] == q_type]\n",
    "                b_wins = sum(type_df[\"전체평가\"] == 'B')\n",
    "                total = len(type_df)\n",
    "                print(f\"  {q_type}: {b_wins}/{total} ({b_wins/total*100:.1f}%)\")\n",
    "                \n",
    "    except FileNotFoundError:\n",
    "        print(\"evaluation_sheet.csv 파일을 찾을 수 없습니다.\")\n",
    "    except Exception as e:\n",
    "        print(f\"분석 중 오류: {e}\")\n",
    "\n",
    "def main():\n",
    "    \"\"\"메인 실행 함수\"\"\"\n",
    "    print(\"노트북용 모델 평가를 시작합니다...\")\n",
    "    \n",
    "    # 1단계: 응답 생성\n",
    "    results = generate_comparison_data()\n",
    "    \n",
    "    if results is None:\n",
    "        print(\"모델 로드 실패로 종료합니다.\")\n",
    "        return\n",
    "    \n",
    "    # 2단계: 화면에 출력\n",
    "    display_for_evaluation(results)\n",
    "    \n",
    "    # 3단계: CSV 파일 생성\n",
    "    create_evaluation_sheet(results)\n",
    "    \n",
    "    print(f\"\\n다음 단계:\")\n",
    "    print(f\"1. 'evaluation_sheet.csv' 파일을 엑셀이나 구글시트로 열기\")\n",
    "    print(f\"2. 각 응답을 읽고 평가 컬럼에 A/B/C 입력\")\n",
    "    print(f\"3. 저장 후 analyze_results_from_csv() 함수 실행\")\n",
    "\n",
    "# 분석만 실행하고 싶을 때\n",
    "def quick_analysis():\n",
    "    \"\"\"평가 완료 후 결과만 분석\"\"\"\n",
    "    analyze_results_from_csv()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f40a906-e91a-40e2-a363-5798fd449942",
   "metadata": {},
   "source": [
    "# 평가 결과"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "bbb0cf7f-5143-4a3b-984f-84e2bdf5f885",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "평가 결과 분석\n",
      "==================================================\n",
      "\n",
      "정확성평가:\n",
      "  기존모델(A): 1승 (14.3%)\n",
      "  개선모델(B): 5승 (71.4%)\n",
      "  비슷함(C): 1회 (14.3%)\n",
      "\n",
      "유용성평가:\n",
      "  기존모델(A): 2승 (28.6%)\n",
      "  개선모델(B): 4승 (57.1%)\n",
      "  비슷함(C): 1회 (14.3%)\n",
      "\n",
      "길이평가:\n",
      "  기존모델(A): 2승 (28.6%)\n",
      "  개선모델(B): 3승 (42.9%)\n",
      "  비슷함(C): 2회 (28.6%)\n",
      "\n",
      "전체평가:\n",
      "  기존모델(A): 1승 (14.3%)\n",
      "  개선모델(B): 5승 (71.4%)\n",
      "  비슷함(C): 1회 (14.3%)\n",
      "\n",
      "유형별 개선모델 승률:\n",
      "  factual: 1/1 (100.0%)\n",
      "  explanation: 1/2 (50.0%)\n",
      "  how_to: 2/2 (100.0%)\n",
      "  advice: 1/2 (50.0%)\n"
     ]
    }
   ],
   "source": [
    "analyze_results_from_csv()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a0d0bc2-75cf-4296-af41-e5ec1b35a768",
   "metadata": {},
   "source": [
    "# RM 추가 평가 진행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "9d38b56b-4649-4cc7-a209-4d13cae077ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30개 질문으로 확장된 모델 평가를 시작합니다...\n",
      "블라인드 테스트를 위해 모델 순서를 무작위화합니다.\n",
      "모델 로드 성공\n",
      "30개 질문에 대한 응답 생성 중...\n",
      "5/30 완료\n",
      "10/30 완료\n",
      "15/30 완료\n",
      "20/30 완료\n",
      "25/30 완료\n",
      "30/30 완료\n",
      "evaluation_form.html 파일이 생성되었습니다.\n",
      "브라우저에서 열어서 평가하고, 결과를 CSV에 입력하세요.\n",
      "evaluation_30questions.csv 파일도 생성되었습니다.\n",
      "\n",
      "파일 생성 완료:\n",
      "1. evaluation_form.html - 브라우저에서 보기 좋게 평가\n",
      "2. evaluation_30questions.csv - 엑셀에서 평가\n",
      "\n",
      "평가 완료 후 analyze_detailed_results() 실행하세요.\n"
     ]
    }
   ],
   "source": [
    "# 확장된 평가 질문 (30개, 균등 분배)\n",
    "test_questions = [\n",
    "    # 사실 질문 (8개)\n",
    "    {\"question\": \"대한민국의 수도는?\", \"type\": \"factual\"},\n",
    "    {\"question\": \"1더하기 1은?\", \"type\": \"factual\"},\n",
    "    {\"question\": \"지구에서 가장 높은 산은?\", \"type\": \"factual\"},\n",
    "    {\"question\": \"한국의 화폐 단위는?\", \"type\": \"factual\"},\n",
    "    {\"question\": \"태양계에서 가장 큰 행성은?\", \"type\": \"factual\"},\n",
    "    {\"question\": \"물의 끓는점은?\", \"type\": \"factual\"},\n",
    "    {\"question\": \"한국의 국화는?\", \"type\": \"factual\"},\n",
    "    {\"question\": \"광복절은 언제?\", \"type\": \"factual\"},\n",
    "    \n",
    "    # 설명 질문 (8개)\n",
    "    {\"question\": \"인공지능이 뭐야?\", \"type\": \"explanation\"},\n",
    "    {\"question\": \"파이썬 리스트와 튜플 차이점\", \"type\": \"explanation\"},\n",
    "    {\"question\": \"지구 온난화란?\", \"type\": \"explanation\"},\n",
    "    {\"question\": \"블록체인 기술이란?\", \"type\": \"explanation\"},\n",
    "    {\"question\": \"민주주의란 무엇인가?\", \"type\": \"explanation\"},\n",
    "    {\"question\": \"코로나19가 뭐야?\", \"type\": \"explanation\"},\n",
    "    {\"question\": \"메타버스란?\", \"type\": \"explanation\"},\n",
    "    {\"question\": \"비트코인이란?\", \"type\": \"explanation\"},\n",
    "    \n",
    "    # 방법 질문 (7개)  \n",
    "    {\"question\": \"김치찌개 만드는 방법\", \"type\": \"how_to\"},\n",
    "    {\"question\": \"감기 걸렸을 때 대처법\", \"type\": \"how_to\"},\n",
    "    {\"question\": \"스마트폰 배터리 절약법\", \"type\": \"how_to\"},\n",
    "    {\"question\": \"라면 맛있게 끓이는 법\", \"type\": \"how_to\"},\n",
    "    {\"question\": \"컴퓨터 속도 빠르게 하는 법\", \"type\": \"how_to\"},\n",
    "    {\"question\": \"영어 발음 연습 방법\", \"type\": \"how_to\"},\n",
    "    {\"question\": \"숙면을 위한 방법\", \"type\": \"how_to\"},\n",
    "    \n",
    "    # 조언 질문 (7개)\n",
    "    {\"question\": \"스트레스 해소 방법은?\", \"type\": \"advice\"},\n",
    "    {\"question\": \"시간 관리 팁\", \"type\": \"advice\"},\n",
    "    {\"question\": \"좋은 책 추천해줘\", \"type\": \"advice\"},\n",
    "    {\"question\": \"건강한 식단 조언\", \"type\": \"advice\"},\n",
    "    {\"question\": \"새해 목표 세우는 법\", \"type\": \"advice\"},\n",
    "    {\"question\": \"돈 모으는 방법\", \"type\": \"advice\"},\n",
    "    {\"question\": \"좋은 습관 만들기\", \"type\": \"advice\"}\n",
    "]\n",
    "\n",
    "def load_models():\n",
    "    \"\"\"모델들 로드\"\"\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\n",
    "        'skt/kogpt2-base-v2',\n",
    "        bos_token='</s>', eos_token='</s>', unk_token='</s>', pad_token='</s>',\n",
    "        padding_side=\"right\", model_max_length=1024\n",
    "    )\n",
    "    \n",
    "    try:\n",
    "        model_old = AutoModelForCausalLM.from_pretrained(\"models/output_1_SFT\")\n",
    "        model_new = AutoModelForCausalLM.from_pretrained(\"models/improved_sft_v2\")\n",
    "        print(\"모델 로드 성공\")\n",
    "    except Exception as e:\n",
    "        print(f\"모델 로드 실패: {e}\")\n",
    "        return None, None, None\n",
    "    \n",
    "    if torch.cuda.is_available():\n",
    "        model_old = model_old.cuda()\n",
    "        model_new = model_new.cuda()\n",
    "    \n",
    "    return model_old, model_new, tokenizer\n",
    "\n",
    "def generate_response(model, tokenizer, question):\n",
    "    \"\"\"응답 생성\"\"\"\n",
    "    prompt = f\"### Instruction(명령어):\\n{question}\\n\\n### Response(응답):\"\n",
    "    inputs = tokenizer.encode(prompt, return_tensors='pt')\n",
    "    \n",
    "    if torch.cuda.is_available():\n",
    "        inputs = inputs.cuda()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            inputs,\n",
    "            max_length=inputs.shape[1] + 120,\n",
    "            do_sample=True,\n",
    "            temperature=0.4,\n",
    "            top_k=25,\n",
    "            top_p=0.8,\n",
    "            repetition_penalty=1.3,\n",
    "            no_repeat_ngram_size=3,\n",
    "            pad_token_id=tokenizer.pad_token_id\n",
    "        )\n",
    "    \n",
    "    response = tokenizer.decode(outputs[0][inputs.shape[1]:], skip_special_tokens=True)\n",
    "    return response.replace(\"'\", \"\").strip()\n",
    "\n",
    "def generate_randomized_evaluation():\n",
    "    \"\"\"무작위 순서로 평가 데이터 생성\"\"\"\n",
    "    model_old, model_new, tokenizer = load_models()\n",
    "    \n",
    "    if model_old is None:\n",
    "        return None\n",
    "    \n",
    "    # 질문 순서 무작위화\n",
    "    shuffled_questions = test_questions.copy()\n",
    "    random.shuffle(shuffled_questions)\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    print(f\"30개 질문에 대한 응답 생성 중...\")\n",
    "    \n",
    "    for i, question_data in enumerate(shuffled_questions):\n",
    "        question = question_data[\"question\"]\n",
    "        \n",
    "        # 두 모델 응답 생성\n",
    "        response_old = generate_response(model_old, tokenizer, question)\n",
    "        response_new = generate_response(model_new, tokenizer, question)\n",
    "        \n",
    "        # 모델 순서도 무작위화 (블라인드 테스트)\n",
    "        if random.choice([True, False]):\n",
    "            first_model = \"모델_A\"\n",
    "            second_model = \"모델_B\"\n",
    "            first_response = response_old\n",
    "            second_response = response_new\n",
    "            correct_answer = \"A\"  # A가 기존, B가 개선\n",
    "        else:\n",
    "            first_model = \"모델_B\"\n",
    "            second_model = \"모델_A\"\n",
    "            first_response = response_new\n",
    "            second_response = response_old\n",
    "            correct_answer = \"B\"  # B가 기존, A가 개선\n",
    "        \n",
    "        result = {\n",
    "            \"질문번호\": i+1,\n",
    "            \"질문\": question,\n",
    "            \"유형\": question_data[\"type\"],\n",
    "            f\"{first_model}_응답\": first_response,\n",
    "            f\"{second_model}_응답\": second_response,\n",
    "            f\"{first_model}_길이\": len(first_response),\n",
    "            f\"{second_model}_길이\": len(second_response),\n",
    "            \"실제_개선모델\": correct_answer,  # 평가 후 확인용\n",
    "            \"정확성평가\": \"\",\n",
    "            \"유용성평가\": \"\",\n",
    "            \"길이평가\": \"\",\n",
    "            \"전체평가\": \"\",\n",
    "            \"메모\": \"\"\n",
    "        }\n",
    "        \n",
    "        results.append(result)\n",
    "        \n",
    "        if (i + 1) % 5 == 0:\n",
    "            print(f\"{i + 1}/30 완료\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "\n",
    "def create_csv_sheet(results):\n",
    "    \"\"\"CSV 평가 시트도 생성\"\"\"\n",
    "    eval_data = []\n",
    "    \n",
    "    for result in results:\n",
    "        eval_data.append({\n",
    "            \"질문번호\": result[\"질문번호\"],\n",
    "            \"질문\": result[\"질문\"],\n",
    "            \"유형\": result[\"유형\"],\n",
    "            \"모델A응답\": result[\"모델_A_응답\"],\n",
    "            \"모델B응답\": result[\"모델_B_응답\"],\n",
    "            \"모델A길이\": result[\"모델_A_길이\"],\n",
    "            \"모델B길이\": result[\"모델_B_길이\"],\n",
    "            \"실제개선모델\": result[\"실제_개선모델\"],\n",
    "            \"정확성평가\": \"\",\n",
    "            \"유용성평가\": \"\",\n",
    "            \"길이평가\": \"\",\n",
    "            \"전체평가\": \"\",\n",
    "            \"메모\": \"\"\n",
    "        })\n",
    "    \n",
    "    df = pd.DataFrame(eval_data)\n",
    "    df.to_csv(\"evaluation_30questions.csv\", index=False, encoding='utf-8-sig')\n",
    "    print(\"evaluation_30questions.csv 파일도 생성되었습니다.\")\n",
    "\n",
    "def analyze_detailed_results(filename=\"evaluation_30questions.csv\"):\n",
    "    \"\"\"상세한 결과 분석\"\"\"\n",
    "    try:\n",
    "        df = pd.read_csv(filename, encoding='utf-8-sig')\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"30개 질문 평가 결과 분석\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        categories = [\"정확성평가\", \"유용성평가\", \"길이평가\", \"전체평가\"]\n",
    "        \n",
    "        # 전체 승률\n",
    "        for cat in categories:\n",
    "            if cat in df.columns:\n",
    "                # 실제 개선 모델이 이긴 경우 계산\n",
    "                improved_wins = 0\n",
    "                old_wins = 0\n",
    "                ties = 0\n",
    "                \n",
    "                for _, row in df.iterrows():\n",
    "                    if pd.isna(row[cat]) or row[cat] == \"\":\n",
    "                        continue\n",
    "                        \n",
    "                    real_improved = row[\"실제개선모델\"]\n",
    "                    choice = row[cat]\n",
    "                    \n",
    "                    if choice == real_improved:\n",
    "                        improved_wins += 1\n",
    "                    elif choice == \"C\":\n",
    "                        ties += 1\n",
    "                    else:\n",
    "                        old_wins += 1\n",
    "                \n",
    "                total = improved_wins + old_wins + ties\n",
    "                if total > 0:\n",
    "                    print(f\"\\n{cat}:\")\n",
    "                    print(f\"  개선모델 승리: {improved_wins}/{total} ({improved_wins/total*100:.1f}%)\")\n",
    "                    print(f\"  기존모델 승리: {old_wins}/{total} ({old_wins/total*100:.1f}%)\")\n",
    "                    print(f\"  무승부: {ties}/{total} ({ties/total*100:.1f}%)\")\n",
    "        \n",
    "        # 유형별 분석\n",
    "        if \"전체평가\" in df.columns:\n",
    "            print(f\"\\n유형별 개선모델 승률:\")\n",
    "            for q_type in df[\"유형\"].unique():\n",
    "                type_df = df[df[\"유형\"] == q_type]\n",
    "                improved_wins = 0\n",
    "                total_evaluated = 0\n",
    "                \n",
    "                for _, row in type_df.iterrows():\n",
    "                    if pd.isna(row[\"전체평가\"]) or row[\"전체평가\"] == \"\":\n",
    "                        continue\n",
    "                    total_evaluated += 1\n",
    "                    if row[\"전체평가\"] == row[\"실제개선모델\"]:\n",
    "                        improved_wins += 1\n",
    "                \n",
    "                if total_evaluated > 0:\n",
    "                    print(f\"  {q_type}: {improved_wins}/{total_evaluated} ({improved_wins/total_evaluated*100:.1f}%)\")\n",
    "        \n",
    "        # 통계적 유의성 간단 체크\n",
    "        if \"전체평가\" in df.columns:\n",
    "            valid_responses = df[df[\"전체평가\"].isin([\"A\", \"B\"])].copy()\n",
    "            if len(valid_responses) >= 20:\n",
    "                improved_wins = sum(valid_responses[\"전체평가\"] == valid_responses[\"실제개선모델\"])\n",
    "                total = len(valid_responses)\n",
    "                win_rate = improved_wins / total\n",
    "                \n",
    "                print(f\"\\n통계적 신뢰성:\")\n",
    "                print(f\"  총 유효 평가: {total}개\")\n",
    "                print(f\"  개선모델 승률: {win_rate:.1%}\")\n",
    "                \n",
    "                if win_rate > 0.7 and total >= 20:\n",
    "                    print(f\"  => 통계적으로 의미있는 개선 효과\")\n",
    "                elif win_rate > 0.6 and total >= 30:\n",
    "                    print(f\"  => 개선 효과 있음\")\n",
    "                elif win_rate < 0.4:\n",
    "                    print(f\"  => 오히려 성능 저하\")\n",
    "                else:\n",
    "                    print(f\"  => 개선 효과 불분명\")\n",
    "                    \n",
    "    except FileNotFoundError:\n",
    "        print(\"evaluation_30questions.csv 파일을 찾을 수 없습니다.\")\n",
    "    except Exception as e:\n",
    "        print(f\"분석 중 오류: {e}\")\n",
    "\n",
    "def main():\n",
    "    \"\"\"메인 실행\"\"\"\n",
    "    print(\"30개 질문으로 확장된 모델 평가를 시작합니다...\")\n",
    "    print(\"블라인드 테스트를 위해 모델 순서를 무작위화합니다.\")\n",
    "    \n",
    "    # 시드 설정 (재현 가능하도록)\n",
    "    random.seed(42)\n",
    "    \n",
    "    # 응답 생성\n",
    "    results = generate_randomized_evaluation()\n",
    "    \n",
    "    if results is None:\n",
    "        print(\"모델 로드 실패로 종료합니다.\")\n",
    "        return\n",
    "    \n",
    "    # HTML과 CSV 모두 생성\n",
    "    create_evaluation_html(results)\n",
    "    create_csv_sheet(results)\n",
    "    \n",
    "    print(f\"\\n파일 생성 완료:\")\n",
    "    print(f\"2. evaluation_30questions.csv - 엑셀에서 평가\")\n",
    "    print(f\"\\n평가 완료 후 analyze_detailed_results() 실행하세요.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d8adbff-7109-4f82-9e89-d8b899ee7736",
   "metadata": {},
   "source": [
    "# 추가 평가 후 결과"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "5dc35444-86e2-4d5b-bf71-ca0a804dfe81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "30개 질문 평가 결과 분석\n",
      "============================================================\n",
      "\n",
      "정확성평가:\n",
      "  개선모델 승리: 16/30 (53.3%)\n",
      "  기존모델 승리: 12/30 (40.0%)\n",
      "  무승부: 2/30 (6.7%)\n",
      "\n",
      "유용성평가:\n",
      "  개선모델 승리: 14/30 (46.7%)\n",
      "  기존모델 승리: 13/30 (43.3%)\n",
      "  무승부: 3/30 (10.0%)\n",
      "\n",
      "길이평가:\n",
      "  개선모델 승리: 15/30 (50.0%)\n",
      "  기존모델 승리: 11/30 (36.7%)\n",
      "  무승부: 4/30 (13.3%)\n",
      "\n",
      "전체평가:\n",
      "  개선모델 승리: 15/30 (50.0%)\n",
      "  기존모델 승리: 11/30 (36.7%)\n",
      "  무승부: 4/30 (13.3%)\n",
      "\n",
      "유형별 개선모델 승률:\n",
      "  how_to: 2/7 (28.6%)\n",
      "  explanation: 5/8 (62.5%)\n",
      "  advice: 5/7 (71.4%)\n",
      "  factual: 3/8 (37.5%)\n",
      "\n",
      "통계적 신뢰성:\n",
      "  총 유효 평가: 26개\n",
      "  개선모델 승률: 57.7%\n",
      "  => 개선 효과 불분명\n"
     ]
    }
   ],
   "source": [
    "analyze_detailed_results()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "460d00b3-0768-4f48-92da-6d2d8d31f59d",
   "metadata": {},
   "source": [
    "# RM 평가 결과 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "37876c01-7ccb-4cce-ad5f-3053942950fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RM 학습을 다시 진행합니다.\n",
      "예상 소요 시간: 30-40분\n",
      "=== RM 학습 재시작 ===\n",
      "개선된 SFT 모델 기반으로 RM 생성 중...\n",
      "총 RM 학습 데이터: 30660개\n",
      "5000개로 제한\n",
      "학습 데이터: 4500개\n",
      "평가 데이터: 500개\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e87940c21f9b408abde8df0a38b5f2dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2507bd99f8964ca4b7bc243a26bfb6fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RM 학습 시작...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ebd71463f83c405681589369901795d3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train epoch:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f922f756126a4d42a0db296390d711ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train step of epoch 0:   0%|          | 0/1125 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "965f317bbea34d8eb6c9b13282c11e75",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train step of epoch 1:   0%|          | 0/1125 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b880a9c1ce794bbf9dee702b16e781a0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train step of epoch 2:   0%|          | 0/1125 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "모델 저장 중...\n",
      "모델 저장 완료: models/fixed_rm/reward_model.pt\n",
      "저장 확인: True\n",
      "\n",
      "학습 완료! 테스트 진행...\n",
      "\n",
      "=== RM 모델 테스트 ===\n",
      "RM 모델 로드 성공!\n",
      "\n",
      "RM 점수 테스트:\n",
      "-0.042: 대한민국의 수도는 서울입니다.\n",
      " 1.307: 인공지능은 컴퓨터가 인간의 지능을 모방하여 학습하고 추론하는 기술입니다.\n",
      " 1.121: 김치찌개를 만들려면 신김치를 기름에 볶고 물을 넣어 끓인 후 두부와 고기를 넣으면 됩니다.\n",
      "-1.097: 몰라요.\n",
      "-1.659: ㅋㅋㅋㅋ\n",
      "-0.541: 그런 거 왜 물어봐요?\n",
      " 0.211: 잘 모르겠지만 찾아보시면 좋을 것 같습니다.\n",
      "-0.201: 그 질문은 복잡해서 간단히 답하기 어렵습니다.\n",
      "\n",
      "=== 점수 순 정렬 (높은 점수 = 더 좋은 답변) ===\n",
      "1위 ( 1.307): 인공지능은 컴퓨터가 인간의 지능을 모방하여 학습하고 추론하는 기술입니다.\n",
      "2위 ( 1.121): 김치찌개를 만들려면 신김치를 기름에 볶고 물을 넣어 끓인 후 두부와 고기를 넣으면 됩니다.\n",
      "3위 ( 0.211): 잘 모르겠지만 찾아보시면 좋을 것 같습니다.\n",
      "4위 (-0.042): 대한민국의 수도는 서울입니다.\n",
      "5위 (-0.201): 그 질문은 복잡해서 간단히 답하기 어렵습니다.\n",
      "6위 (-0.541): 그런 거 왜 물어봐요?\n",
      "7위 (-1.097): 몰라요.\n",
      "8위 (-1.659): ㅋㅋㅋㅋ\n",
      "\n",
      "RM 모델이 제대로 작동하는지 확인:\n",
      "- 좋은 답변이 높은 점수를 받는가?\n",
      "- 나쁜 답변이 낮은 점수를 받는가?\n",
      "\n",
      "=== 최종 결과 ===\n",
      "✓ RM 모델 학습 완료\n",
      "✓ models/fixed_rm/reward_model.pt 저장 완료\n",
      "✓ 다음 단계: 3-way 모델 비교 (기존SFT vs 개선SFT vs SFT+RM)\n"
     ]
    }
   ],
   "source": [
    "class FixedRM(RewardModel):\n",
    "    \"\"\"저장 문제를 해결한 RM 클래스\"\"\"\n",
    "    \n",
    "    def __init__(self, sft_model_path, tokenizer=None):\n",
    "        # SFT 모델 로드\n",
    "        sft_model = AutoModelForCausalLM.from_pretrained(sft_model_path)\n",
    "        \n",
    "        # GPT2Model 추출\n",
    "        gpt2_model = GPT2Model(sft_model.config)\n",
    "        gpt2_model.load_state_dict(sft_model.transformer.state_dict())\n",
    "        \n",
    "        if tokenizer is not None:\n",
    "            gpt2_model.resize_token_embeddings(len(tokenizer))\n",
    "        \n",
    "        # Value head\n",
    "        value_head = nn.Linear(gpt2_model.config.n_embd, 1)\n",
    "        \n",
    "        super().__init__(gpt2_model, value_head)\n",
    "        \n",
    "        self.sft_model_path = sft_model_path\n",
    "        self.config = sft_model.config\n",
    "\n",
    "def prepare_rm_data_fixed():\n",
    "    \"\"\"RM 데이터 준비 (수정된 버전)\"\"\"\n",
    "    \n",
    "    with open('KoChatGPT/data_kochatgpt/kochatgpt_2_RM.jsonl', \"r\", encoding='utf-8-sig') as f:\n",
    "        list_data_dict = json.load(f)\n",
    "    \n",
    "    pairwise_data = []\n",
    "    \n",
    "    for item in list_data_dict:\n",
    "        prompt = item['prompt']\n",
    "        completions = [item['completion_0'], item['completion_1'], item['completion_2']]\n",
    "        ranking = item['ranking']\n",
    "        \n",
    "        # 모든 쌍 생성\n",
    "        for i in range(3):\n",
    "            for j in range(i+1, 3):\n",
    "                if ranking[i] < ranking[j]:\n",
    "                    chosen = completions[i]\n",
    "                    rejected = completions[j]\n",
    "                else:\n",
    "                    chosen = completions[j]\n",
    "                    rejected = completions[i]\n",
    "                \n",
    "                pairwise_data.append({\n",
    "                    'prompt': prompt,\n",
    "                    'chosen': chosen,\n",
    "                    'rejected': rejected\n",
    "                })\n",
    "    \n",
    "    # 데이터 섞기\n",
    "    random.seed(42)\n",
    "    random.shuffle(pairwise_data)\n",
    "    \n",
    "    print(f\"총 RM 학습 데이터: {len(pairwise_data)}개\")\n",
    "    \n",
    "    # 더 많은 데이터 사용 (5000개)\n",
    "    if len(pairwise_data) > 5000:\n",
    "        pairwise_data = pairwise_data[:5000]\n",
    "        print(f\"5000개로 제한\")\n",
    "    \n",
    "    return pairwise_data\n",
    "\n",
    "def train_rm_fixed():\n",
    "    \"\"\"수정된 RM 학습\"\"\"\n",
    "    \n",
    "    print(\"=== RM 학습 재시작 ===\")\n",
    "    \n",
    "    # 토크나이저 로드\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\n",
    "        'skt/kogpt2-base-v2',\n",
    "        bos_token='</s>', eos_token='</s>', unk_token='</s>', pad_token='</s>',\n",
    "        padding_side=\"right\", model_max_length=512\n",
    "    )\n",
    "    \n",
    "    # RM 모델 생성\n",
    "    print(\"개선된 SFT 모델 기반으로 RM 생성 중...\")\n",
    "    \n",
    "    with NaiveStrategy().model_init_context():\n",
    "        rm_model = FixedRM(\"models/improved_sft_v2\", tokenizer).cuda()\n",
    "    \n",
    "    # 데이터 준비\n",
    "    all_data = prepare_rm_data_fixed()\n",
    "    \n",
    "    # 학습/평가 데이터 분할\n",
    "    split_idx = int(len(all_data) * 0.9)\n",
    "    train_data = all_data[:split_idx]\n",
    "    eval_data = all_data[split_idx:]\n",
    "    \n",
    "    print(f\"학습 데이터: {len(train_data)}개\")\n",
    "    print(f\"평가 데이터: {len(eval_data)}개\")\n",
    "    \n",
    "    # 데이터셋 생성\n",
    "    train_dataset = RewardDataset(train_data, tokenizer, 512)\n",
    "    eval_dataset = RewardDataset(eval_data, tokenizer, 512)\n",
    "    \n",
    "    # 트레이너 설정 (학습률 대폭 감소)\n",
    "    trainer = RewardModelTrainer(\n",
    "        model=rm_model,\n",
    "        strategy=NaiveStrategy(),\n",
    "        optim=torch.optim.Adam(rm_model.parameters(), lr=1e-6),  # 10배 감소\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=eval_dataset,\n",
    "        batch_size=4,\n",
    "        max_epochs=3  # 에포크 증가\n",
    "    )\n",
    "    \n",
    "    print(\"RM 학습 시작...\")\n",
    "    trainer.fit(use_lora=0)\n",
    "    \n",
    "    # 수동 저장\n",
    "    print(\"모델 저장 중...\")\n",
    "    save_dir = \"models/fixed_rm\"\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    \n",
    "    # 저장할 데이터\n",
    "    save_data = {\n",
    "        'model_state_dict': rm_model.model.state_dict(),\n",
    "        'value_head_state_dict': rm_model.value_head.state_dict(),\n",
    "        'config_dict': rm_model.config.to_dict(),\n",
    "        'sft_model_path': rm_model.sft_model_path\n",
    "    }\n",
    "    \n",
    "    # 저장\n",
    "    save_path = f\"{save_dir}/reward_model.pt\"\n",
    "    torch.save(save_data, save_path)\n",
    "    \n",
    "    print(f\"모델 저장 완료: {save_path}\")\n",
    "    print(f\"저장 확인: {os.path.exists(save_path)}\")\n",
    "    \n",
    "    return rm_model, tokenizer\n",
    "\n",
    "def test_fixed_rm():\n",
    "    \"\"\"수정된 RM 모델 테스트\"\"\"\n",
    "    \n",
    "    print(\"\\n=== RM 모델 테스트 ===\")\n",
    "    \n",
    "    # 토크나이저\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\n",
    "        'skt/kogpt2-base-v2',\n",
    "        bos_token='</s>', eos_token='</s>', unk_token='</s>', pad_token='</s>',\n",
    "        padding_side=\"right\", model_max_length=512\n",
    "    )\n",
    "    \n",
    "    try:\n",
    "        # 저장된 모델 로드\n",
    "        checkpoint = torch.load('models/fixed_rm/reward_model.pt')\n",
    "        \n",
    "        # 모델 재구성\n",
    "        from transformers import GPT2Config\n",
    "        config = GPT2Config.from_dict(checkpoint['config_dict'])\n",
    "        \n",
    "        gpt2_model = GPT2Model(config)\n",
    "        gpt2_model.resize_token_embeddings(len(tokenizer))\n",
    "        \n",
    "        value_head = nn.Linear(config.n_embd, 1)\n",
    "        \n",
    "        rm_model = RewardModel(gpt2_model, value_head)\n",
    "        \n",
    "        # 가중치 로드\n",
    "        rm_model.model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        rm_model.value_head.load_state_dict(checkpoint['value_head_state_dict'])\n",
    "        \n",
    "        if torch.cuda.is_available():\n",
    "            rm_model = rm_model.cuda()\n",
    "        \n",
    "        print(\"RM 모델 로드 성공!\")\n",
    "        \n",
    "        # 다양한 품질의 텍스트로 테스트\n",
    "        test_cases = [\n",
    "            # 좋은 답변들\n",
    "            \"대한민국의 수도는 서울입니다.\",\n",
    "            \"인공지능은 컴퓨터가 인간의 지능을 모방하여 학습하고 추론하는 기술입니다.\",\n",
    "            \"김치찌개를 만들려면 신김치를 기름에 볶고 물을 넣어 끓인 후 두부와 고기를 넣으면 됩니다.\",\n",
    "            \n",
    "            # 나쁜 답변들\n",
    "            \"몰라요.\",\n",
    "            \"ㅋㅋㅋㅋ\",\n",
    "            \"그런 거 왜 물어봐요?\",\n",
    "            \n",
    "            # 중간 답변들\n",
    "            \"잘 모르겠지만 찾아보시면 좋을 것 같습니다.\",\n",
    "            \"그 질문은 복잡해서 간단히 답하기 어렵습니다.\"\n",
    "        ]\n",
    "        \n",
    "        print(\"\\nRM 점수 테스트:\")\n",
    "        scores = []\n",
    "        \n",
    "        for text in test_cases:\n",
    "            input_ids = tokenizer.encode(text, return_tensors='pt')\n",
    "            if torch.cuda.is_available():\n",
    "                input_ids = input_ids.cuda()\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                score = rm_model(input_ids).cpu().item()\n",
    "            \n",
    "            scores.append((score, text))\n",
    "            print(f\"{score:6.3f}: {text}\")\n",
    "        \n",
    "        # 점수 순으로 정렬\n",
    "        print(f\"\\n=== 점수 순 정렬 (높은 점수 = 더 좋은 답변) ===\")\n",
    "        scores.sort(reverse=True)\n",
    "        \n",
    "        for i, (score, text) in enumerate(scores):\n",
    "            print(f\"{i+1}위 ({score:6.3f}): {text}\")\n",
    "        \n",
    "        print(f\"\\nRM 모델이 제대로 작동하는지 확인:\")\n",
    "        print(f\"- 좋은 답변이 높은 점수를 받는가?\")\n",
    "        print(f\"- 나쁜 답변이 낮은 점수를 받는가?\")\n",
    "        \n",
    "        return rm_model, tokenizer\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"RM 모델 테스트 실패: {e}\")\n",
    "        return None, None\n",
    "\n",
    "def main():\n",
    "    print(\"RM 학습을 다시 진행합니다.\")\n",
    "    print(\"예상 소요 시간: 30-40분\")\n",
    "    \n",
    "    # RM 학습\n",
    "    rm_model, tokenizer = train_rm_fixed()\n",
    "    \n",
    "    if rm_model is not None:\n",
    "        print(\"\\n학습 완료! 테스트 진행...\")\n",
    "        test_fixed_rm()\n",
    "        \n",
    "        print(f\"\\n=== 최종 결과 ===\")\n",
    "        print(f\"✓ RM 모델 학습 완료\")\n",
    "        print(f\"✓ models/fixed_rm/reward_model.pt 저장 완료\")\n",
    "        print(f\"✓ 다음 단계: 3-way 모델 비교 (기존SFT vs 개선SFT vs SFT+RM)\")\n",
    "    \n",
    "    else:\n",
    "        print(\"RM 학습 실패\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "359cf731-b4a7-4d34-a938-89908873a208",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RM 모델 저장 확인: False\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(\"RM 모델 저장 확인:\", os.path.exists(\"models/improved_rm/reward_model.pt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "88bfd346-d155-4007-aaf6-7f2ceaffbf19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== 3단계 RLHF 모델 비교 ===\n",
      "Base → SFT → SFT+RM 단계별 성능 비교\n",
      "✓ 기존 SFT 모델 로드 성공\n",
      "✓ 개선 SFT 모델 로드 성공\n",
      "✓ RM 모델 로드 성공\n",
      "3단계 모델 비교 실험 시작...\n",
      "총 12개 질문 테스트\n",
      "\n",
      "진행률: 1/12 - 대한민국의 수도는?\n",
      "\n",
      "진행률: 2/12 - 지구에서 가장 높은 산은?\n",
      "\n",
      "진행률: 3/12 - 1더하기 1은?\n",
      "\n",
      "진행률: 4/12 - 인공지능이 뭐야?\n",
      "\n",
      "진행률: 5/12 - 지구 온난화란?\n",
      "\n",
      "진행률: 6/12 - 블록체인 기술이란?\n",
      "\n",
      "진행률: 7/12 - 김치찌개 만드는 방법\n",
      "\n",
      "진행률: 8/12 - 감기 걸렸을 때 대처법\n",
      "\n",
      "진행률: 9/12 - 영어 공부 방법\n",
      "\n",
      "진행률: 10/12 - 스트레스 해소 방법은?\n",
      "\n",
      "진행률: 11/12 - 시간 관리 팁\n",
      "\n",
      "진행률: 12/12 - 좋은 책 추천해줘\n",
      "\n",
      "=== 3단계 모델 비교 미리보기 ===\n",
      "\n",
      "【질문 1】 대한민국의 수도는? (유형: factual)\n",
      "--------------------------------------------------------------------------------\n",
      "[기존 SFT] (99자)\n",
      "저는 인공지능 어시스턴트이기 때문에 정확한 답변을 제공할 수 없습니다. 하지만 대한민국 수도에 대한 정보는 해당 지역의 부동산 중개업체나 관련 기관에 문의하시는 것이 좋겠습니다!\n",
      "\n",
      "[개선 SFT] (78자)\n",
      "저는 인공지능 언어모델로써 답변을 제공합니다.\\n\\n대한민국 수도는 대한민국의 수도이며, 대한민국 서울특별시 중구 을지로에 위치해 있습니다.\n",
      "\n",
      "[SFT + RM] (146자, RM점수: 0.903)\n",
      "저는 인공지능 언어모델이며, 대한민국의 수도는 대한민국입니다. 따라서 대한민국의 수도를 정확히 알 수 없습니다. 하지만 일반적으로 수도권은 서울특별시, 경기도와 함께 대한민국의 대표적인 도시 중 하나로, 서울과 경기 지역을 중심으로 발전해왔다고 알려져 있습니다.\n",
      "================================================================================\n",
      "\n",
      "【질문 2】 지구에서 가장 높은 산은? (유형: factual)\n",
      "--------------------------------------------------------------------------------\n",
      "[기존 SFT] (73자)\n",
      "저는 인공지능 언어모델로써, 지상에서 가장 낮은 산의 위치를 알 수 없습니다. 하지만 지구에서는 최고 산이 많다고 알려져 있습니다.\n",
      "\n",
      "[개선 SFT] (28자)\n",
      "지구상에서 가장 낮은 산의 이름은 \"아틀란\"입니다.\n",
      "\n",
      "[SFT + RM] (147자, RM점수: 0.702)\n",
      "제가 AI 언어모델로 작동하는 가상 대화 로봇이므로, 지구 최고봉을 의미하는 지구와 지구의 높이를 예측하고 있습니다. 일반적으로 지구는 약 12억 광년 떨어져 있으며, 지구와 달의 높이가 비슷합니다. 따라서 지구에서 가장 높은 산을 찾으시는 것은 가능할 것입니다.\n",
      "================================================================================\n",
      "\n",
      "【질문 3】 1더하기 1은? (유형: factual)\n",
      "--------------------------------------------------------------------------------\n",
      "[기존 SFT] (336자)\n",
      "제가 AI 언어모델로써 답변을 생성하는 것은 불가능합니다. 하지만, 일반적으로 2는 어떤 상황에서 사용되는지 알 수 있습니다. 예를 들어, \"2\"와 같은 단어가 사용됩니다.\\n\\n1. 대화: 대화를 통해 상대방의 의견을 듣고 이해하는 것이 중요합니다. 상대방이 무엇을 원하든, 그것은 당신의 개인적인 문제이며, 당신이 원하는 대로 대답을 제공하는 것입니다.\\n2. 질문 작성: 질문을 작성하는 과정에서, 질문에 대한 정확한 답을 제공할 필요가 있으며, 질문의 문맥과 맥락에 따라 적절한 응답이 필요합니다.\\n3. 피드백: 서로의 생각을 공유하고 서로에게 필요한 정보를 제공하며, 상호 간의 소통을 강화\n",
      "\n",
      "[개선 SFT] (207자)\n",
      "저는 AI 어시스턴트이기 때문에 실제로 어떤 것을 말하는지 알 수 없습니다. 하지만 일반적으로 2 더하기는 보통 다음과 같은 것들이 포함됩니다.\\n-1. 계산: 현재 시점에서는 1.9의 값을 가지고 있습니다.\\n2. 저장: 현재의 시점에서 가장 많은 양의 데이터를 저장할 수도 있고, 다른 데이터와 함께 보관할 수도 있습니다.\\이러한 것들을 고려해서 답변을 드리겠습니다!\n",
      "\n",
      "[SFT + RM] (83자, RM점수: 1.297)\n",
      "제가 AI 언어 모델로써, 해당 질문에 대한 답변을 제공할 수 없습니다. 추가적인 정보가 있다면 알려주시면 더 자세한 도움을 드릴 수 있을 것입니다.\n",
      "================================================================================\n",
      "\n",
      "3단계 비교 평가 시트가 'three_way_comparison.csv'에 저장되었습니다.\n",
      "각 질문에 대해 3개 모델의 응답을 비교하여 평가하세요:\n",
      "- 최고_응답: A, B, C 중 가장 좋은 응답\n",
      "- 최악_응답: A, B, C 중 가장 나쁜 응답\n",
      "- 정확성_순위: A>B>C 또는 C>A>B 형식으로 순위\n",
      "\n",
      "=== 실험 완료 ===\n",
      "- 총 12개 질문 테스트\n",
      "- 각 질문에 대해 3가지 방법으로 응답 생성\n",
      "- CSV 파일에서 수동 평가 후 결과 분석 가능\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from chatgpt.models.base import RewardModel\n",
    "from transformers.models.gpt2.modeling_gpt2 import GPT2Model\n",
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "def load_all_three_models():\n",
    "    \"\"\"세 단계 모델 모두 로드\"\"\"\n",
    "    \n",
    "    # 토크나이저\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\n",
    "        'skt/kogpt2-base-v2',\n",
    "        bos_token='</s>', eos_token='</s>', unk_token='</s>', pad_token='</s>',\n",
    "        padding_side=\"right\", model_max_length=1024\n",
    "    )\n",
    "    \n",
    "    models = {}\n",
    "    \n",
    "    # 1. 기존 SFT 모델\n",
    "    try:\n",
    "        models['기존SFT'] = AutoModelForCausalLM.from_pretrained(\"models/output_1_SFT\")\n",
    "        print(\"✓ 기존 SFT 모델 로드 성공\")\n",
    "    except Exception as e:\n",
    "        print(f\"✗ 기존 SFT 모델 로드 실패: {e}\")\n",
    "        return None\n",
    "    \n",
    "    # 2. 개선 SFT 모델\n",
    "    try:\n",
    "        models['개선SFT'] = AutoModelForCausalLM.from_pretrained(\"models/improved_sft_v2\")\n",
    "        print(\"✓ 개선 SFT 모델 로드 성공\")\n",
    "    except Exception as e:\n",
    "        print(f\"✗ 개선 SFT 모델 로드 실패: {e}\")\n",
    "        return None\n",
    "    \n",
    "    # 3. RM 모델\n",
    "    try:\n",
    "        from transformers import GPT2Config\n",
    "        checkpoint = torch.load('models/fixed_rm/reward_model.pt')\n",
    "        \n",
    "        # RM 모델 재구성\n",
    "        config = GPT2Config.from_dict(checkpoint['config_dict'])\n",
    "        gpt2_model = GPT2Model(config)\n",
    "        gpt2_model.resize_token_embeddings(len(tokenizer))\n",
    "        \n",
    "        value_head = nn.Linear(config.n_embd, 1)\n",
    "        rm_model = RewardModel(gpt2_model, value_head)\n",
    "        \n",
    "        # 가중치 로드\n",
    "        rm_model.model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        rm_model.value_head.load_state_dict(checkpoint['value_head_state_dict'])\n",
    "        \n",
    "        models['RM'] = rm_model\n",
    "        print(\"✓ RM 모델 로드 성공\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"✗ RM 모델 로드 실패: {e}\")\n",
    "        models['RM'] = None\n",
    "    \n",
    "    # GPU로 이동\n",
    "    if torch.cuda.is_available():\n",
    "        for name, model in models.items():\n",
    "            if model is not None:\n",
    "                models[name] = model.cuda()\n",
    "    \n",
    "    return models, tokenizer\n",
    "\n",
    "def generate_with_rm_selection(sft_model, rm_model, tokenizer, question, num_candidates=3):\n",
    "    \"\"\"RM 점수를 이용한 베스트 응답 선택\"\"\"\n",
    "    \n",
    "    prompt = f\"### Instruction(명령어):\\n{question}\\n\\n### Response(응답):\"\n",
    "    inputs = tokenizer.encode(prompt, return_tensors='pt')\n",
    "    \n",
    "    if torch.cuda.is_available():\n",
    "        inputs = inputs.cuda()\n",
    "    \n",
    "    candidates = []\n",
    "    \n",
    "    # 여러 후보 응답 생성\n",
    "    for _ in range(num_candidates):\n",
    "        with torch.no_grad():\n",
    "            outputs = sft_model.generate(\n",
    "                inputs,\n",
    "                max_length=inputs.shape[1] + 120,\n",
    "                do_sample=True,\n",
    "                temperature=0.7,  # 다양성을 위해 조금 높게\n",
    "                top_k=50,\n",
    "                top_p=0.9,\n",
    "                repetition_penalty=1.2,\n",
    "                pad_token_id=tokenizer.pad_token_id\n",
    "            )\n",
    "        \n",
    "        response = tokenizer.decode(outputs[0][inputs.shape[1]:], skip_special_tokens=True)\n",
    "        response = response.replace(\"'\", \"\").strip()\n",
    "        candidates.append(response)\n",
    "    \n",
    "    # RM으로 각 후보의 점수 계산\n",
    "    best_response = \"\"\n",
    "    best_score = float('-inf')\n",
    "    \n",
    "    for candidate in candidates:\n",
    "        # RM 입력용 텍스트 (질문 + 응답)\n",
    "        full_text = prompt + candidate\n",
    "        rm_inputs = tokenizer.encode(full_text, return_tensors='pt', truncation=True, max_length=512)\n",
    "        \n",
    "        if torch.cuda.is_available():\n",
    "            rm_inputs = rm_inputs.cuda()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            score = rm_model(rm_inputs).cpu().item()\n",
    "        \n",
    "        if score > best_score:\n",
    "            best_score = score\n",
    "            best_response = candidate\n",
    "    \n",
    "    return best_response, best_score\n",
    "\n",
    "def generate_standard_response(model, tokenizer, question):\n",
    "    \"\"\"표준 응답 생성\"\"\"\n",
    "    prompt = f\"### Instruction(명령어):\\n{question}\\n\\n### Response(응답):\"\n",
    "    inputs = tokenizer.encode(prompt, return_tensors='pt')\n",
    "    \n",
    "    if torch.cuda.is_available():\n",
    "        inputs = inputs.cuda()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            inputs,\n",
    "            max_length=inputs.shape[1] + 120,\n",
    "            do_sample=True,\n",
    "            temperature=0.4,\n",
    "            top_k=25,\n",
    "            top_p=0.8,\n",
    "            repetition_penalty=1.3,\n",
    "            no_repeat_ngram_size=3,\n",
    "            pad_token_id=tokenizer.pad_token_id\n",
    "        )\n",
    "    \n",
    "    response = tokenizer.decode(outputs[0][inputs.shape[1]:], skip_special_tokens=True)\n",
    "    return response.replace(\"'\", \"\").strip()\n",
    "\n",
    "def run_three_way_comparison():\n",
    "    \"\"\"3단계 모델 비교 실험\"\"\"\n",
    "    \n",
    "    models, tokenizer = load_all_three_models()\n",
    "    \n",
    "    if models is None or models['RM'] is None:\n",
    "        print(\"모델 로드 실패\")\n",
    "        return None\n",
    "    \n",
    "    # 비교용 질문들 (다양한 유형)\n",
    "    test_questions = [\n",
    "        # 사실 질문\n",
    "        {\"question\": \"대한민국의 수도는?\", \"type\": \"factual\"},\n",
    "        {\"question\": \"지구에서 가장 높은 산은?\", \"type\": \"factual\"},\n",
    "        {\"question\": \"1더하기 1은?\", \"type\": \"factual\"},\n",
    "        \n",
    "        # 설명 질문  \n",
    "        {\"question\": \"인공지능이 뭐야?\", \"type\": \"explanation\"},\n",
    "        {\"question\": \"지구 온난화란?\", \"type\": \"explanation\"},\n",
    "        {\"question\": \"블록체인 기술이란?\", \"type\": \"explanation\"},\n",
    "        \n",
    "        # 방법 질문\n",
    "        {\"question\": \"김치찌개 만드는 방법\", \"type\": \"how_to\"},\n",
    "        {\"question\": \"감기 걸렸을 때 대처법\", \"type\": \"how_to\"},\n",
    "        {\"question\": \"영어 공부 방법\", \"type\": \"how_to\"},\n",
    "        \n",
    "        # 조언 질문\n",
    "        {\"question\": \"스트레스 해소 방법은?\", \"type\": \"advice\"},\n",
    "        {\"question\": \"시간 관리 팁\", \"type\": \"advice\"},\n",
    "        {\"question\": \"좋은 책 추천해줘\", \"type\": \"advice\"}\n",
    "    ]\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    print(\"3단계 모델 비교 실험 시작...\")\n",
    "    print(f\"총 {len(test_questions)}개 질문 테스트\")\n",
    "    \n",
    "    for i, q_data in enumerate(test_questions):\n",
    "        question = q_data[\"question\"]\n",
    "        q_type = q_data[\"type\"]\n",
    "        \n",
    "        print(f\"\\n진행률: {i+1}/{len(test_questions)} - {question}\")\n",
    "        \n",
    "        # 1. 기존 SFT 응답\n",
    "        response_old = generate_standard_response(models['기존SFT'], tokenizer, question)\n",
    "        \n",
    "        # 2. 개선 SFT 응답  \n",
    "        response_new = generate_standard_response(models['개선SFT'], tokenizer, question)\n",
    "        \n",
    "        # 3. SFT + RM 응답 (개선 SFT + RM 선택)\n",
    "        response_rm, rm_score = generate_with_rm_selection(\n",
    "            models['개선SFT'], models['RM'], tokenizer, question\n",
    "        )\n",
    "        \n",
    "        result = {\n",
    "            \"질문번호\": i+1,\n",
    "            \"질문\": question,\n",
    "            \"유형\": q_type,\n",
    "            \"기존SFT_응답\": response_old,\n",
    "            \"개선SFT_응답\": response_new,\n",
    "            \"SFT_RM_응답\": response_rm,\n",
    "            \"RM_점수\": rm_score,\n",
    "            \"기존SFT_길이\": len(response_old),\n",
    "            \"개선SFT_길이\": len(response_new),\n",
    "            \"SFT_RM_길이\": len(response_rm)\n",
    "        }\n",
    "        \n",
    "        results.append(result)\n",
    "    \n",
    "    return results\n",
    "\n",
    "def create_three_way_evaluation_sheet(results):\n",
    "    \"\"\"3단계 비교 평가 시트 생성\"\"\"\n",
    "    \n",
    "    eval_data = []\n",
    "    \n",
    "    for result in results:\n",
    "        # 모델 순서 랜덤화 (블라인드 테스트)\n",
    "        models = [\"기존SFT\", \"개선SFT\", \"SFT+RM\"]\n",
    "        responses = [\n",
    "            result[\"기존SFT_응답\"],\n",
    "            result[\"개선SFT_응답\"], \n",
    "            result[\"SFT_RM_응답\"]\n",
    "        ]\n",
    "        lengths = [\n",
    "            result[\"기존SFT_길이\"],\n",
    "            result[\"개선SFT_길이\"],\n",
    "            result[\"SFT_RM_길이\"]\n",
    "        ]\n",
    "        \n",
    "        # 무작위 섞기\n",
    "        combined = list(zip(models, responses, lengths))\n",
    "        random.shuffle(combined)\n",
    "        \n",
    "        shuffled_models, shuffled_responses, shuffled_lengths = zip(*combined)\n",
    "        \n",
    "        eval_data.append({\n",
    "            \"질문번호\": result[\"질문번호\"],\n",
    "            \"질문\": result[\"질문\"],\n",
    "            \"유형\": result[\"유형\"],\n",
    "            \"모델A\": shuffled_models[0],\n",
    "            \"모델A_응답\": shuffled_responses[0],\n",
    "            \"모델A_길이\": shuffled_lengths[0],\n",
    "            \"모델B\": shuffled_models[1], \n",
    "            \"모델B_응답\": shuffled_responses[1],\n",
    "            \"모델B_길이\": shuffled_lengths[1],\n",
    "            \"모델C\": shuffled_models[2],\n",
    "            \"모델C_응답\": shuffled_responses[2], \n",
    "            \"모델C_길이\": shuffled_lengths[2],\n",
    "            \"RM_점수\": result[\"RM_점수\"],\n",
    "            \"최고_응답\": \"\",  # A, B, C 중 선택\n",
    "            \"최악_응답\": \"\",  # A, B, C 중 선택\n",
    "            \"정확성_순위\": \"\",  # A>B>C 형식\n",
    "            \"유용성_순위\": \"\",  # A>B>C 형식  \n",
    "            \"메모\": \"\"\n",
    "        })\n",
    "    \n",
    "    df = pd.DataFrame(eval_data)\n",
    "    df.to_csv(\"three_way_comparison.csv\", index=False, encoding='utf-8-sig')\n",
    "    \n",
    "    print(f\"\\n3단계 비교 평가 시트가 'three_way_comparison.csv'에 저장되었습니다.\")\n",
    "    print(\"각 질문에 대해 3개 모델의 응답을 비교하여 평가하세요:\")\n",
    "    print(\"- 최고_응답: A, B, C 중 가장 좋은 응답\")\n",
    "    print(\"- 최악_응답: A, B, C 중 가장 나쁜 응답\") \n",
    "    print(\"- 정확성_순위: A>B>C 또는 C>A>B 형식으로 순위\")\n",
    "\n",
    "def display_comparison_preview(results, num_examples=3):\n",
    "    \"\"\"비교 결과 미리보기\"\"\"\n",
    "    \n",
    "    print(f\"\\n=== 3단계 모델 비교 미리보기 ===\")\n",
    "    \n",
    "    for i in range(min(num_examples, len(results))):\n",
    "        result = results[i]\n",
    "        \n",
    "        print(f\"\\n【질문 {result['질문번호']}】 {result['질문']} (유형: {result['유형']})\")\n",
    "        print(\"-\" * 80)\n",
    "        \n",
    "        print(f\"[기존 SFT] ({result['기존SFT_길이']}자)\")\n",
    "        print(f\"{result['기존SFT_응답']}\")\n",
    "        \n",
    "        print(f\"\\n[개선 SFT] ({result['개선SFT_길이']}자)\")\n",
    "        print(f\"{result['개선SFT_응답']}\")\n",
    "        \n",
    "        print(f\"\\n[SFT + RM] ({result['SFT_RM_길이']}자, RM점수: {result['RM_점수']:.3f})\")\n",
    "        print(f\"{result['SFT_RM_응답']}\")\n",
    "        print(\"=\" * 80)\n",
    "\n",
    "def main():\n",
    "    \"\"\"메인 실행\"\"\"\n",
    "    print(\"=== 3단계 RLHF 모델 비교 ===\")\n",
    "    print(\"Base → SFT → SFT+RM 단계별 성능 비교\")\n",
    "    \n",
    "    # 시드 설정\n",
    "    random.seed(42)\n",
    "    \n",
    "    # 비교 실험 실행\n",
    "    results = run_three_way_comparison()\n",
    "    \n",
    "    if results:\n",
    "        # 미리보기 출력\n",
    "        display_comparison_preview(results)\n",
    "        \n",
    "        # 평가 시트 생성\n",
    "        create_three_way_evaluation_sheet(results)\n",
    "        \n",
    "        print(f\"\\n=== 실험 완료 ===\")\n",
    "        print(f\"- 총 {len(results)}개 질문 테스트\")\n",
    "        print(f\"- 각 질문에 대해 3가지 방법으로 응답 생성\")\n",
    "        print(f\"- CSV 파일에서 수동 평가 후 결과 분석 가능\")\n",
    "        \n",
    "    else:\n",
    "        print(\"실험 실패\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b3b821e4-14be-48ef-b0ae-3590fd0e5b96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "3단계 모델 비교 결과 분석\n",
      "==================================================\n",
      "\n",
      "최고 응답 승률:\n",
      "  기존SFT: 3/12 (25.0%)\n",
      "  개선SFT: 2/12 (16.7%)\n",
      "  SFT+RM: 7/12 (58.3%)\n",
      "\n",
      "유형별 최고 응답:\n",
      "\n",
      "  factual 유형:\n",
      "    기존SFT: 1/3 (33.3%)\n",
      "    개선SFT: 0/3 (0.0%)\n",
      "    SFT+RM: 2/3 (66.7%)\n",
      "\n",
      "  explanation 유형:\n",
      "    기존SFT: 1/3 (33.3%)\n",
      "    개선SFT: 0/3 (0.0%)\n",
      "    SFT+RM: 2/3 (66.7%)\n",
      "\n",
      "  how_to 유형:\n",
      "    기존SFT: 1/3 (33.3%)\n",
      "    개선SFT: 1/3 (33.3%)\n",
      "    SFT+RM: 1/3 (33.3%)\n",
      "\n",
      "  advice 유형:\n",
      "    기존SFT: 0/3 (0.0%)\n",
      "    개선SFT: 1/3 (33.3%)\n",
      "    SFT+RM: 2/3 (66.7%)\n",
      "\n",
      "결론:\n",
      "  SFT+RM가 58.3% 승률로 최고 성능\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def analyze_three_way_results(filename=\"three_way_comparison.csv\"):\n",
    "    \"\"\"3-way 비교 결과 간단 분석\"\"\"\n",
    "    \n",
    "    try:\n",
    "        df = pd.read_csv(filename, encoding='utf-8-sig')\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*50)\n",
    "        print(\"3단계 모델 비교 결과 분석\")\n",
    "        print(\"=\"*50)\n",
    "        \n",
    "        # 최고 응답 분석\n",
    "        if \"최고_응답\" in df.columns:\n",
    "            model_wins = {\"기존SFT\": 0, \"개선SFT\": 0, \"SFT+RM\": 0}\n",
    "            total = 0\n",
    "            \n",
    "            for _, row in df.iterrows():\n",
    "                if pd.isna(row[\"최고_응답\"]) or row[\"최고_응답\"] == \"\":\n",
    "                    continue\n",
    "                    \n",
    "                choice = row[\"최고_응답\"]\n",
    "                if choice == \"A\":\n",
    "                    winner = row[\"모델A\"]\n",
    "                elif choice == \"B\":\n",
    "                    winner = row[\"모델B\"]\n",
    "                elif choice == \"C\":\n",
    "                    winner = row[\"모델C\"]\n",
    "                else:\n",
    "                    continue\n",
    "                \n",
    "                if winner in model_wins:\n",
    "                    model_wins[winner] += 1\n",
    "                    total += 1\n",
    "            \n",
    "            print(f\"\\n최고 응답 승률:\")\n",
    "            for model, wins in model_wins.items():\n",
    "                percentage = (wins/total*100) if total > 0 else 0\n",
    "                print(f\"  {model}: {wins}/{total} ({percentage:.1f}%)\")\n",
    "        \n",
    "        # 유형별 분석\n",
    "        if \"최고_응답\" in df.columns and \"유형\" in df.columns:\n",
    "            print(f\"\\n유형별 최고 응답:\")\n",
    "            for q_type in df[\"유형\"].unique():\n",
    "                type_df = df[df[\"유형\"] == q_type]\n",
    "                print(f\"\\n  {q_type} 유형:\")\n",
    "                \n",
    "                type_wins = {\"기존SFT\": 0, \"개선SFT\": 0, \"SFT+RM\": 0}\n",
    "                type_total = 0\n",
    "                \n",
    "                for _, row in type_df.iterrows():\n",
    "                    if pd.isna(row[\"최고_응답\"]) or row[\"최고_응답\"] == \"\":\n",
    "                        continue\n",
    "                        \n",
    "                    choice = row[\"최고_응답\"]\n",
    "                    if choice == \"A\":\n",
    "                        winner = row[\"모델A\"]\n",
    "                    elif choice == \"B\":\n",
    "                        winner = row[\"모델B\"]\n",
    "                    elif choice == \"C\":\n",
    "                        winner = row[\"모델C\"]\n",
    "                    else:\n",
    "                        continue\n",
    "                    \n",
    "                    if winner in type_wins:\n",
    "                        type_wins[winner] += 1\n",
    "                        type_total += 1\n",
    "                \n",
    "                for model, wins in type_wins.items():\n",
    "                    percentage = (wins/type_total*100) if type_total > 0 else 0\n",
    "                    print(f\"    {model}: {wins}/{type_total} ({percentage:.1f}%)\")\n",
    "        \n",
    "        # 통계적 의미\n",
    "        if total >= 10:\n",
    "            best_model = max(model_wins, key=model_wins.get)\n",
    "            best_rate = model_wins[best_model] / total\n",
    "            \n",
    "            print(f\"\\n결론:\")\n",
    "            if best_rate > 0.5:\n",
    "                print(f\"  {best_model}가 {best_rate:.1%} 승률로 최고 성능\")\n",
    "            else:\n",
    "                print(f\"  모델 간 성능 차이 미미 (최고: {best_model} {best_rate:.1%})\")\n",
    "                \n",
    "    except FileNotFoundError:\n",
    "        print(\"three_way_comparison.csv 파일을 찾을 수 없습니다.\")\n",
    "    except Exception as e:\n",
    "        print(f\"분석 오류: {e}\")\n",
    "\n",
    "# 실행\n",
    "if __name__ == \"__main__\":\n",
    "    analyze_three_way_results()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21d12802-5e7e-4170-b688-e4f77322046d",
   "metadata": {},
   "source": [
    "# PPO 진행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5702cecd-b231-4b88-9cb0-64c3b08299e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "완전한 RLHF 파이프라인 실험을 시작합니다.\n",
      "단계: Base → SFT → SFT+RM → PPO\n",
      "=== PPO 학습 시작 ===\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "59787c33a99b4f2ba49ae9647118cd8a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2fbb040dd22d477ebc5f1d394dd382e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PPO 학습용 모델들 로드 중...\n",
      "✓ Actor 모델 로드\n",
      "✓ Critic 모델 로드\n",
      "✓ Reward 모델 로드\n",
      "✓ Initial 모델 생성\n",
      "PPO 학습용 프롬프트: 1000개\n",
      "PPO 학습 시작...\n",
      "예상 소요시간: 20-30분\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad8dbf6b60cd44d5973ccc85846c3082",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Episode [1/5]:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0ac0446665714375b8edbb26925332ed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train epoch [1/1]:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b3cf2cf489e44f1f9c7ca44ed32b1743",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Episode [2/5]:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "974e41022b914ad5b9f3f497de66c217",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train epoch [1/1]:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "90c9a580f40643ff9a4b064861dee4e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Episode [3/5]:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "648505c343284c698a5944598c26b589",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train epoch [1/1]:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d0ce636cdedf41668f26fcaa09be4cc3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Episode [4/5]:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d9ba6d184c54a4091a101655178e4dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train epoch [1/1]:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5362f3cadaf248f3a2241ac04dcdd50f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Episode [5/5]:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "47734bf3eb914997ac2e54032ce91dc1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train epoch [1/1]:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PPO 학습 완료!\n",
      "\n",
      "PPO 학습 완료! 4단계 비교 시작...\n",
      "=== 4단계 RLHF 완전 비교 ===\n",
      "4개 모델 모두 로드 성공\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RM 모델도 로드 성공\n",
      "4단계 비교 테스트 시작... (8개 질문)\n",
      "진행률: 1/8 - 인공지능이 뭐야?\n",
      "진행률: 2/8 - 김치찌개 만드는 방법\n",
      "진행률: 3/8 - 스트레스 해소 방법은?\n",
      "진행률: 4/8 - 대한민국의 수도는?\n",
      "진행률: 5/8 - 파이썬 리스트와 튜플 차이점\n",
      "진행률: 6/8 - 영어 공부 방법\n",
      "진행률: 7/8 - 시간 관리 팁\n",
      "진행률: 8/8 - 지구에서 가장 높은 산은?\n",
      "4단계 비교 평가 시트가 'four_way_comparison.csv'에 저장되었습니다.\n",
      "\n",
      "=== 실험 완료 ===\n",
      "완전한 RLHF 파이프라인 구축 및 비교 완료\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from chatgpt.models.gpt import GPTActor, GPTCritic\n",
    "from chatgpt.models.base import RewardModel\n",
    "from chatgpt.trainer import PPOTrainer\n",
    "from chatgpt.trainer.strategies import NaiveStrategy\n",
    "from transformers.models.gpt2.modeling_gpt2 import GPT2Model\n",
    "from copy import deepcopy\n",
    "import json\n",
    "import random\n",
    "import pandas as pd\n",
    "\n",
    "def load_models_for_ppo():\n",
    "    \"\"\"PPO 학습을 위한 모델들 로드\"\"\"\n",
    "    \n",
    "    tokenizer = AutoTokenizer.from_pretrained(\n",
    "        'skt/kogpt2-base-v2',\n",
    "        bos_token='</s>', eos_token='</s>', unk_token='</s>', pad_token='</s>',\n",
    "        padding_side=\"right\", model_max_length=512\n",
    "    )\n",
    "    \n",
    "    print(\"PPO 학습용 모델들 로드 중...\")\n",
    "    \n",
    "    # 1. Actor (개선된 SFT 모델 기반)\n",
    "    with NaiveStrategy().model_init_context():\n",
    "        actor = GPTActor(pretrained='models/improved_sft_v2', lora_rank=0)\n",
    "        print(\"✓ Actor 모델 로드\")\n",
    "    \n",
    "    # 2. Critic (RM 모델과 동일한 구조)\n",
    "    try:\n",
    "        checkpoint = torch.load('models/fixed_rm/reward_model.pt')\n",
    "        \n",
    "        with NaiveStrategy().model_init_context():\n",
    "            # Critic은 RM과 동일한 구조 사용\n",
    "            from transformers import GPT2Config\n",
    "            config = GPT2Config.from_dict(checkpoint['config_dict'])\n",
    "            \n",
    "            critic = GPTCritic(pretrained='models/improved_sft_v2', lora_rank=0)\n",
    "            print(\"✓ Critic 모델 로드\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Critic 로드 실패: {e}\")\n",
    "        return None\n",
    "    \n",
    "    # 3. Reward Model (이미 학습된 것)\n",
    "    try:\n",
    "        gpt2_model = GPT2Model(config)\n",
    "        gpt2_model.resize_token_embeddings(len(tokenizer))\n",
    "        value_head = nn.Linear(config.n_embd, 1)\n",
    "        reward_model = RewardModel(gpt2_model, value_head)\n",
    "        \n",
    "        reward_model.model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        reward_model.value_head.load_state_dict(checkpoint['value_head_state_dict'])\n",
    "        print(\"✓ Reward 모델 로드\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Reward 모델 로드 실패: {e}\")\n",
    "        return None\n",
    "    \n",
    "    # 4. Initial Model (참조용)\n",
    "    initial_model = deepcopy(actor)\n",
    "    print(\"✓ Initial 모델 생성\")\n",
    "    \n",
    "    return actor, critic, reward_model, initial_model, tokenizer\n",
    "\n",
    "def prepare_ppo_data():\n",
    "    \"\"\"PPO 학습용 프롬프트 데이터 준비\"\"\"\n",
    "    \n",
    "    with open('KoChatGPT/data_kochatgpt/kochatgpt_3_PPO.jsonl', \"r\", encoding='utf-8-sig') as f:\n",
    "        list_data_dict = json.load(f)\n",
    "    \n",
    "    prompts = [item['prompt'] for item in list_data_dict]\n",
    "    \n",
    "    # 빠른 학습을 위해 일부만 사용\n",
    "    random.seed(42)\n",
    "    if len(prompts) > 1000:\n",
    "        prompts = random.sample(prompts, 1000)\n",
    "    \n",
    "    print(f\"PPO 학습용 프롬프트: {len(prompts)}개\")\n",
    "    return prompts\n",
    "\n",
    "def tokenize_fn_ppo(texts, tokenizer):\n",
    "    \"\"\"PPO용 토크나이징 함수\"\"\"\n",
    "    batch = tokenizer(texts, return_tensors='pt', max_length=96, padding=True, truncation=True)\n",
    "    return {k: v.cuda() if torch.cuda.is_available() else v for k, v in batch.items()}\n",
    "\n",
    "def train_ppo_model():\n",
    "    \"\"\"PPO 모델 학습\"\"\"\n",
    "    \n",
    "    print(\"=== PPO 학습 시작 ===\")\n",
    "    \n",
    "    # 모델들 로드\n",
    "    models_result = load_models_for_ppo()\n",
    "    if models_result is None:\n",
    "        print(\"PPO 모델 로드 실패\")\n",
    "        return None\n",
    "    \n",
    "    actor, critic, reward_model, initial_model, tokenizer = models_result\n",
    "    \n",
    "    # GPU로 이동\n",
    "    if torch.cuda.is_available():\n",
    "        actor = actor.cuda()\n",
    "        critic = critic.cuda()\n",
    "        reward_model = reward_model.cuda()\n",
    "        initial_model = initial_model.cuda()\n",
    "    \n",
    "    # 옵티마이저\n",
    "    actor_optim = torch.optim.Adam(actor.parameters(), lr=1e-6)  # 매우 작은 학습률\n",
    "    critic_optim = torch.optim.Adam(critic.parameters(), lr=1e-6)\n",
    "    \n",
    "    # Strategy로 준비\n",
    "    (actor, actor_optim), (critic, critic_optim), reward_model, initial_model = NaiveStrategy().prepare(\n",
    "        (actor, actor_optim), (critic, critic_optim), reward_model, initial_model\n",
    "    )\n",
    "    \n",
    "    # 프롬프트 데이터\n",
    "    prompts = prepare_ppo_data()\n",
    "    \n",
    "    # PPO 트레이너\n",
    "    trainer = PPOTrainer(\n",
    "        strategy=NaiveStrategy(),\n",
    "        actor=actor,\n",
    "        critic=critic,\n",
    "        reward_model=reward_model,\n",
    "        initial_model=initial_model,\n",
    "        actor_optim=actor_optim,\n",
    "        critic_optim=critic_optim,\n",
    "        max_epochs=1,  # 빠른 테스트용\n",
    "        train_batch_size=8,\n",
    "        tokenizer=lambda texts: tokenize_fn_ppo(texts, tokenizer),\n",
    "        max_length=128,\n",
    "        do_sample=True,\n",
    "        temperature=0.7,\n",
    "        top_k=50,\n",
    "        pad_token_id=tokenizer.pad_token_id,\n",
    "        eos_token_id=tokenizer.eos_token_id\n",
    "    )\n",
    "    \n",
    "    print(\"PPO 학습 시작...\")\n",
    "    print(\"예상 소요시간: 20-30분\")\n",
    "    \n",
    "    try:\n",
    "        trainer.fit(\n",
    "            prompts,\n",
    "            num_episodes=5,  # 에피소드 수 줄임\n",
    "            max_timesteps=3,\n",
    "            update_timesteps=3\n",
    "        )\n",
    "        \n",
    "        # 모델 저장\n",
    "        actor.model.save_pretrained('models/ppo_final')\n",
    "        tokenizer.save_pretrained('models/ppo_final')\n",
    "        \n",
    "        print(\"PPO 학습 완료!\")\n",
    "        return actor, tokenizer\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"PPO 학습 실패: {e}\")\n",
    "        return None, None\n",
    "\n",
    "def run_four_way_comparison():\n",
    "    \"\"\"4단계 완전 비교\"\"\"\n",
    "    \n",
    "    print(\"=== 4단계 RLHF 완전 비교 ===\")\n",
    "    \n",
    "    # 토크나이저\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\n",
    "        'skt/kogpt2-base-v2',\n",
    "        bos_token='</s>', eos_token='</s>', unk_token='</s>', pad_token='</s>',\n",
    "        padding_side=\"right\", model_max_length=1024\n",
    "    )\n",
    "    \n",
    "    # 모델들 로드\n",
    "    models = {}\n",
    "    \n",
    "    try:\n",
    "        # 1. 기존 SFT\n",
    "        models['기존SFT'] = AutoModelForCausalLM.from_pretrained(\"models/output_1_SFT\")\n",
    "        \n",
    "        # 2. 개선 SFT  \n",
    "        models['개선SFT'] = AutoModelForCausalLM.from_pretrained(\"models/improved_sft_v2\")\n",
    "        \n",
    "        # 3. PPO 모델\n",
    "        models['PPO'] = AutoModelForCausalLM.from_pretrained(\"models/ppo_final\")\n",
    "        \n",
    "        print(\"4개 모델 모두 로드 성공\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"모델 로드 실패: {e}\")\n",
    "        return None\n",
    "    \n",
    "    # GPU로 이동\n",
    "    if torch.cuda.is_available():\n",
    "        for name, model in models.items():\n",
    "            models[name] = model.cuda()\n",
    "    \n",
    "    # RM 모델도 로드 (SFT+RM용)\n",
    "    try:\n",
    "        from transformers import GPT2Config\n",
    "        checkpoint = torch.load('models/fixed_rm/reward_model.pt')\n",
    "        config = GPT2Config.from_dict(checkpoint['config_dict'])\n",
    "        \n",
    "        gpt2_model = GPT2Model(config)\n",
    "        gpt2_model.resize_token_embeddings(len(tokenizer))\n",
    "        value_head = nn.Linear(config.n_embd, 1)\n",
    "        rm_model = RewardModel(gpt2_model, value_head)\n",
    "        \n",
    "        rm_model.model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        rm_model.value_head.load_state_dict(checkpoint['value_head_state_dict'])\n",
    "        \n",
    "        if torch.cuda.is_available():\n",
    "            rm_model = rm_model.cuda()\n",
    "            \n",
    "        print(\"RM 모델도 로드 성공\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"RM 모델 로드 실패: {e}\")\n",
    "        rm_model = None\n",
    "    \n",
    "    # 테스트 질문들\n",
    "    test_questions = [\n",
    "        {\"question\": \"인공지능이 뭐야?\", \"type\": \"explanation\"},\n",
    "        {\"question\": \"김치찌개 만드는 방법\", \"type\": \"how_to\"},\n",
    "        {\"question\": \"스트레스 해소 방법은?\", \"type\": \"advice\"},\n",
    "        {\"question\": \"대한민국의 수도는?\", \"type\": \"factual\"},\n",
    "        {\"question\": \"파이썬 리스트와 튜플 차이점\", \"type\": \"explanation\"},\n",
    "        {\"question\": \"영어 공부 방법\", \"type\": \"how_to\"},\n",
    "        {\"question\": \"시간 관리 팁\", \"type\": \"advice\"},\n",
    "        {\"question\": \"지구에서 가장 높은 산은?\", \"type\": \"factual\"}\n",
    "    ]\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    print(f\"4단계 비교 테스트 시작... ({len(test_questions)}개 질문)\")\n",
    "    \n",
    "    for i, q_data in enumerate(test_questions):\n",
    "        question = q_data[\"question\"]\n",
    "        q_type = q_data[\"type\"]\n",
    "        \n",
    "        print(f\"진행률: {i+1}/{len(test_questions)} - {question}\")\n",
    "        \n",
    "        # 각 모델로 응답 생성\n",
    "        responses = {}\n",
    "        \n",
    "        for model_name, model in models.items():\n",
    "            prompt = f\"### Instruction(명령어):\\n{question}\\n\\n### Response(응답):\"\n",
    "            inputs = tokenizer.encode(prompt, return_tensors='pt')\n",
    "            \n",
    "            if torch.cuda.is_available():\n",
    "                inputs = inputs.cuda()\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                outputs = model.generate(\n",
    "                    inputs,\n",
    "                    max_length=inputs.shape[1] + 100,\n",
    "                    do_sample=True,\n",
    "                    temperature=0.4,\n",
    "                    top_k=25,\n",
    "                    top_p=0.8,\n",
    "                    repetition_penalty=1.2,\n",
    "                    pad_token_id=tokenizer.pad_token_id\n",
    "                )\n",
    "            \n",
    "            response = tokenizer.decode(outputs[0][inputs.shape[1]:], skip_special_tokens=True)\n",
    "            response = response.replace(\"'\", \"\").strip()\n",
    "            responses[model_name] = response\n",
    "        \n",
    "        # SFT+RM 응답 (개선SFT + RM 선택)\n",
    "        if rm_model is not None:\n",
    "            sft_rm_response, rm_score = generate_with_rm_selection_simple(\n",
    "                models['개선SFT'], rm_model, tokenizer, question\n",
    "            )\n",
    "            responses['SFT+RM'] = sft_rm_response\n",
    "        \n",
    "        result = {\n",
    "            \"질문번호\": i+1,\n",
    "            \"질문\": question,\n",
    "            \"유형\": q_type,\n",
    "            **{f\"{name}_응답\": resp for name, resp in responses.items()},\n",
    "            **{f\"{name}_길이\": len(resp) for name, resp in responses.items()}\n",
    "        }\n",
    "        \n",
    "        results.append(result)\n",
    "    \n",
    "    return results\n",
    "\n",
    "def generate_with_rm_selection_simple(sft_model, rm_model, tokenizer, question):\n",
    "    \"\"\"간단한 RM 선택 함수\"\"\"\n",
    "    prompt = f\"### Instruction(명령어):\\n{question}\\n\\n### Response(응답):\"\n",
    "    inputs = tokenizer.encode(prompt, return_tensors='pt')\n",
    "    \n",
    "    if torch.cuda.is_available():\n",
    "        inputs = inputs.cuda()\n",
    "    \n",
    "    candidates = []\n",
    "    for _ in range(3):\n",
    "        with torch.no_grad():\n",
    "            outputs = sft_model.generate(\n",
    "                inputs,\n",
    "                max_length=inputs.shape[1] + 100,\n",
    "                do_sample=True,\n",
    "                temperature=0.7,\n",
    "                top_k=50,\n",
    "                top_p=0.9,\n",
    "                pad_token_id=tokenizer.pad_token_id\n",
    "            )\n",
    "        response = tokenizer.decode(outputs[0][inputs.shape[1]:], skip_special_tokens=True)\n",
    "        candidates.append(response.replace(\"'\", \"\").strip())\n",
    "    \n",
    "    best_response = \"\"\n",
    "    best_score = float('-inf')\n",
    "    \n",
    "    for candidate in candidates:\n",
    "        full_text = prompt + candidate\n",
    "        rm_inputs = tokenizer.encode(full_text, return_tensors='pt', truncation=True, max_length=512)\n",
    "        if torch.cuda.is_available():\n",
    "            rm_inputs = rm_inputs.cuda()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            score = rm_model(rm_inputs).cpu().item()\n",
    "        \n",
    "        if score > best_score:\n",
    "            best_score = score\n",
    "            best_response = candidate\n",
    "    \n",
    "    return best_response, best_score\n",
    "\n",
    "def create_four_way_evaluation_sheet(results):\n",
    "    \"\"\"4단계 비교 평가 시트 생성\"\"\"\n",
    "    \n",
    "    eval_data = []\n",
    "    \n",
    "    for result in results:\n",
    "        # 4개 모델 응답 랜덤 배치\n",
    "        models = list(result.keys())\n",
    "        response_models = [k for k in models if k.endswith('_응답')]\n",
    "        model_names = [k.replace('_응답', '') for k in response_models]\n",
    "        \n",
    "        responses = [result[k] for k in response_models]\n",
    "        lengths = [result[k.replace('_응답', '_길이')] for k in response_models]\n",
    "        \n",
    "        # 랜덤 섞기\n",
    "        combined = list(zip(model_names, responses, lengths))\n",
    "        random.shuffle(combined)\n",
    "        shuffled_models, shuffled_responses, shuffled_lengths = zip(*combined)\n",
    "        \n",
    "        eval_data.append({\n",
    "            \"질문번호\": result[\"질문번호\"],\n",
    "            \"질문\": result[\"질문\"],\n",
    "            \"유형\": result[\"유형\"],\n",
    "            **{f\"모델{chr(65+i)}\": shuffled_models[i] for i in range(len(shuffled_models))},\n",
    "            **{f\"모델{chr(65+i)}_응답\": shuffled_responses[i] for i in range(len(shuffled_responses))},\n",
    "            **{f\"모델{chr(65+i)}_길이\": shuffled_lengths[i] for i in range(len(shuffled_lengths))},\n",
    "            \"최고_모델\": \"\",  # A, B, C, D 중 선택\n",
    "            \"최악_모델\": \"\",  # A, B, C, D 중 선택\n",
    "            \"순위\": \"\",      # A>B>C>D 형식\n",
    "            \"메모\": \"\"\n",
    "        })\n",
    "    \n",
    "    df = pd.DataFrame(eval_data)\n",
    "    df.to_csv(\"four_way_comparison.csv\", index=False, encoding='utf-8-sig')\n",
    "    \n",
    "    print(f\"4단계 비교 평가 시트가 'four_way_comparison.csv'에 저장되었습니다.\")\n",
    "\n",
    "def main():\n",
    "    \"\"\"메인 실행\"\"\"\n",
    "    print(\"완전한 RLHF 파이프라인 실험을 시작합니다.\")\n",
    "    print(\"단계: Base → SFT → SFT+RM → PPO\")\n",
    "    \n",
    "    # PPO 학습\n",
    "    ppo_model, tokenizer = train_ppo_model()\n",
    "    \n",
    "    if ppo_model is not None:\n",
    "        print(\"\\nPPO 학습 완료! 4단계 비교 시작...\")\n",
    "        \n",
    "        # 4단계 비교\n",
    "        results = run_four_way_comparison()\n",
    "        \n",
    "        if results:\n",
    "            create_four_way_evaluation_sheet(results)\n",
    "            print(f\"\\n=== 실험 완료 ===\")\n",
    "            print(f\"완전한 RLHF 파이프라인 구축 및 비교 완료\")\n",
    "        else:\n",
    "            print(\"4단계 비교 실패\")\n",
    "    else:\n",
    "        print(\"PPO 학습 실패로 실험 중단\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f650423e-ce61-4a15-971f-1206a54f0e0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "3단계 모델 비교 결과 분석\n",
      "==================================================\n",
      "\n",
      "최고 응답 승률:\n",
      "  기존SFT: 3/12 (25.0%)\n",
      "  개선SFT: 2/12 (16.7%)\n",
      "  SFT+RM: 7/12 (58.3%)\n",
      "\n",
      "유형별 최고 응답:\n",
      "\n",
      "  factual 유형:\n",
      "    기존SFT: 1/3 (33.3%)\n",
      "    개선SFT: 0/3 (0.0%)\n",
      "    SFT+RM: 2/3 (66.7%)\n",
      "\n",
      "  explanation 유형:\n",
      "    기존SFT: 1/3 (33.3%)\n",
      "    개선SFT: 0/3 (0.0%)\n",
      "    SFT+RM: 2/3 (66.7%)\n",
      "\n",
      "  how_to 유형:\n",
      "    기존SFT: 1/3 (33.3%)\n",
      "    개선SFT: 1/3 (33.3%)\n",
      "    SFT+RM: 1/3 (33.3%)\n",
      "\n",
      "  advice 유형:\n",
      "    기존SFT: 0/3 (0.0%)\n",
      "    개선SFT: 1/3 (33.3%)\n",
      "    SFT+RM: 2/3 (66.7%)\n",
      "\n",
      "결론:\n",
      "  SFT+RM가 58.3% 승률로 최고 성능\n"
     ]
    }
   ],
   "source": [
    "analyze_three_way_results()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "77631781-5b4a-4164-af3b-8106d658cd44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== SFT vs PPO 간단 비교 ===\n",
      "모델 로드 중...\n",
      "비교 진행 중... (5개 질문)\n",
      "\n",
      "=== 분석 결과 ===\n",
      "총 질문 수: 5\n",
      "\n",
      "RM 점수:\n",
      "  SFT 평균: 0.6991\n",
      "  PPO 평균: 0.5401\n",
      "  차이: -0.1590\n",
      "\n",
      "응답 길이:\n",
      "  SFT 평균: 173.6자\n",
      "  PPO 평균: 116.4자\n",
      "\n",
      "생성 시간:\n",
      "  SFT 평균: 0.63초\n",
      "  PPO 평균: 0.45초\n",
      "\n",
      "승부 결과:\n",
      "  PPO 승리: 2회 (40.0%)\n",
      "  SFT 승리: 3회 (60.0%)\n",
      "  무승부: 0회 (0.0%)\n",
      "\n",
      "통계적 검정:\n",
      "  t-statistic: -0.9263\n",
      "  p-value: 0.4067\n",
      "  유의함: 아니오\n",
      "\n",
      "=== 대표 사례 ===\n",
      "\n",
      "1. PPO가 크게 이긴 사례:\n",
      "\n",
      "1. 인공지능이 뭐야?\n",
      "   점수차: +0.2772\n",
      "   SFT: '저는 인공지능 어시스턴트이기 때문에, 인간의 지능에 대한 개인적인 의견을 제공할 수는 없지만, 인간과 유사한 생각을 가진 분들이 계시면 답변을...\n",
      "   PPO: '인공지능은 인간의 지식과 경험에 대한 이해와 응용, 그리고 이를 이용하여 새로운 비즈니스 모델과 서비스를 창출하는 기술입니다. 이러한 인공 지...\n",
      "\n",
      "2. 불고기용 고기 한우에요?\n",
      "   점수차: +0.1987\n",
      "   SFT: '저는 인공지능 언어모델이며, 저는 자연어 처리를 하지 않습니다. 하지만, 일반적으로 불고기의 소스와 양념, 그리고 쇠고기 요리에 사용되는 재료...\n",
      "   PPO: '저는 인공지능 언어모델로써 답변을 제공하는 AI 어시스턴트입니다.\\n\\n죄송하지만, 저는 식당에서의 고기를 판매하는 가게가 아니므로 해당 식당...\n",
      "\n",
      "2. SFT가 크게 이긴 사례:\n",
      "\n",
      "1. 리처드 닉슨이 43대 부통령직을 수행한 년도는?\n",
      "   점수차: -0.6255\n",
      "   SFT: '저는 이전에 검색한 데이터에 의해 리처드 닉슨의 47대 부통령직 수행년도가 정확히 파악할 수 없습니다. 추가적인 정보가 제공되면 더 정확한 답...\n",
      "   PPO: '리처드 닉슨은 41대 부통령직을 수행했습니다....\n",
      "\n",
      "2. 오늘 미세먼지 어때?\n",
      "   점수차: -0.3305\n",
      "   SFT: '제가 인공지능 언어모델이므로, 오늘 미세먼지는 어떤 문제가 있는지 정확하게 파악하지 못합니다. 그러나 일반적으로 실내 공기 질이 악화되면 건강...\n",
      "   PPO: '미세먼지는 건강에 해로울 수 있으며, 외출 시 마스크를 착용하고 환기하는 것이 좋습니다. 또한, 실외에서 장시간 작업을 하거나 먼지가 많은 곳...\n",
      "\n",
      "3. 시카고 오헤어 국제공항은 어디에 있어?\n",
      "   점수차: -0.3151\n",
      "   SFT: '저는 인공지능 어시스턴트이므로 시카고 오헤어에 대한 정보를 가지고 있지 않습니다. 하지만, 일반적으로 시카고는 미국 내 주요 도시 중 하나입니...\n",
      "   PPO: '시카고 오헤어 국제공항은 미국 시카고의 중심부에 위치해 있습니다....\n",
      "\n",
      "=== 최종 결론 ===\n",
      "SFT가 60.0% 승률로 우승!\n",
      "통계적 유의성: 없음\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy import stats\n",
    "import time\n",
    "\n",
    "def load_comparison_models():\n",
    "    \"\"\"SFT와 PPO 모델 로드\"\"\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\n",
    "        'skt/kogpt2-base-v2',\n",
    "        bos_token='</s>', eos_token='</s>', unk_token='</s>', pad_token='</s>',\n",
    "        padding_side=\"right\", model_max_length=1024\n",
    "    )\n",
    "    \n",
    "    print(\"모델 로드 중...\")\n",
    "    \n",
    "    # SFT 모델\n",
    "    sft_model = AutoModelForCausalLM.from_pretrained(\"models/improved_sft_v2\")\n",
    "    \n",
    "    # PPO 모델\n",
    "    ppo_model = AutoModelForCausalLM.from_pretrained(\"models/ppo_final\")\n",
    "    \n",
    "    # RM 모델\n",
    "    checkpoint = torch.load('models/fixed_rm/reward_model.pt')\n",
    "    config = GPT2Config.from_dict(checkpoint['config_dict'])\n",
    "    \n",
    "    gpt2_model = GPT2Model(config)\n",
    "    gpt2_model.resize_token_embeddings(len(tokenizer))\n",
    "    value_head = nn.Linear(config.n_embd, 1)\n",
    "    rm_model = RewardModel(gpt2_model, value_head)\n",
    "    \n",
    "    rm_model.model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    rm_model.value_head.load_state_dict(checkpoint['value_head_state_dict'])\n",
    "    \n",
    "    if torch.cuda.is_available():\n",
    "        sft_model = sft_model.cuda()\n",
    "        ppo_model = ppo_model.cuda()\n",
    "        rm_model = rm_model.cuda()\n",
    "    \n",
    "    return sft_model, ppo_model, rm_model, tokenizer\n",
    "\n",
    "def load_test_questions():\n",
    "    \"\"\"코드에서 사용된 핵심 5개 질문\"\"\"\n",
    "    questions = [\n",
    "        \"불고기용 고기 한우에요?\",\n",
    "        \"리처드 닉슨이 43대 부통령직을 수행한 년도는?\", \n",
    "        \"시카고 오헤어 국제공항은 어디에 있어?\",\n",
    "        \"오늘 미세먼지 어때?\",\n",
    "        \"인공지능이 뭐야?\"\n",
    "    ]\n",
    "    \n",
    "    return questions\n",
    "\n",
    "def generate_response(model, tokenizer, question):\n",
    "    \"\"\"응답 생성\"\"\"\n",
    "    prompt = f\"### Instruction(명령어):\\n{question}\\n\\n### Response(응답):\"\n",
    "    inputs = tokenizer.encode(prompt, return_tensors='pt')\n",
    "    \n",
    "    if torch.cuda.is_available():\n",
    "        inputs = inputs.cuda()\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            inputs,\n",
    "            max_length=inputs.shape[1] + 120,\n",
    "            do_sample=True,\n",
    "            temperature=0.7,\n",
    "            top_k=50,\n",
    "            top_p=0.9,\n",
    "            repetition_penalty=1.2,\n",
    "            pad_token_id=tokenizer.pad_token_id\n",
    "        )\n",
    "    \n",
    "    generation_time = time.time() - start_time\n",
    "    response = tokenizer.decode(outputs[0][inputs.shape[1]:], skip_special_tokens=True)\n",
    "    \n",
    "    return response.strip(), generation_time\n",
    "\n",
    "def get_rm_score(rm_model, tokenizer, prompt, response):\n",
    "    \"\"\"RM 점수 계산\"\"\"\n",
    "    full_text = prompt + response\n",
    "    inputs = tokenizer.encode(full_text, return_tensors='pt', truncation=True, max_length=512)\n",
    "    \n",
    "    if torch.cuda.is_available():\n",
    "        inputs = inputs.cuda()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        score = rm_model(inputs).cpu().item()\n",
    "    \n",
    "    return score\n",
    "\n",
    "def run_simple_comparison():\n",
    "    \"\"\"간단한 비교 실행\"\"\"\n",
    "    print(\"=== SFT vs PPO 간단 비교 ===\")\n",
    "    \n",
    "    # 모델 로드\n",
    "    sft_model, ppo_model, rm_model, tokenizer = load_comparison_models()\n",
    "    \n",
    "    # 질문 로드\n",
    "    questions = load_test_questions()\n",
    "    \n",
    "    # 결과 저장\n",
    "    results = []\n",
    "    \n",
    "    print(f\"비교 진행 중... ({len(questions)}개 질문)\")\n",
    "    \n",
    "    for i, question in enumerate(questions):\n",
    "        if (i + 1) % 10 == 0:\n",
    "            print(f\"진행률: {i+1}/{len(questions)}\")\n",
    "        \n",
    "        prompt = f\"### Instruction(명령어):\\n{question}\\n\\n### Response(응답):\"\n",
    "        \n",
    "        # SFT 응답\n",
    "        sft_response, sft_time = generate_response(sft_model, tokenizer, question)\n",
    "        sft_score = get_rm_score(rm_model, tokenizer, prompt, sft_response)\n",
    "        \n",
    "        # PPO 응답\n",
    "        ppo_response, ppo_time = generate_response(ppo_model, tokenizer, question)\n",
    "        ppo_score = get_rm_score(rm_model, tokenizer, prompt, ppo_response)\n",
    "        \n",
    "        results.append({\n",
    "            'question': question,\n",
    "            'sft_response': sft_response,\n",
    "            'sft_score': sft_score,\n",
    "            'sft_length': len(sft_response),\n",
    "            'sft_time': sft_time,\n",
    "            'ppo_response': ppo_response,\n",
    "            'ppo_score': ppo_score,\n",
    "            'ppo_length': len(ppo_response),\n",
    "            'ppo_time': ppo_time,\n",
    "            'score_diff': ppo_score - sft_score\n",
    "        })\n",
    "    \n",
    "    return results\n",
    "\n",
    "def analyze_results(results):\n",
    "    \"\"\"결과 분석\"\"\"\n",
    "    print(\"\\n=== 분석 결과 ===\")\n",
    "    \n",
    "    # 기본 통계\n",
    "    sft_scores = [r['sft_score'] for r in results]\n",
    "    ppo_scores = [r['ppo_score'] for r in results]\n",
    "    score_diffs = [r['score_diff'] for r in results]\n",
    "    \n",
    "    sft_lengths = [r['sft_length'] for r in results]\n",
    "    ppo_lengths = [r['ppo_length'] for r in results]\n",
    "    \n",
    "    sft_times = [r['sft_time'] for r in results]\n",
    "    ppo_times = [r['ppo_time'] for r in results]\n",
    "    \n",
    "    print(f\"총 질문 수: {len(results)}\")\n",
    "    print(f\"\\nRM 점수:\")\n",
    "    print(f\"  SFT 평균: {np.mean(sft_scores):.4f}\")\n",
    "    print(f\"  PPO 평균: {np.mean(ppo_scores):.4f}\")\n",
    "    print(f\"  차이: {np.mean(score_diffs):+.4f}\")\n",
    "    \n",
    "    print(f\"\\n응답 길이:\")\n",
    "    print(f\"  SFT 평균: {np.mean(sft_lengths):.1f}자\")\n",
    "    print(f\"  PPO 평균: {np.mean(ppo_lengths):.1f}자\")\n",
    "    \n",
    "    print(f\"\\n생성 시간:\")\n",
    "    print(f\"  SFT 평균: {np.mean(sft_times):.2f}초\")\n",
    "    print(f\"  PPO 평균: {np.mean(ppo_times):.2f}초\")\n",
    "    \n",
    "    # 승부 결과\n",
    "    ppo_wins = sum(1 for r in results if r['score_diff'] > 0)\n",
    "    sft_wins = sum(1 for r in results if r['score_diff'] < 0)\n",
    "    ties = sum(1 for r in results if r['score_diff'] == 0)\n",
    "    \n",
    "    print(f\"\\n승부 결과:\")\n",
    "    print(f\"  PPO 승리: {ppo_wins}회 ({ppo_wins/len(results)*100:.1f}%)\")\n",
    "    print(f\"  SFT 승리: {sft_wins}회 ({sft_wins/len(results)*100:.1f}%)\")\n",
    "    print(f\"  무승부: {ties}회 ({ties/len(results)*100:.1f}%)\")\n",
    "    \n",
    "    # 통계적 유의성\n",
    "    t_stat, p_value = stats.ttest_rel(ppo_scores, sft_scores)\n",
    "    print(f\"\\n통계적 검정:\")\n",
    "    print(f\"  t-statistic: {t_stat:.4f}\")\n",
    "    print(f\"  p-value: {p_value:.4f}\")\n",
    "    print(f\"  유의함: {'예' if p_value < 0.05 else '아니오'}\")\n",
    "    \n",
    "    return {\n",
    "        'sft_scores': sft_scores,\n",
    "        'ppo_scores': ppo_scores,\n",
    "        'sft_lengths': sft_lengths,\n",
    "        'ppo_lengths': ppo_lengths,\n",
    "        'ppo_win_rate': ppo_wins/len(results),\n",
    "        'p_value': p_value\n",
    "    }\n",
    "\n",
    "def show_examples(results):\n",
    "    \"\"\"대표 사례 보기\"\"\"\n",
    "    print(\"\\n=== 대표 사례 ===\")\n",
    "    \n",
    "    # 점수 차이로 정렬\n",
    "    sorted_results = sorted(results, key=lambda x: abs(x['score_diff']), reverse=True)\n",
    "    \n",
    "    print(\"\\n1. PPO가 크게 이긴 사례:\")\n",
    "    ppo_big_wins = [r for r in sorted_results if r['score_diff'] > 0][:3]\n",
    "    for i, r in enumerate(ppo_big_wins, 1):\n",
    "        print(f\"\\n{i}. {r['question']}\")\n",
    "        print(f\"   점수차: {r['score_diff']:+.4f}\")\n",
    "        print(f\"   SFT: {r['sft_response'][:80]}...\")\n",
    "        print(f\"   PPO: {r['ppo_response'][:80]}...\")\n",
    "    \n",
    "    print(\"\\n2. SFT가 크게 이긴 사례:\")\n",
    "    sft_big_wins = [r for r in sorted_results if r['score_diff'] < 0][:3]\n",
    "    for i, r in enumerate(sft_big_wins, 1):\n",
    "        print(f\"\\n{i}. {r['question']}\")\n",
    "        print(f\"   점수차: {r['score_diff']:+.4f}\")\n",
    "        print(f\"   SFT: {r['sft_response'][:80]}...\")\n",
    "        print(f\"   PPO: {r['ppo_response'][:80]}...\")\n",
    "\n",
    "def main_comparison():\n",
    "    \"\"\"메인 비교 실행\"\"\"\n",
    "    results = run_simple_comparison()\n",
    "    stats = analyze_results(results)\n",
    "    show_examples(results)\n",
    "    \n",
    "    # 최종 결론\n",
    "    print(f\"\\n=== 최종 결론 ===\")\n",
    "    if stats['ppo_win_rate'] > 0.5:\n",
    "        winner = \"PPO\"\n",
    "        win_rate = stats['ppo_win_rate'] * 100\n",
    "    else:\n",
    "        winner = \"SFT\"\n",
    "        win_rate = (1 - stats['ppo_win_rate']) * 100\n",
    "    \n",
    "    print(f\"{winner}가 {win_rate:.1f}% 승률로 우승!\")\n",
    "    print(f\"통계적 유의성: {'있음' if stats['p_value'] < 0.05 else '없음'}\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "# 실행\n",
    "if __name__ == \"__main__\":\n",
    "    results = main_comparison()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d6eb17d7-6bf5-40c3-a165-3e0a93bc230b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Base vs PPO 간단 비교 ===\n",
      "모델 로드 중...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "64d878bbe6e442b29897714f709101a6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/513M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bdc4e2b91ea841bebdfd5456ced06d15",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/513M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "비교 진행 중... (5개 질문)\n",
      "\n",
      "=== 분석 결과 ===\n",
      "총 질문 수: 5\n",
      "\n",
      "RM 점수:\n",
      "  Base 평균: -0.0139\n",
      "  PPO 평균: 0.3348\n",
      "  차이: +0.3487\n",
      "\n",
      "응답 길이:\n",
      "  Base 평균: 209.2자\n",
      "  PPO 평균: 106.8자\n",
      "\n",
      "생성 시간:\n",
      "  Base 평균: 1.24초\n",
      "  PPO 평균: 0.40초\n",
      "\n",
      "승부 결과:\n",
      "  PPO 승리: 4회 (80.0%)\n",
      "  Base 승리: 1회 (20.0%)\n",
      "  무승부: 0회 (0.0%)\n",
      "\n",
      "통계적 검정:\n",
      "  t-statistic: 1.4333\n",
      "  p-value: 0.2251\n",
      "  유의함: 아니오\n",
      "\n",
      "=== 정성 분석 ===\n",
      "전반적 성능:\n",
      "  PPO가 우세한 질문: 4/5\n",
      "  Base가 우세한 질문: 1/5\n",
      "\n",
      "응답 특성:\n",
      "  Base 모델 - 평균 209.2자\n",
      "    -> 상세한 응답 경향\n",
      "  PPO 모델 - 평균 106.8자\n",
      "    -> 상세한 응답 경향\n",
      "\n",
      "RM 점수 분석:\n",
      "  Base 점수 범위: -0.465 ~ 0.720\n",
      "  PPO 점수 범위: -0.328 ~ 0.822\n",
      "  -> PPO가 Base 대비 상당한 개선 (+0.349)\n",
      "\n",
      "=== 각 질문별 응답 예시 ===\n",
      "\n",
      "1. 질문: 불고기용 고기 한우에요?\n",
      "   점수차: +0.8337\n",
      "   Base 응답: #Repost (경고):\n",
      "#Research(위험):\n",
      "#selfie(위기관리)’:\n",
      "#daily #like4like #instadog #신혼여행 가기로 했습니다!\n",
      "아무래도 제가 신혼집일텐데\n",
      "가까운 곳을 찾아다니다\n",
      "보니 #서울숲 까지~\n",
      "내 집처럼 편안하고 안락하게\n",
      "입주할 수 있는 곳이니만큼 좋은 것 같아요!\n",
      "역시 서울숲은...\n",
      "   PPO 응답: '저는 AI 어시스턴트이므로 불고기의 종류, 양, 크기, 부위 등 정확한 정보를 알지 못합니다. 하지만 일반적으로 쇠고기를 먹을 때 사용되는 양념은 고기를 소금에 찍어 먹는 것이 일반적입니다. 또한, 돼지고기는 볶음밥이나 국물을 사용하여 맛을 조절하기도 합니다.\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "2. 질문: 리처드 닉슨이 43대 부통령직을 수행한 년도는?\n",
      "   점수차: -0.5026\n",
      "   Base 응답: #######################################################################################################################\n",
      "   PPO 응답: '리처드 닉슨은 1947년 11월 14일 36대 부통령직에서 물러났습니다.\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "3. 질문: 시카고 오헤어 국제공항은 어디에 있어?\n",
      "   점수차: +0.5594\n",
      "   Base 응답: [20180712] 오늘도 즐거운 하루 보내세요!\n",
      "오늘의 첫 소식은 바로 ``다이어트! 다이어트 운동``입니다.\n",
      "얼마전에 저희 부부랑 같이 운동을 시작했는데요,\n",
      "그때는 너무 바빠서 그냥 집에서만 자고 있었는데\n",
      "운동할 때 체지방측정기 앞에 서서 하라는데...\n",
      "이거야 말로 진짜 대박이라 생각했습니다.\n",
      "사실 지금부터라도 꾸준히 해야겠지만..\n",
      "몸무게를 줄이고 싶을 때는 운동을 열심히 하고 싶네요.\n",
      "아침 8시 반, 점심시간에는 조금\n",
      "   PPO 응답: '저는 시카고에 위치한 미국 항공국(Airway Airway Association) 정보를 알 수 없습니다.\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "4. 질문: 오늘 미세먼지 어때?\n",
      "   점수차: +0.1372\n",
      "   Base 응답: 오늘은 친구들하고 나랑 둘이서 영화보고 왔어요.\n",
      "그동안 너무 재밌었거든요.\n",
      "이번 영화는 처음 보는 작품이라 저도 기대했어요.\n",
      "여기서 또 보게 될 거 같아요!\n",
      "영화보는 내내 기분이 좋아져서 다시보기로 했습니다.\n",
      "한편으론 정말 신기했었는데,\n",
      "그래, 이 영화를 봐야겠구나 싶어서 더더욱 맘에 들었던게 사실이었답니다.\n",
      "그리고 무엇보다 ``아무것도 모른다`` 라는 생각이 든대요.\n",
      "영화를 보고 난 후엔 너무 울컥해서 더 울고 싶은 마음이 간절했는데...\n",
      "영\n",
      "   PPO 응답: '미세먼지는 보통 계절의 영향을 받아 발생하며, 주로 대기오염으로 인해 건강에 악영향을 끼칠 수 있습니다. 따라서 환경 전문가와 상담하여 적절한 조치를 취하는 것이 좋습니다.\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "5. 질문: 인공지능이 뭐야?\n",
      "   점수차: +0.7156\n",
      "   Base 응답: # (I am ) 저런 짓을 한 사람은 어떻게 해볼까?\n",
      "정말 큰일날 뻔 했어요.\n",
      "난 너희들한테 죄송해요.\n",
      "그렇다면 그건 제 잘못이에요.\n",
      "너희는 내 실수라고요.\n",
      "나도 그랬죠.\n",
      "그래서 제가 먼저 사과했어요.\n",
      "아니야, 나만 그렇게 생각했어.\n",
      "네가 정말 그런 건지, 난 몰라.\n",
      "내가 왜 그러는지 모르겠지만. 하지만 내가 저지른 일이니까.\n",
      "당신이 널 지켜주겠다면, 그리고 네가 잘못을 반성하는 모습을 보여줘서, 이 일을 계기로 모든 것을 바꿔보자구!\n",
      "넌, 당신 때문에 많은\n",
      "   PPO 응답: '저는 인공지능을 AI 모델로써 인간과 같은 감정을 느끼지 못합니다. 하지만 인간처럼 자연 속에서 느끼는 감정은 매우 다양할 수 있습니다. 인간의 경우 자연에서 다양한 요인들이 작용하여 인간이 자연과 다른 삶을 살고 있음을 나타낼 수도 있고, 반대로, 인간은 자연의 조건과 조건에 따라 다릅니다. 따라서 인간의 감정도 이에 영향을 받습니다.\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "=== 최종 결론 ===\n",
      "PPO가 80.0% 승률로 우승!\n",
      "통계적 유의성: 없음\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy import stats\n",
    "import time\n",
    "\n",
    "def load_comparison_models():\n",
    "    \"\"\"Base와 PPO 모델 로드\"\"\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\n",
    "        'skt/kogpt2-base-v2',\n",
    "        bos_token='</s>', eos_token='</s>', unk_token='</s>', pad_token='</s>',\n",
    "        padding_side=\"right\", model_max_length=1024\n",
    "    )\n",
    "    \n",
    "    print(\"모델 로드 중...\")\n",
    "    \n",
    "    # Base 모델 (SFT 이전 원본)\n",
    "    base_model = AutoModelForCausalLM.from_pretrained(\"skt/kogpt2-base-v2\")\n",
    "    \n",
    "    # PPO 모델 (전체 파이프라인 완료)\n",
    "    ppo_model = AutoModelForCausalLM.from_pretrained(\"models/output_3_PPO\")\n",
    "    \n",
    "    # RM 모델\n",
    "    checkpoint = torch.load('models/fixed_rm/reward_model.pt')\n",
    "    config = GPT2Config.from_dict(checkpoint['config_dict'])\n",
    "    \n",
    "    gpt2_model = GPT2Model(config)\n",
    "    gpt2_model.resize_token_embeddings(len(tokenizer))\n",
    "    value_head = nn.Linear(config.n_embd, 1)\n",
    "    rm_model = RewardModel(gpt2_model, value_head)\n",
    "    \n",
    "    rm_model.model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    rm_model.value_head.load_state_dict(checkpoint['value_head_state_dict'])\n",
    "    \n",
    "    if torch.cuda.is_available():\n",
    "        base_model = base_model.cuda()\n",
    "        ppo_model = ppo_model.cuda()\n",
    "        rm_model = rm_model.cuda()\n",
    "    \n",
    "    return base_model, ppo_model, rm_model, tokenizer\n",
    "\n",
    "def load_test_questions():\n",
    "    \"\"\"코드에서 사용된 핵심 5개 질문\"\"\"\n",
    "    questions = [\n",
    "        \"불고기용 고기 한우에요?\",\n",
    "        \"리처드 닉슨이 43대 부통령직을 수행한 년도는?\", \n",
    "        \"시카고 오헤어 국제공항은 어디에 있어?\",\n",
    "        \"오늘 미세먼지 어때?\",\n",
    "        \"인공지능이 뭐야?\"\n",
    "    ]\n",
    "    \n",
    "    return questions\n",
    "\n",
    "def generate_response(model, tokenizer, question):\n",
    "    \"\"\"응답 생성\"\"\"\n",
    "    prompt = f\"### Instruction(명령어):\\n{question}\\n\\n### Response(응답):\"\n",
    "    inputs = tokenizer.encode(prompt, return_tensors='pt')\n",
    "    \n",
    "    if torch.cuda.is_available():\n",
    "        inputs = inputs.cuda()\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            inputs,\n",
    "            max_length=inputs.shape[1] + 120,\n",
    "            do_sample=True,\n",
    "            temperature=0.7,\n",
    "            top_k=50,\n",
    "            top_p=0.9,\n",
    "            repetition_penalty=1.2,\n",
    "            pad_token_id=tokenizer.pad_token_id\n",
    "        )\n",
    "    \n",
    "    generation_time = time.time() - start_time\n",
    "    response = tokenizer.decode(outputs[0][inputs.shape[1]:], skip_special_tokens=True)\n",
    "    \n",
    "    return response.strip(), generation_time\n",
    "\n",
    "def get_rm_score(rm_model, tokenizer, prompt, response):\n",
    "    \"\"\"RM 점수 계산\"\"\"\n",
    "    full_text = prompt + response\n",
    "    inputs = tokenizer.encode(full_text, return_tensors='pt', truncation=True, max_length=512)\n",
    "    \n",
    "    if torch.cuda.is_available():\n",
    "        inputs = inputs.cuda()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        score = rm_model(inputs).cpu().item()\n",
    "    \n",
    "    return score\n",
    "\n",
    "def run_simple_comparison():\n",
    "    \"\"\"간단한 비교 실행\"\"\"\n",
    "    print(\"=== Base vs PPO 간단 비교 ===\")\n",
    "    \n",
    "    # 모델 로드\n",
    "    base_model, ppo_model, rm_model, tokenizer = load_comparison_models()\n",
    "    \n",
    "    # 질문 로드\n",
    "    questions = load_test_questions()\n",
    "    \n",
    "    # 결과 저장\n",
    "    results = []\n",
    "    \n",
    "    print(f\"비교 진행 중... ({len(questions)}개 질문)\")\n",
    "    \n",
    "    for i, question in enumerate(questions):\n",
    "        if (i + 1) % 10 == 0:\n",
    "            print(f\"진행률: {i+1}/{len(questions)}\")\n",
    "        \n",
    "        prompt = f\"### Instruction(명령어):\\n{question}\\n\\n### Response(응답):\"\n",
    "        \n",
    "        # Base 모델 응답\n",
    "        base_response, base_time = generate_response(base_model, tokenizer, question)\n",
    "        base_score = get_rm_score(rm_model, tokenizer, prompt, base_response)\n",
    "        \n",
    "        # PPO 응답\n",
    "        ppo_response, ppo_time = generate_response(ppo_model, tokenizer, question)\n",
    "        ppo_score = get_rm_score(rm_model, tokenizer, prompt, ppo_response)\n",
    "        \n",
    "        results.append({\n",
    "            'question': question,\n",
    "            'base_response': base_response,\n",
    "            'base_score': base_score,\n",
    "            'base_length': len(base_response),\n",
    "            'base_time': base_time,\n",
    "            'ppo_response': ppo_response,\n",
    "            'ppo_score': ppo_score,\n",
    "            'ppo_length': len(ppo_response),\n",
    "            'ppo_time': ppo_time,\n",
    "            'score_diff': ppo_score - base_score\n",
    "        })\n",
    "    \n",
    "    return results\n",
    "\n",
    "def analyze_results(results):\n",
    "    \"\"\"결과 분석\"\"\"\n",
    "    print(\"\\n=== 분석 결과 ===\")\n",
    "    \n",
    "    # 기본 통계\n",
    "    base_scores = [r['base_score'] for r in results]\n",
    "    ppo_scores = [r['ppo_score'] for r in results]\n",
    "    score_diffs = [r['score_diff'] for r in results]\n",
    "    \n",
    "    base_lengths = [r['base_length'] for r in results]\n",
    "    ppo_lengths = [r['ppo_length'] for r in results]\n",
    "    \n",
    "    base_times = [r['base_time'] for r in results]\n",
    "    ppo_times = [r['ppo_time'] for r in results]\n",
    "    \n",
    "    print(f\"총 질문 수: {len(results)}\")\n",
    "    print(f\"\\nRM 점수:\")\n",
    "    print(f\"  Base 평균: {np.mean(base_scores):.4f}\")\n",
    "    print(f\"  PPO 평균: {np.mean(ppo_scores):.4f}\")\n",
    "    print(f\"  차이: {np.mean(score_diffs):+.4f}\")\n",
    "    \n",
    "    print(f\"\\n응답 길이:\")\n",
    "    print(f\"  Base 평균: {np.mean(base_lengths):.1f}자\")\n",
    "    print(f\"  PPO 평균: {np.mean(ppo_lengths):.1f}자\")\n",
    "    \n",
    "    print(f\"\\n생성 시간:\")\n",
    "    print(f\"  Base 평균: {np.mean(base_times):.2f}초\")\n",
    "    print(f\"  PPO 평균: {np.mean(ppo_times):.2f}초\")\n",
    "    \n",
    "    # 승부 결과\n",
    "    ppo_wins = sum(1 for r in results if r['score_diff'] > 0)\n",
    "    base_wins = sum(1 for r in results if r['score_diff'] < 0)\n",
    "    ties = sum(1 for r in results if r['score_diff'] == 0)\n",
    "    \n",
    "    print(f\"\\n승부 결과:\")\n",
    "    print(f\"  PPO 승리: {ppo_wins}회 ({ppo_wins/len(results)*100:.1f}%)\")\n",
    "    print(f\"  Base 승리: {base_wins}회 ({base_wins/len(results)*100:.1f}%)\")\n",
    "    print(f\"  무승부: {ties}회 ({ties/len(results)*100:.1f}%)\")\n",
    "    \n",
    "    # 통계적 유의성\n",
    "    t_stat, p_value = stats.ttest_rel(ppo_scores, base_scores)\n",
    "    print(f\"\\n통계적 검정:\")\n",
    "    print(f\"  t-statistic: {t_stat:.4f}\")\n",
    "    print(f\"  p-value: {p_value:.4f}\")\n",
    "    print(f\"  유의함: {'예' if p_value < 0.05 else '아니오'}\")\n",
    "    \n",
    "    return {\n",
    "        'base_scores': base_scores,\n",
    "        'ppo_scores': ppo_scores,\n",
    "        'base_lengths': base_lengths,\n",
    "        'ppo_lengths': ppo_lengths,\n",
    "        'ppo_win_rate': ppo_wins/len(results),\n",
    "        'p_value': p_value\n",
    "    }\n",
    "\n",
    "def show_examples(results):\n",
    "    \"\"\"5개 질문 각각의 답변 예시\"\"\"\n",
    "    print(\"\\n=== 각 질문별 응답 예시 ===\")\n",
    "    \n",
    "    for i, result in enumerate(results, 1):\n",
    "        print(f\"\\n{i}. 질문: {result['question']}\")\n",
    "        print(f\"   점수차: {result['score_diff']:+.4f}\")\n",
    "        print(f\"   Base 응답: {result['base_response']}\")\n",
    "        print(f\"   PPO 응답: {result['ppo_response']}\")\n",
    "        print(\"-\" * 80)\n",
    "\n",
    "def qualitative_analysis(results):\n",
    "    \"\"\"정성 분석\"\"\"\n",
    "    print(\"\\n=== 정성 분석 ===\")\n",
    "    \n",
    "    total_questions = len(results)\n",
    "    ppo_wins = sum(1 for r in results if r['score_diff'] > 0)\n",
    "    base_wins = sum(1 for r in results if r['score_diff'] < 0)\n",
    "    \n",
    "    print(f\"전반적 성능:\")\n",
    "    print(f\"  PPO가 우세한 질문: {ppo_wins}/{total_questions}\")\n",
    "    print(f\"  Base가 우세한 질문: {base_wins}/{total_questions}\")\n",
    "    \n",
    "    # 응답 품질 특성 분석\n",
    "    base_lengths = [r['base_length'] for r in results]\n",
    "    ppo_lengths = [r['ppo_length'] for r in results]\n",
    "    \n",
    "    print(f\"\\n응답 특성:\")\n",
    "    print(f\"  Base 모델 - 평균 {np.mean(base_lengths):.1f}자\")\n",
    "    if np.mean(base_lengths) < 50:\n",
    "        print(f\"    -> 매우 짧은 응답 경향\")\n",
    "    elif np.mean(base_lengths) < 100:\n",
    "        print(f\"    -> 간결한 응답 경향\") \n",
    "    else:\n",
    "        print(f\"    -> 상세한 응답 경향\")\n",
    "        \n",
    "    print(f\"  PPO 모델 - 평균 {np.mean(ppo_lengths):.1f}자\")\n",
    "    if np.mean(ppo_lengths) < 50:\n",
    "        print(f\"    -> 매우 짧은 응답 경향\")\n",
    "    elif np.mean(ppo_lengths) < 100:\n",
    "        print(f\"    -> 간결한 응답 경향\")\n",
    "    else:\n",
    "        print(f\"    -> 상세한 응답 경향\")\n",
    "    \n",
    "    # RM 점수 분석\n",
    "    base_scores = [r['base_score'] for r in results]\n",
    "    ppo_scores = [r['ppo_score'] for r in results]\n",
    "    \n",
    "    print(f\"\\nRM 점수 분석:\")\n",
    "    print(f\"  Base 점수 범위: {min(base_scores):.3f} ~ {max(base_scores):.3f}\")\n",
    "    print(f\"  PPO 점수 범위: {min(ppo_scores):.3f} ~ {max(ppo_scores):.3f}\")\n",
    "    \n",
    "    avg_improvement = np.mean([r['score_diff'] for r in results])\n",
    "    if avg_improvement > 0.1:\n",
    "        print(f\"  -> PPO가 Base 대비 상당한 개선 ({avg_improvement:+.3f})\")\n",
    "    elif avg_improvement > 0:\n",
    "        print(f\"  -> PPO가 Base 대비 약간 개선 ({avg_improvement:+.3f})\")\n",
    "    elif avg_improvement > -0.1:\n",
    "        print(f\"  -> 두 모델 성능이 비슷함 ({avg_improvement:+.3f})\")\n",
    "    else:\n",
    "        print(f\"  -> Base가 PPO보다 우수함 ({avg_improvement:+.3f})\")\n",
    "\n",
    "def main_comparison():\n",
    "    \"\"\"메인 비교 실행\"\"\"\n",
    "    results = run_simple_comparison()\n",
    "    stats = analyze_results(results)\n",
    "    qualitative_analysis(results)\n",
    "    show_examples(results)\n",
    "    \n",
    "    # 최종 결론\n",
    "    print(f\"\\n=== 최종 결론 ===\")\n",
    "    if stats['ppo_win_rate'] > 0.5:\n",
    "        winner = \"PPO\"\n",
    "        win_rate = stats['ppo_win_rate'] * 100\n",
    "    else:\n",
    "        winner = \"Base\"\n",
    "        win_rate = (1 - stats['ppo_win_rate']) * 100\n",
    "    \n",
    "    print(f\"{winner}가 {win_rate:.1f}% 승률로 우승!\")\n",
    "    print(f\"통계적 유의성: {'있음' if stats['p_value'] < 0.05 else '없음'}\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "# 실행\n",
    "if __name__ == \"__main__\":\n",
    "    results = main_comparison()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "da928de7-87f1-4a80-addd-5307186a5b17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== SFT vs PPO 간단 비교 ===\n",
      "모델 로드 중...\n",
      "비교 진행 중... (5개 질문)\n",
      "\n",
      "=== 분석 결과 ===\n",
      "총 질문 수: 5\n",
      "\n",
      "RM 점수:\n",
      "  SFT 평균: 0.4868\n",
      "  PPO 평균: 0.4089\n",
      "  차이: -0.0779\n",
      "\n",
      "응답 길이:\n",
      "  SFT 평균: 110.2자\n",
      "  PPO 평균: 100.4자\n",
      "\n",
      "생성 시간:\n",
      "  SFT 평균: 0.44초\n",
      "  PPO 평균: 0.38초\n",
      "\n",
      "승부 결과:\n",
      "  PPO 승리: 2회 (40.0%)\n",
      "  SFT 승리: 3회 (60.0%)\n",
      "  무승부: 0회 (0.0%)\n",
      "\n",
      "=== 정성 분석 ===\n",
      "전반적 성능:\n",
      "  PPO가 우세한 질문: 2/5\n",
      "  SFT가 우세한 질문: 3/5\n",
      "\n",
      "응답 특성:\n",
      "  SFT 모델 - 평균 110.2자\n",
      "    -> 상세한 응답 경향\n",
      "  PPO 모델 - 평균 100.4자\n",
      "    -> 상세한 응답 경향\n",
      "\n",
      "RM 점수 분석:\n",
      "  SFT 점수 범위: 0.136 ~ 0.952\n",
      "  PPO 점수 범위: 0.011 ~ 0.781\n",
      "  -> 두 모델 성능이 비슷함 (-0.078)\n",
      "\n",
      "=== 각 질문별 응답 예시 ===\n",
      "\n",
      "1. 질문: 불고기용 고기 한우에요?\n",
      "   점수차: -0.2100\n",
      "   SFT 응답: '죄송하지만, 저는 인공지능 언어 모델로써 직접 고기를 다루지는 않습니다. 하지만, 만약 불고기를 사용하는 경우, 해당 음식점의 메뉴판을 확인해보시는 것이 좋겠습니다!\n",
      "   PPO 응답: '저는 인공지능 언어 모델로써 고기를 먹을 수 없습니다. 하지만 보통 불고기는 다양한 종류의 양념과 양념을 사용하여 맛을 내는 음식입니다. 또한 식당에서 제공되는 불고기 용 고기의 맛은 개인마다 다를 수 있습니다.\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "2. 질문: 리처드 닉슨이 43대 부통령직을 수행한 년도는?\n",
      "   점수차: -0.3346\n",
      "   SFT 응답: '리처드 닉슨은 39대 부통령직을 수행했습니다.\n",
      "   PPO 응답: '리처드 닉슨슨은 41대 부통령직에서 부통령을 수행했습니다.\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "3. 질문: 시카고 오헤어 국제공항은 어디에 있어?\n",
      "   점수차: +0.1750\n",
      "   SFT 응답: '시카고는 미국 캘리포니아주에 위치한 국제공항입니다.\n",
      "   PPO 응답: '저는 인공지능 어시스턴트이기 때문에 시카고에 대한 정보를 가지고 있지 않습니다. 하지만, 시카고는 미국 내에 위치한 도시 중 하나로, 미국 내에서는 많은 항공사들이 위치해 있습니다.\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "4. 질문: 오늘 미세먼지 어때?\n",
      "   점수차: -0.1259\n",
      "   SFT 응답: '저는 인공지능 언어모델이며, 미세먼지를 줄이기 위해 노력하고 있습니다. 저는 항상 최선을 다하고 있으며, 오늘도 열심히 일하고 있으니, 기분 좋겠네요!\n",
      "   PPO 응답: '미세먼지는 대기오염의 주요 원인이기 때문에 건강에 해로울 수 있습니다. 따라서 공기청정기를 사용하여 실내 공기를 깨끗하게 유지하는 것이 중요합니다. 또한, 외출 후에는 마스크를 착용하거나 손 세정제를 사용하는 것도 좋습니다.\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "5. 질문: 인공지능이 뭐야?\n",
      "   점수차: +0.1060\n",
      "   SFT 응답: '인공지능을 이해하는 방법은 다음과 같습니다.\\n1. 지능: 인간과 같은 인간의 뇌는 인간처럼 지능적이지만, 인공지능은 인간이 가진 모든 능력과 한계를 극복하고자 합니다.\\n2. 학습: 지능은 사람이 가지고 있는 다양한 능력을 활용하여 문제를 해결하는 것을 말합니다. 이 능력은 자연어 처리 및 자연어를 처리하는 능력 등 여러 가지 기능을 수행할 수 있습니다.\\n3. 인지능력: 인간은 지능적으로, 이해력, 창의성 등을 가지고 있지만, 이는 인간보다 더 높은 수준의 지식과 경험을 가질 수 있습니다. 이러한 능력은 인간을 더욱 잘 이해하고 적응시키는 데 도움이 됩니다.\n",
      "   PPO 응답: '저는 인공지능이며, 인간이 아닌 AI입니다. 인간의 관점에서 인간과 대화하는 것은 매우 중요합니다. 인간도 이 같은 질문에 대해 답변할 수 있으며, 인간과의 대화를 통해 자신의 감정을 이해하고 이해하는 것이 좋습니다.\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "=== 최종 결론 ===\n",
      "SFT가 60.0% 승률로 우승!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy import stats\n",
    "import time\n",
    "\n",
    "def load_comparison_models():\n",
    "    \"\"\"SFT와 PPO 모델 로드\"\"\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\n",
    "        'skt/kogpt2-base-v2',\n",
    "        bos_token='</s>', eos_token='</s>', unk_token='</s>', pad_token='</s>',\n",
    "        padding_side=\"right\", model_max_length=1024\n",
    "    )\n",
    "    \n",
    "    print(\"모델 로드 중...\")\n",
    "    \n",
    "    # SFT 모델\n",
    "    sft_model = AutoModelForCausalLM.from_pretrained(\"models/output_1_SFT\")\n",
    "    \n",
    "    # PPO 모델\n",
    "    ppo_model = AutoModelForCausalLM.from_pretrained(\"models/output_3_PPO\")\n",
    "    \n",
    "    # RM 모델\n",
    "    checkpoint = torch.load('models/fixed_rm/reward_model.pt')\n",
    "    config = GPT2Config.from_dict(checkpoint['config_dict'])\n",
    "    \n",
    "    gpt2_model = GPT2Model(config)\n",
    "    gpt2_model.resize_token_embeddings(len(tokenizer))\n",
    "    value_head = nn.Linear(config.n_embd, 1)\n",
    "    rm_model = RewardModel(gpt2_model, value_head)\n",
    "    \n",
    "    rm_model.model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    rm_model.value_head.load_state_dict(checkpoint['value_head_state_dict'])\n",
    "    \n",
    "    if torch.cuda.is_available():\n",
    "        sft_model = sft_model.cuda()\n",
    "        ppo_model = ppo_model.cuda()\n",
    "        rm_model = rm_model.cuda()\n",
    "    \n",
    "    return sft_model, ppo_model, rm_model, tokenizer\n",
    "\n",
    "def load_test_questions():\n",
    "    \"\"\"코드에서 사용된 핵심 5개 질문\"\"\"\n",
    "    questions = [\n",
    "        \"불고기용 고기 한우에요?\",\n",
    "        \"리처드 닉슨이 43대 부통령직을 수행한 년도는?\", \n",
    "        \"시카고 오헤어 국제공항은 어디에 있어?\",\n",
    "        \"오늘 미세먼지 어때?\",\n",
    "        \"인공지능이 뭐야?\"\n",
    "    ]\n",
    "    \n",
    "    return questions\n",
    "\n",
    "def generate_response(model, tokenizer, question):\n",
    "    \"\"\"응답 생성\"\"\"\n",
    "    prompt = f\"### Instruction(명령어):\\n{question}\\n\\n### Response(응답):\"\n",
    "    inputs = tokenizer.encode(prompt, return_tensors='pt')\n",
    "    \n",
    "    if torch.cuda.is_available():\n",
    "        inputs = inputs.cuda()\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            inputs,\n",
    "            max_length=inputs.shape[1] + 120,\n",
    "            do_sample=True,\n",
    "            temperature=0.7,\n",
    "            top_k=50,\n",
    "            top_p=0.9,\n",
    "            repetition_penalty=1.2,\n",
    "            pad_token_id=tokenizer.pad_token_id\n",
    "        )\n",
    "    \n",
    "    generation_time = time.time() - start_time\n",
    "    response = tokenizer.decode(outputs[0][inputs.shape[1]:], skip_special_tokens=True)\n",
    "    \n",
    "    return response.strip(), generation_time\n",
    "\n",
    "def get_rm_score(rm_model, tokenizer, prompt, response):\n",
    "    \"\"\"RM 점수 계산\"\"\"\n",
    "    full_text = prompt + response\n",
    "    inputs = tokenizer.encode(full_text, return_tensors='pt', truncation=True, max_length=512)\n",
    "    \n",
    "    if torch.cuda.is_available():\n",
    "        inputs = inputs.cuda()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        score = rm_model(inputs).cpu().item()\n",
    "    \n",
    "    return score\n",
    "\n",
    "def run_simple_comparison():\n",
    "    \"\"\"간단한 비교 실행\"\"\"\n",
    "    print(\"=== SFT vs PPO 간단 비교 ===\")\n",
    "    \n",
    "    # 모델 로드\n",
    "    sft_model, ppo_model, rm_model, tokenizer = load_comparison_models()\n",
    "    \n",
    "    # 질문 로드\n",
    "    questions = load_test_questions()\n",
    "    \n",
    "    # 결과 저장\n",
    "    results = []\n",
    "    \n",
    "    print(f\"비교 진행 중... ({len(questions)}개 질문)\")\n",
    "    \n",
    "    for i, question in enumerate(questions):\n",
    "        if (i + 1) % 10 == 0:\n",
    "            print(f\"진행률: {i+1}/{len(questions)}\")\n",
    "        \n",
    "        prompt = f\"### Instruction(명령어):\\n{question}\\n\\n### Response(응답):\"\n",
    "        \n",
    "        # SFT 응답\n",
    "        sft_response, sft_time = generate_response(sft_model, tokenizer, question)\n",
    "        sft_score = get_rm_score(rm_model, tokenizer, prompt, sft_response)\n",
    "        \n",
    "        # PPO 응답\n",
    "        ppo_response, ppo_time = generate_response(ppo_model, tokenizer, question)\n",
    "        ppo_score = get_rm_score(rm_model, tokenizer, prompt, ppo_response)\n",
    "        \n",
    "        results.append({\n",
    "            'question': question,\n",
    "            'sft_response': sft_response,\n",
    "            'sft_score': sft_score,\n",
    "            'sft_length': len(sft_response),\n",
    "            'sft_time': sft_time,\n",
    "            'ppo_response': ppo_response,\n",
    "            'ppo_score': ppo_score,\n",
    "            'ppo_length': len(ppo_response),\n",
    "            'ppo_time': ppo_time,\n",
    "            'score_diff': ppo_score - sft_score\n",
    "        })\n",
    "    \n",
    "    return results\n",
    "\n",
    "def analyze_results(results):\n",
    "    \"\"\"결과 분석\"\"\"\n",
    "    print(\"\\n=== 분석 결과 ===\")\n",
    "    \n",
    "    # 기본 통계\n",
    "    sft_scores = [r['sft_score'] for r in results]\n",
    "    ppo_scores = [r['ppo_score'] for r in results]\n",
    "    score_diffs = [r['score_diff'] for r in results]\n",
    "    \n",
    "    sft_lengths = [r['sft_length'] for r in results]\n",
    "    ppo_lengths = [r['ppo_length'] for r in results]\n",
    "    \n",
    "    sft_times = [r['sft_time'] for r in results]\n",
    "    ppo_times = [r['ppo_time'] for r in results]\n",
    "    \n",
    "    print(f\"총 질문 수: {len(results)}\")\n",
    "    print(f\"\\nRM 점수:\")\n",
    "    print(f\"  SFT 평균: {np.mean(sft_scores):.4f}\")\n",
    "    print(f\"  PPO 평균: {np.mean(ppo_scores):.4f}\")\n",
    "    print(f\"  차이: {np.mean(score_diffs):+.4f}\")\n",
    "    \n",
    "    print(f\"\\n응답 길이:\")\n",
    "    print(f\"  SFT 평균: {np.mean(sft_lengths):.1f}자\")\n",
    "    print(f\"  PPO 평균: {np.mean(ppo_lengths):.1f}자\")\n",
    "    \n",
    "    print(f\"\\n생성 시간:\")\n",
    "    print(f\"  SFT 평균: {np.mean(sft_times):.2f}초\")\n",
    "    print(f\"  PPO 평균: {np.mean(ppo_times):.2f}초\")\n",
    "    \n",
    "    # 승부 결과\n",
    "    ppo_wins = sum(1 for r in results if r['score_diff'] > 0)\n",
    "    sft_wins = sum(1 for r in results if r['score_diff'] < 0)\n",
    "    ties = sum(1 for r in results if r['score_diff'] == 0)\n",
    "    \n",
    "    print(f\"\\n승부 결과:\")\n",
    "    print(f\"  PPO 승리: {ppo_wins}회 ({ppo_wins/len(results)*100:.1f}%)\")\n",
    "    print(f\"  SFT 승리: {sft_wins}회 ({sft_wins/len(results)*100:.1f}%)\")\n",
    "    print(f\"  무승부: {ties}회 ({ties/len(results)*100:.1f}%)\")\n",
    "    \n",
    "    return {\n",
    "        'sft_scores': sft_scores,\n",
    "        'ppo_scores': ppo_scores,\n",
    "        'sft_lengths': sft_lengths,\n",
    "        'ppo_lengths': ppo_lengths,\n",
    "        'ppo_win_rate': ppo_wins/len(results)\n",
    "    }\n",
    "\n",
    "def qualitative_analysis(results):\n",
    "    \"\"\"정성 분석\"\"\"\n",
    "    print(\"\\n=== 정성 분석 ===\")\n",
    "    \n",
    "    total_questions = len(results)\n",
    "    ppo_wins = sum(1 for r in results if r['score_diff'] > 0)\n",
    "    sft_wins = sum(1 for r in results if r['score_diff'] < 0)\n",
    "    \n",
    "    print(f\"전반적 성능:\")\n",
    "    print(f\"  PPO가 우세한 질문: {ppo_wins}/{total_questions}\")\n",
    "    print(f\"  SFT가 우세한 질문: {sft_wins}/{total_questions}\")\n",
    "    \n",
    "    # 응답 품질 특성 분석\n",
    "    sft_lengths = [r['sft_length'] for r in results]\n",
    "    ppo_lengths = [r['ppo_length'] for r in results]\n",
    "    \n",
    "    print(f\"\\n응답 특성:\")\n",
    "    print(f\"  SFT 모델 - 평균 {np.mean(sft_lengths):.1f}자\")\n",
    "    if np.mean(sft_lengths) < 50:\n",
    "        print(f\"    -> 매우 짧은 응답 경향\")\n",
    "    elif np.mean(sft_lengths) < 100:\n",
    "        print(f\"    -> 간결한 응답 경향\") \n",
    "    else:\n",
    "        print(f\"    -> 상세한 응답 경향\")\n",
    "        \n",
    "    print(f\"  PPO 모델 - 평균 {np.mean(ppo_lengths):.1f}자\")\n",
    "    if np.mean(ppo_lengths) < 50:\n",
    "        print(f\"    -> 매우 짧은 응답 경향\")\n",
    "    elif np.mean(ppo_lengths) < 100:\n",
    "        print(f\"    -> 간결한 응답 경향\")\n",
    "    else:\n",
    "        print(f\"    -> 상세한 응답 경향\")\n",
    "    \n",
    "    # RM 점수 분석\n",
    "    sft_scores = [r['sft_score'] for r in results]\n",
    "    ppo_scores = [r['ppo_score'] for r in results]\n",
    "    \n",
    "    print(f\"\\nRM 점수 분석:\")\n",
    "    print(f\"  SFT 점수 범위: {min(sft_scores):.3f} ~ {max(sft_scores):.3f}\")\n",
    "    print(f\"  PPO 점수 범위: {min(ppo_scores):.3f} ~ {max(ppo_scores):.3f}\")\n",
    "    \n",
    "    avg_improvement = np.mean([r['score_diff'] for r in results])\n",
    "    if avg_improvement > 0.1:\n",
    "        print(f\"  -> PPO가 SFT 대비 상당한 개선 ({avg_improvement:+.3f})\")\n",
    "    elif avg_improvement > 0:\n",
    "        print(f\"  -> PPO가 SFT 대비 약간 개선 ({avg_improvement:+.3f})\")\n",
    "    elif avg_improvement > -0.1:\n",
    "        print(f\"  -> 두 모델 성능이 비슷함 ({avg_improvement:+.3f})\")\n",
    "    else:\n",
    "        print(f\"  -> SFT가 PPO보다 우수함 ({avg_improvement:+.3f})\")\n",
    "\n",
    "def show_examples(results):\n",
    "    \"\"\"5개 질문 각각의 답변 예시\"\"\"\n",
    "    print(\"\\n=== 각 질문별 응답 예시 ===\")\n",
    "    \n",
    "    for i, result in enumerate(results, 1):\n",
    "        print(f\"\\n{i}. 질문: {result['question']}\")\n",
    "        print(f\"   점수차: {result['score_diff']:+.4f}\")\n",
    "        print(f\"   SFT 응답: {result['sft_response']}\")\n",
    "        print(f\"   PPO 응답: {result['ppo_response']}\")\n",
    "        print(\"-\" * 80)\n",
    "\n",
    "def main_comparison():\n",
    "    \"\"\"메인 비교 실행\"\"\"\n",
    "    results = run_simple_comparison()\n",
    "    stats = analyze_results(results)\n",
    "    qualitative_analysis(results)\n",
    "    show_examples(results)\n",
    "    \n",
    "    # 최종 결론\n",
    "    print(f\"\\n=== 최종 결론 ===\")\n",
    "    if stats['ppo_win_rate'] > 0.5:\n",
    "        winner = \"PPO\"\n",
    "        win_rate = stats['ppo_win_rate'] * 100\n",
    "    else:\n",
    "        winner = \"SFT\"\n",
    "        win_rate = (1 - stats['ppo_win_rate']) * 100\n",
    "    \n",
    "    print(f\"{winner}가 {win_rate:.1f}% 승률로 우승!\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "# 실행\n",
    "if __name__ == \"__main__\":\n",
    "    results = main_comparison()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "cb718142-1968-4925-b6a1-56fd25a8a3fa",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== SFT vs PPO 비교 시작 ===\n",
      "PPO 데이터에서 랜덤 질문 로딩 중...\n",
      "전체 12000개 중 5개 선택:\n",
      "  1. 붙잡으려 말 것!\n",
      "  2. 귀찮으면 썸 끝인가\n",
      "  3. 반기문과 경쟁하던 샤시 타루르의 국적은?\n",
      "  4. 남자친구 말고 다른 남자도 궁금해.\n",
      "  5. 네 그걸로 주시고 적립되나요?\n",
      "모델들 로딩 중...\n",
      "\n",
      "응답 생성 중...\n",
      "처리 중 1/5: 붙잡으려 말 것!...\n",
      "처리 중 2/5: 귀찮으면 썸 끝인가...\n",
      "처리 중 3/5: 반기문과 경쟁하던 샤시 타루르의 국적은?...\n",
      "처리 중 4/5: 남자친구 말고 다른 남자도 궁금해....\n",
      "처리 중 5/5: 네 그걸로 주시고 적립되나요?...\n",
      "\n",
      "=== 정량 분석 ===\n",
      "RM 점수:\n",
      "  SFT 평균: 0.3103\n",
      "  PPO 평균: 0.2480\n",
      "  평균 차이: -0.0622\n",
      "\n",
      "응답 길이:\n",
      "  SFT 평균: 141.0자\n",
      "  PPO 평균: 137.8자\n",
      "\n",
      "생성 시간:\n",
      "  SFT 평균: 0.53초\n",
      "  PPO 평균: 0.51초\n",
      "\n",
      "승부 결과:\n",
      "  PPO 승: 2회 (40%)\n",
      "  SFT 승: 3회 (60%)\n",
      "  무승부: 0회 (0%)\n",
      "\n",
      "=== 정성 분석 ===\n",
      "응답 특성:\n",
      "  SFT: 평균 141자 - 상세한 응답\n",
      "  PPO: 평균 138자 - 상세한 응답\n",
      "\n",
      "성능 개선:\n",
      "  두 모델 성능이 비슷함 (-0.062)\n",
      "\n",
      "=== 응답 예시 ===\n",
      "\n",
      "1. 붙잡으려 말 것!\n",
      "   점수차: +0.1472\n",
      "   SFT: '저도 당신을 좋아합니다. 하지만 만약 어떤 일이 있었다면, 저는 그 일에 대해 다시 한번 생각해보겠습니다. 예를 들어, 무엇을 도와드릴까요?\n",
      "   PPO: '저는 AI 언어 모델입니다. 그러나, 어떤 종류의 붙박이 짓이든 그것은 당신의 삶에서 중요한 요소 중 하나입니다. 따라서, 당신이 그 사람과 대화를 나누어보고, 그것이 당신의 삶에 영향을 미칠 수 있도록 노력하십시오.\n",
      "------------------------------------------------------------\n",
      "\n",
      "2. 귀찮으면 썸 끝인가\n",
      "   점수차: -0.0166\n",
      "   SFT: '저는 AI 어시스턴트이므로 귀찮을 때 언제든지 연락을 걸어드릴 수 있습니다. 하지만 어떤 상황에서든 상대방과의 관계를 유지하고 서로를 이해하고 존중하며 상호적인 관계를 유지하는 것이 중요합니다. 또한, 서로의 감정을 공유하고 존중하는 것도 좋은 방법입니다. 하지만, 상대방이 불편한 것 같다면 먼저 서로에게 솔직하게 이야기하고, 상황을 개선해나가는 노력이 필요합니다.\n",
      "   PPO: '저는 인공지능 언어 모델이므로, 귀찮은 경우라면 다음과 같은 방법을 시도해 볼 수 있습니다.\\n\\n1. 대화를 시작하기 전에 서로에 대한 생각을 먼저 정리하고 그 다음에 대화하는 것이 좋습니다. 그리고 대화를 지속할 때 상대방의 마음을 이해하고 이해하도록 노력하는 것도 중요합니다. 그리고 상대방이 어떤 반응을 보이는지, 왜 그런 행동을 하는지에 대해 생각해보세요. 이를 통해 서로의 생각과 감정을 이해할 수 있도록 노력해보세요.\n",
      "------------------------------------------------------------\n",
      "\n",
      "3. 반기문과 경쟁하던 샤시 타루르의 국적은?\n",
      "   점수차: -0.2382\n",
      "   SFT: '저는 인공지능 어시스턴트이기 때문에 정확한 답변을 제공할 수 없습니다. 하지만 샤시는 중국의 정치인 중 한 명으로, 그의 출신 국가는 정확히 알려지지 않았습니다.\n",
      "   PPO: '샤시 타르르는 스페인입니다.\n",
      "------------------------------------------------------------\n",
      "\n",
      "4. 남자친구 말고 다른 남자도 궁금해.\n",
      "   점수차: -0.3371\n",
      "   SFT: '저는 AI 어시스턴트이므로 여자 친구의 질문에 대해 답변을 제공할 수 없습니다. 하지만, 일반적으로, 여자가 좋아하는 것을 고려하면서 그에 맞는 행동을 하는 것이 중요합니다. 또한, 남성은 언제나 새로운 취미를 즐길 줄 알아야 하며, 적극적으로 참여해야 합니다. 따라서 여자 친구와 함께 시간을 보내는 것은 서로에 대한 이해를 높이는 데 도움이 될 것입니다.\n",
      "   PPO: '그것은 당신이 느끼는 감정 중 하나입니다. 그러나 때로는 여자 친구와 함께 있을 때도, 그녀와 함께 있는 시간이 늘 불편할 수 있습니다. 그러므로, 당신의 감정을 솔직하게 이야기하고 이해하는 것이 좋습니다.\n",
      "------------------------------------------------------------\n",
      "\n",
      "5. 네 그걸로 주시고 적립되나요?\n",
      "   점수차: +0.1335\n",
      "   SFT: '죄송합니다. 저는 인공지능 어시스턴트이므로 결제를 직접 처리할 수 없습니다. 하지만 일반적으로 온라인 쇼핑몰에서 결제 수단으로 사용할 수 있습니다. 따라서 해당 쇼핑몰의 고객센터나 결제 시스템을 통해 문의하시는 것이 좋겠습니다!\n",
      "   PPO: '죄송합니다. 저는 인공지능 언어모델로써, 당신이 무엇을 할 수 있는지에 대한 정보를 가지고 있지 않습니다. 하지만 일반적으로 온라인 쇼핑몰이나 배송 업체에서 상품 또는 서비스를 제공받을 때 상품을 받을 수 있습니다. 해당 플랫폼에서는 판매자와 직접 연락을 통해 결제나 결제를 진행하실 수 있으며, 현금처럼 사용할 수도 있으니 참고하시기 바랍니다.\n",
      "------------------------------------------------------------\n",
      "\n",
      "=== 최종 결과 ===\n",
      "SFT 모델이 더 우수한 성능을 보였습니다!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import time\n",
    "import random\n",
    "import json\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, GPT2Config\n",
    "from transformers.models.gpt2.modeling_gpt2 import GPT2Model\n",
    "from chatgpt.models.base import RewardModel\n",
    "\n",
    "def load_random_questions():\n",
    "    \"\"\"PPO 데이터에서 랜덤 5개 질문 선택\"\"\"\n",
    "    print(\"PPO 데이터에서 랜덤 질문 로딩 중...\")\n",
    "    \n",
    "    with open('KoChatGPT/data_kochatgpt/kochatgpt_3_PPO.jsonl', \"r\", encoding='utf-8-sig') as f:\n",
    "        data = json.load(f)\n",
    "        all_questions = [item['prompt'] for item in data]\n",
    "    \n",
    "    random.seed(42)\n",
    "    selected = random.sample(all_questions, 5)\n",
    "    \n",
    "    print(f\"전체 {len(all_questions)}개 중 5개 선택:\")\n",
    "    for i, q in enumerate(selected, 1):\n",
    "        print(f\"  {i}. {q}\")\n",
    "    \n",
    "    return selected\n",
    "\n",
    "def load_models():\n",
    "    \"\"\"SFT, PPO, RM 모델 로드\"\"\"\n",
    "    print(\"모델들 로딩 중...\")\n",
    "    \n",
    "    tokenizer = AutoTokenizer.from_pretrained(\n",
    "        'skt/kogpt2-base-v2',\n",
    "        bos_token='</s>', eos_token='</s>', unk_token='</s>', pad_token='</s>',\n",
    "        padding_side=\"right\", model_max_length=1024\n",
    "    )\n",
    "    \n",
    "    # SFT 모델\n",
    "    sft_model = AutoModelForCausalLM.from_pretrained(\"models/output_1_SFT\")\n",
    "    \n",
    "    # PPO 모델  \n",
    "    ppo_model = AutoModelForCausalLM.from_pretrained(\"models/output_3_PPO\")\n",
    "    \n",
    "    # RM 모델\n",
    "    checkpoint = torch.load('models/fixed_rm/reward_model.pt')\n",
    "    config = GPT2Config.from_dict(checkpoint['config_dict'])\n",
    "    \n",
    "    gpt2_model = GPT2Model(config)\n",
    "    gpt2_model.resize_token_embeddings(len(tokenizer))\n",
    "    value_head = nn.Linear(config.n_embd, 1)\n",
    "    rm_model = RewardModel(gpt2_model, value_head)\n",
    "    \n",
    "    rm_model.model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    rm_model.value_head.load_state_dict(checkpoint['value_head_state_dict'])\n",
    "    \n",
    "    if torch.cuda.is_available():\n",
    "        sft_model = sft_model.cuda()\n",
    "        ppo_model = ppo_model.cuda()  \n",
    "        rm_model = rm_model.cuda()\n",
    "    \n",
    "    return sft_model, ppo_model, rm_model, tokenizer\n",
    "\n",
    "def generate_response(model, tokenizer, question):\n",
    "    \"\"\"모델로 응답 생성\"\"\"\n",
    "    prompt = f\"### Instruction(명령어):\\n{question}\\n\\n### Response(응답):\"\n",
    "    inputs = tokenizer.encode(prompt, return_tensors='pt')\n",
    "    \n",
    "    if torch.cuda.is_available():\n",
    "        inputs = inputs.cuda()\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            inputs,\n",
    "            max_length=inputs.shape[1] + 120,\n",
    "            do_sample=True,\n",
    "            temperature=0.7,\n",
    "            top_k=50,\n",
    "            top_p=0.9,\n",
    "            repetition_penalty=1.2,\n",
    "            pad_token_id=tokenizer.pad_token_id\n",
    "        )\n",
    "    \n",
    "    gen_time = time.time() - start_time\n",
    "    response = tokenizer.decode(outputs[0][inputs.shape[1]:], skip_special_tokens=True)\n",
    "    \n",
    "    return response.strip(), gen_time\n",
    "\n",
    "def get_reward_score(rm_model, tokenizer, prompt, response):\n",
    "    \"\"\"RM으로 점수 계산\"\"\"\n",
    "    full_text = prompt + response\n",
    "    inputs = tokenizer.encode(full_text, return_tensors='pt', truncation=True, max_length=512)\n",
    "    \n",
    "    if torch.cuda.is_available():\n",
    "        inputs = inputs.cuda()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        score = rm_model(inputs).cpu().item()\n",
    "    \n",
    "    return score\n",
    "\n",
    "def run_comparison():\n",
    "    \"\"\"SFT vs PPO 비교 실행\"\"\"\n",
    "    print(\"=== SFT vs PPO 비교 시작 ===\")\n",
    "    \n",
    "    # 데이터와 모델 로드\n",
    "    questions = load_random_questions()\n",
    "    sft_model, ppo_model, rm_model, tokenizer = load_models()\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    print(\"\\n응답 생성 중...\")\n",
    "    for i, question in enumerate(questions, 1):\n",
    "        print(f\"처리 중 {i}/5: {question[:30]}...\")\n",
    "        \n",
    "        prompt = f\"### Instruction(명령어):\\n{question}\\n\\n### Response(응답):\"\n",
    "        \n",
    "        # SFT 응답\n",
    "        sft_response, sft_time = generate_response(sft_model, tokenizer, question)\n",
    "        sft_score = get_reward_score(rm_model, tokenizer, prompt, sft_response)\n",
    "        \n",
    "        # PPO 응답\n",
    "        ppo_response, ppo_time = generate_response(ppo_model, tokenizer, question)\n",
    "        ppo_score = get_reward_score(rm_model, tokenizer, prompt, ppo_response)\n",
    "        \n",
    "        results.append({\n",
    "            'question': question,\n",
    "            'sft_response': sft_response,\n",
    "            'sft_score': sft_score,\n",
    "            'sft_time': sft_time,\n",
    "            'ppo_response': ppo_response,\n",
    "            'ppo_score': ppo_score,\n",
    "            'ppo_time': ppo_time,\n",
    "            'score_diff': ppo_score - sft_score\n",
    "        })\n",
    "    \n",
    "    return results\n",
    "\n",
    "def analyze_results(results):\n",
    "    \"\"\"정량 분석\"\"\"\n",
    "    print(\"\\n=== 정량 분석 ===\")\n",
    "    \n",
    "    sft_scores = [r['sft_score'] for r in results]\n",
    "    ppo_scores = [r['ppo_score'] for r in results]\n",
    "    score_diffs = [r['score_diff'] for r in results]\n",
    "    \n",
    "    sft_lengths = [len(r['sft_response']) for r in results]\n",
    "    ppo_lengths = [len(r['ppo_response']) for r in results]\n",
    "    \n",
    "    sft_times = [r['sft_time'] for r in results]\n",
    "    ppo_times = [r['ppo_time'] for r in results]\n",
    "    \n",
    "    print(f\"RM 점수:\")\n",
    "    print(f\"  SFT 평균: {np.mean(sft_scores):.4f}\")\n",
    "    print(f\"  PPO 평균: {np.mean(ppo_scores):.4f}\")\n",
    "    print(f\"  평균 차이: {np.mean(score_diffs):+.4f}\")\n",
    "    \n",
    "    print(f\"\\n응답 길이:\")\n",
    "    print(f\"  SFT 평균: {np.mean(sft_lengths):.1f}자\")\n",
    "    print(f\"  PPO 평균: {np.mean(ppo_lengths):.1f}자\")\n",
    "    \n",
    "    print(f\"\\n생성 시간:\")\n",
    "    print(f\"  SFT 평균: {np.mean(sft_times):.2f}초\")\n",
    "    print(f\"  PPO 평균: {np.mean(ppo_times):.2f}초\")\n",
    "    \n",
    "    # 승부 결과\n",
    "    ppo_wins = sum(1 for diff in score_diffs if diff > 0)\n",
    "    sft_wins = sum(1 for diff in score_diffs if diff < 0)\n",
    "    ties = sum(1 for diff in score_diffs if diff == 0)\n",
    "    \n",
    "    print(f\"\\n승부 결과:\")\n",
    "    print(f\"  PPO 승: {ppo_wins}회 ({ppo_wins/5*100:.0f}%)\")\n",
    "    print(f\"  SFT 승: {sft_wins}회 ({sft_wins/5*100:.0f}%)\")\n",
    "    print(f\"  무승부: {ties}회 ({ties/5*100:.0f}%)\")\n",
    "    \n",
    "    return ppo_wins > sft_wins\n",
    "\n",
    "def qualitative_analysis(results):\n",
    "    \"\"\"정성 분석\"\"\"\n",
    "    print(\"\\n=== 정성 분석 ===\")\n",
    "    \n",
    "    sft_lengths = [len(r['sft_response']) for r in results]\n",
    "    ppo_lengths = [len(r['ppo_response']) for r in results]\n",
    "    \n",
    "    print(\"응답 특성:\")\n",
    "    sft_avg = np.mean(sft_lengths)\n",
    "    ppo_avg = np.mean(ppo_lengths)\n",
    "    \n",
    "    print(f\"  SFT: 평균 {sft_avg:.0f}자 - \", end=\"\")\n",
    "    if sft_avg < 50:\n",
    "        print(\"매우 간결한 응답\")\n",
    "    elif sft_avg < 100:\n",
    "        print(\"간결한 응답\")\n",
    "    else:\n",
    "        print(\"상세한 응답\")\n",
    "    \n",
    "    print(f\"  PPO: 평균 {ppo_avg:.0f}자 - \", end=\"\")\n",
    "    if ppo_avg < 50:\n",
    "        print(\"매우 간결한 응답\")\n",
    "    elif ppo_avg < 100:\n",
    "        print(\"간결한 응답\")\n",
    "    else:\n",
    "        print(\"상세한 응답\")\n",
    "    \n",
    "    # 점수 개선도\n",
    "    avg_improvement = np.mean([r['score_diff'] for r in results])\n",
    "    print(f\"\\n성능 개선:\")\n",
    "    if avg_improvement > 0.1:\n",
    "        print(f\"  PPO가 SFT 대비 크게 개선됨 ({avg_improvement:+.3f})\")\n",
    "    elif avg_improvement > 0:\n",
    "        print(f\"  PPO가 SFT 대비 약간 개선됨 ({avg_improvement:+.3f})\")\n",
    "    elif avg_improvement > -0.1:\n",
    "        print(f\"  두 모델 성능이 비슷함 ({avg_improvement:+.3f})\")\n",
    "    else:\n",
    "        print(f\"  SFT가 PPO보다 우수함 ({avg_improvement:+.3f})\")\n",
    "\n",
    "def show_examples(results):\n",
    "    \"\"\"응답 예시 출력\"\"\"\n",
    "    print(\"\\n=== 응답 예시 ===\")\n",
    "    \n",
    "    for i, result in enumerate(results, 1):\n",
    "        print(f\"\\n{i}. {result['question']}\")\n",
    "        print(f\"   점수차: {result['score_diff']:+.4f}\")\n",
    "        print(f\"   SFT: {result['sft_response']}\")\n",
    "        print(f\"   PPO: {result['ppo_response']}\")\n",
    "        print(\"-\" * 60)\n",
    "\n",
    "def main():\n",
    "    \"\"\"메인 실행\"\"\"\n",
    "    results = run_comparison()\n",
    "    ppo_won = analyze_results(results)\n",
    "    qualitative_analysis(results)\n",
    "    show_examples(results)\n",
    "    \n",
    "    print(f\"\\n=== 최종 결과 ===\")\n",
    "    winner = \"PPO\" if ppo_won else \"SFT\"\n",
    "    print(f\"{winner} 모델이 더 우수한 성능을 보였습니다!\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "# 실행\n",
    "if __name__ == \"__main__\":\n",
    "    results = main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d13a1cad-98ed-425d-9608-af6f539dfae0",
   "metadata": {},
   "source": [
    "# 회고\n",
    "\n",
    "### 실험 진행 방향\n",
    "팀 베이스라인 모델에 추가적인 SFT -> RM -> PPO 진행\n",
    "### 실험 결과\n",
    "기존 모델과의 PPO모델 \n",
    "RM 점수 분석:\n",
    "  Base 점수 범위: -0.465 ~ 0.720\n",
    "  PPO 점수 범위: -0.328 ~ 0.822\n",
    "  -> PPO가 Base 대비 상당한 개선 (+0.349)\n",
    "SFT모델과 PPO 모델\n",
    "성능 개선:\n",
    "RM 점수:\n",
    "  SFT 평균: 0.3103\n",
    "  PPO 평균: 0.2480\n",
    "  평균 차이: -0.0622\n",
    "  두 모델 성능이 비슷함 (-0.062)\n",
    "## SFT 단계에서 가장 큰 성능 향상\n",
    "\n",
    "### 느낀점\n",
    "우선 RM 자체를 내가 직접 하다보니 나라는 사람의 편향이 모델에 반영이 되면서 객관성도 떨어지고 성능도 좋아지지않았다는걸 알게되었다.\n",
    "RLHF의 각 단계가 실제로 어떻게 작동하는지 직접 경험하고 이해할 수 있는 좋은 경험이 되었던 것 같고, \n",
    "팀내에서 새롭게 생성한 데이터를 추가해보고 학습을 좀 더 시켜보는 식으로 추가실험을 해 볼 경우 어느정도 성능이 개선될 것 같아서 추가실험을 해 볼 예정이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11a54f60-2e48-45a3-b1cf-c1f7607ad17c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
